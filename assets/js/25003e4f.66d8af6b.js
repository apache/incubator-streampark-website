"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[2838],{4351:e=>{e.exports=JSON.parse("{\"archive\":{\"blogPosts\":[{\"id\":\"streampark-flink-on-k8s\",\"metadata\":{\"permalink\":\"/blog/streampark-flink-on-k8s\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/0-streampark-flink-on-k8s.md\",\"source\":\"@site/blog/0-streampark-flink-on-k8s.md\",\"title\":\"Apache StreamPark\u2122 Flink on Kubernetes practice\",\"description\":\"Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"},{\"label\":\"FlinkSQL\",\"permalink\":\"/blog/tags/flink-sql\"},{\"label\":\"Kubernetes\",\"permalink\":\"/blog/tags/kubernetes\"}],\"readingTime\":13.98,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-flink-on-k8s\",\"title\":\"Apache StreamPark\u2122 Flink on Kubernetes practice\",\"tags\":[\"StreamPark\",\"Production Practice\",\"FlinkSQL\",\"Kubernetes\"],\"description\":\"Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe\"},\"unlisted\":false,\"nextItem\":{\"title\":\"Apache StreamPark\u2122 - Powerful Flink Development Framework\",\"permalink\":\"/blog/flink-development-framework-streampark\"}},\"content\":\"Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe.\\n\\n\x3c!-- truncate --\x3e\\n\\n## **Why Choose Native Kubernetes**\\n\\nNative Kubernetes offers the following advantages:\\n\\n- Shorter Failover time\\n- Resource hosting can be implemented without the need to manually create TaskManager Pods, which can be automatically destroyed\\n- With more convenient HA, in Native Kubernetes mode after Flink version 1.12, you can rely on the Leader election mechanism of native Kubernetes to complete JobManager's HA\\n\\n  The main difference between Native Kubernetes and Standalone Kubernetes lies in the way Flink interacts with Kubernetes and the resulting series of advantages. Standalone Kubernetes requires users to customize the Kubernetes resource description files of JobManager and TaskManager. When submitting a job, you need to use kubectl combined with the resource description file to start the Flink cluster. The Native Kubernetes mode Flink Client integrates a Kubernetes Client, which can directly communicate with the Kubernetes API Server to complete the creation of JobManager Deployment and ConfigMap. After JobManager Development is created, the Resource Manager module in it can directly communicate with the Kubernetes API Server to complete the creation and destruction of TaskManager pods and the elastic scaling of Taskmanager. Therefore, it is recommended to use Flink on Native Kubernetes mode to deploy Flink tasks in production environments.\\n\\n![](/blog/relx/nativekubernetes_architecture.png)\\n\\nWhen Flink On Kubernetes meets StreamPark\\n\\n  Flink on Native Kubernetes currently supports Application mode and Session mode. Compared with the two, Application mode deployment avoids the resource isolation problem and client resource consumption problem of Session mode. Therefore, it is recommended to use Application Mode to deploy Flink tasks in ** production environments. **Let\u2019s take a look at the method of using the original script and the process of using StreamPark to develop and deploy a Flink on Native Kubernetes job.\\nDeploy Kubernetes using scripts\\n\\nIn the absence of a platform that supports Flink on Kubernetes task development and deployment, you need to use scripts to submit and stop tasks. This is also the default method provided by Flink. The specific steps are as follows:\\n\\n1. Prepare the kubectl and Docker command running environment on the Flink client node, create the Kubernetes Namespace and Service Account used to deploy the Flink job, and perform RBAC\\n2. Write a Dockerfile file to package the Flink base image and the user\u2019s job Jar together\\n\\n```dockerfile\\n\\nFROM flink:1.13.6-scala_2.11\\nRUN mkdir -p $FLINK_HOME/usrlib\\nCOPY my-flink-job.jar $FLINK_HOME/usrlib/my-flink-job.jar\\n```\\n\\n3. Use Flink client script to start Flink tasks\\n\\n```shell\\n\\n./bin/flink run-application \\\\\\n    --target kubernetes-application \\\\\\n    -Dkubernetes.namespace=flink-cluster \\\\\\n    -Dkubernetes.jobmanager.service-account=default \\\\\\n    -Dkubernetes.cluster-id=my-first-application-cluster \\\\\\n    -Dkubernetes.container.image=relx_docker_url/streamx/relx_flink_1.13.6-scala_2.11:latest \\\\\\n    -Dkubernetes.rest-service.exposed.type=NodePort \\\\\\n    local:///opt/flink/usrlib/my-flink-job.jar\\n```\\n\\n4. Use the Kubectl command to obtain the WebUI access address and JobId of the Flink job.\\n\\n```shell\\nkubectl -n flink-cluster get svc\\n```\\n\\n5. Stop the job using Flink command\\n\\n```shell\\n./bin/flink cancel\\n    --target kubernetes-application\\n    -Dkubernetes.cluster-id=my-first-application-cluster\\n    -Dkubernetes.namespace=flink-cluster <jobId>\\n```\\n\\n  The above is the process of deploying a Flink task to Kubernetes using the most original script method provided by Flink. Only the most basic task submission is achieved. If it is to reach the production use level, there are still a series of problems that need to be solved, such as: the method is too Originally, it was unable to adapt to large batches of tasks, unable to record task checkpoints and real-time status tracking, difficult to operate and monitor tasks, had no alarm mechanism, and could not be managed in a centralized manner, etc.\\n\\n## **Deploy Flink on Kubernetes using Apache StreamPark\u2122**\\n\\n  There will be higher requirements for using Flink on Kubernetes in enterprise-level production environments. Generally, you will choose to build your own platform or purchase related commercial products. No matter which solution meets the product capabilities: large-scale task development and deployment, status tracking, operation and maintenance monitoring , failure alarms, unified task management, high availability, etc. are common demands.\\n\\n  In response to the above issues, we investigated open source projects in the open source field that support the development and deployment of Flink on Kubernetes tasks. During the investigation, we also encountered other excellent open source projects. After comprehensively comparing multiple open source projects, we came to the conclusion: **StreamPark has great performance in either completness, user experience, or stability, so we finally chose StreamPark as our one-stop real-time computing platform. **\\n\\n  Let\u2019s take a look at how StreamPark supports Flink on Kubernetes:\\n\\n### **Basic environment configuration**\\n\\n  Basic environment configuration includes Kubernetes and Docker repository information as well as Flink client information configuration. The simplest way for the Kubernetes basic environment is to directly copy the .kube/config of the Kubernetes node to the StreamPark node user directory, and then use the kubectl command to create a Flink-specific Kubernetes Namespace and perform RBAC configuration.\\n\\n```shell\\n# Create k8s namespace used by Flink jobs\\nkubectl create ns flink-cluster\\n# Bind RBAC resources to the default user\\nkubectl create clusterrolebinding flink-role-binding-default --clusterrole=edit --serviceaccount=flink-cluster:default\\n\\n```\\n\\nDocker account information can be configured directly in the Docker Setting interface:\\n\\n![](/blog/relx/docker_setting.png)\\n\\nStreamPark can adapt to multi-version Flink job development. The Flink client can be configured directly on the StreamPark Setting interface:\\n\\n![](/blog/relx/flinkversion_setting.png)\\n\\n### **Job development**\\n\\nAfter StreamPark has configured the basic environment, it only takes three steps to develop and deploy a Flink job:\\n\\n![](/blog/relx/development_process.png)\\n\\n  StreamPark supports both Upload Jar and direct writing of Flink SQL jobs. **Flink SQL jobs only need to enter SQL and dependencies. This method greatly improves the development experience and avoids problems such as dependency conflicts.** This article does not focus on this part\u3002\\n\\n  Here you need to select the deployment mode as kubernetes application, and configure the following parameters on the job development page: The parameters in the red box are the basic parameters of Flink on Kubernetes.\\n\\n![](/blog/relx/kubernetes_base_parameters.png)\\n\\n  The following parameters are parameters related to Flink jobs and resources. The choice of Resolve Order is related to the code loading mode. For jobs uploaded by the Upload Jar developed by the DataStream API, choose to use Child-first, and for Flink SQL jobs, choose to use Parent-first loading.\\n\\n![](/blog/relx/flink_parameters.png)\\n\\n  Finally, there are the following two heavyweight parameters. For Native Kubernetes, k8s-pod-template generally only requires pod-template configuration. Dynamic Option is a supplement to the pod-template parameters. For some personalized configurations, you can Configured in Dynamic Option. For more Dynamic Option, please directly refer to the Flink official website.\\n\\n![](/blog/relx/pod_template.png)\\n\\n### **Job online**\\n\\nAfter the job development is completed, the job comes online. In this step, StreamPark has done a lot of work, as follows:\\n\\n- Prepare environment\\n- Dependency download in job\\n- Build job (JAR package)\\n- Build image\\n- Push the image to the remote repository\\n\\n**For users: Just click the cloud-shaped online button in the task list**\\n\\n![](/blog/relx/operation.png)\\n\\nWe can see a series of work done by StreamPark when building and pushing the image: **Read the configuration, build the image, and push the image to the remote repository...** I want to give StreamPark a big thumbs up!\\n\\n![](/blog/relx/step_details.png)\\n\\n### **Assignment submission**\\n\\n  Finally, you only need to click the start Application button in Operation to start a Flink on Kubernetes job. After the job is successfully started, click on the job name to jump to the Jobmanager WebUI page. The whole process is very simple and smooth.\\n\\n![](/blog/relx/homework_submit.png)\\n\\n  The entire process only requires the above three steps to complete the development and deployment of a Flink on Kubernetes job on StreamPark. StreamPark's support for Flink on Kubernetes goes far beyond simply submitting a task.\\n\\n### **Job management**\\n\\n**After the job is submitted, StreamPark can obtain the latest checkpoint address of the task, the running status of the task, and the real-time resource consumption information of the cluster in real time. It can very conveniently start and stop the running task with one click, and supports recording the savepoint location when stopping the job. , as well as functions such as restoring the state from savepoint when restarting, thus ensuring the data consistency of the production environment and truly possessing the one-stop development, deployment, operation and maintenance monitoring capabilities of Flink on Kubernetes.**\\n\\nNext, let\u2019s take a look at how StreamPark supports this capability:\\n\\n- **Record checkpoint in real time**\\n\\n  After the job is submitted, sometimes it is necessary to change the job logic but to ensure data consistency, then the platform needs to have the ability to record the location of each checkpoint in real time, as well as the ability to record the last savepoint location when stopped. StreamPark is on Flink on Kubernetes This function is implemented very well. By default, checkpoint information will be obtained and recorded in the corresponding table every 5 seconds, and according to the policy of retaining the number of checkpoints in Flink, only state.checkpoints.num-retained will be retained, and the excess will be deleted. There is an option to check the savepoint when the task is stopped. If the savepoint option is checked, the savepoint operation will be performed when the task is stopped, and the specific location of the savepoint will also be recorded in the table.\\n\\n  The root path of the default savepoint only needs to be configured in the Flink Home flink-conf.yaml file to automatically identify it. In addition to the default address, the root path of the savepoint can also be customized and specified when stopping.\\n\\n![](/blog/relx/savepoint.png)\\n\\n![](/blog/relx/checkpoint.png)\\n\\n- **Track running status in real time**\\n\\n  For challenges in the production environment, a very important point is whether monitoring is in place, especially for Flink on Kubernetes. This is very important and is the most basic capability. StreamPark can monitor the running status of Flink on Kubernetes jobs in real time and display it to users on the platform. Tasks can be easily retrieved based on various running statuses on the page.\\n\\n![](/blog/relx/run_status.png)\\n\\n- **Complete alarm mechanism**\\n\\n  In addition, StreamPark also has complete alarm functions: supporting email, DingTalk, WeChat and SMS, etc. This is also an important reason why the company chose StreamPark as the one-stop platform for Flink on Kubernetes after initial research.\\n\\n![](/blog/relx/alarm.png)\\n\\n  From the above, we can see that StreamPark has the capabilities to support the development and deployment process of Flink on Kubernetes, including: ** job development capabilities, deployment capabilities, monitoring capabilities, operation and maintenance capabilities, exception handling capabilities, etc. StreamPark provides a relatively complete set of s solution. And it already has some CICD/DevOps capabilities, and the overall completion level continues to improve. It is a product that supports the full link of Flink on Kubernetes one-stop development, deployment, operation and maintenance work in the entire open source field. StreamPark is worthy of praise. **\\n\\n## **Apache StreamPark\u2122\u2019s implementation in Wuxin Technology**\\n\\n  StreamPark was launched late in Wuxin Technology. It is currently mainly used for the development and deployment of real-time data integration jobs and real-time indicator calculation jobs. There are Jar tasks and Flink SQL tasks, all deployed using Native Kubernetes; data sources include CDC, Kafka, etc., and Sink end There are Maxcompute, kafka, Hive, etc. The following is a screenshot of the company's development environment StreamPark platform:\\n\\n![](/blog/relx/screenshot.png)\\n\\n## Problems encountered\\n\\n  Any new technology has a process of exploration and fall into pitfalls. The experience of failure is precious. Here are some pitfalls and experiences that StreamPark has stepped into during the implementation of fog core technology. **The content of this section is not only about StreamPark. I believe it will bring some reference to all friends who use Flink on Kubernetes**.\\n\\n### **FAQs are summarized below**\\n\\n- **Kubernetes pod failed to pull the image**\\n\\n  The main problem is that Kubernetes pod-template lacks docker\u2019s imagePullSecrets\\n\\n- **Scala version inconsistent**\\n\\n  Since StreamPark deployment requires a Scala environment, and Flink SQL operation requires the Flink SQL Client provided by StreamPark, it is necessary to ensure that the Scala version of the Flink job is consistent with the Scala version of StreamPark.\\n\\n- **Be aware of class conflicts**\\n\\n  When developing Flink SQL jobs, you need to pay attention to whether there are any class conflicts between the Flink image and the Flink connector and UDF. The best way to avoid class conflicts is to make the Flink image and the commonly used Flink connector and user UDF into a usable basic image. After that, other Flink SQL jobs can be reused directly.\\n\\n- **How to store checkpoint without Hadoop environment?**\\n\\n  HDFS, Alibaba Cloud OSS/AWS S3 can both perform checkpoint and savepoint storage. The Flink basic image already has support for OSS and S3. If you do not have HDFS, you can use Alibaba Cloud OSS or S3 to store status and checkpoint and savepoint data. You only need to use Flink Simply configure it in the dynamic parameters.\\n\\n```shell\\n\\n-Dstate.backend=rocksdb\\n-Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-oss-fs-hadoop-1.13.6.jar\\n-Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-oss-fs-hadoop-1.13.6.jar\\n-Dfs.oss.endpoint=xxyy.aliyuncs.com\\n-Dfs.oss.accessKeyId=xxxxxxxxxx\\n-Dfs.oss.accessKeySecret=xxxxxxxxxx\\n-Dstate.checkpoints.dir=oss://realtime-xxx/streamx/dev/checkpoints/\\n-Dstate.savepoints.dir=oss://realtime-xxx/streamx/dev/savepoints/\\n```\\n\\n- **The changed code did not take effect after it was republished**\\n\\n  This issue is related to the Kubernetes pod image pull policy. It is recommended to set the Pod image pull policy to Always:\\n\\n```shell\\n\u200d-Dkubernetes.container.image.pull-policy=Always\\n```\\n\\n- **Each restart of the task will result in one more Job instance**\\n\\n  Under the premise that kubernetes-based HA is configured, when you need to stop the Flink task, you need to use cancel of StreamPark. Do not delete the Deployment of the Flink task directly through the kubernetes cluster. Because Flink's shutdown has its own shutdown process, when deleting a pod, the corresponding configuration files in the Configmap will also be deleted. Direct deletion of the pod will result in the remnants of the Configmap. When a task with the same name is restarted, two identical jobs will appear because at startup, the task will load the remaining configuration files and try to restore the closed task.\\n\\n- **How to implement kubernetes pod domain name access**\\n\\n  Domain name configuration only needs to be configured in pod-template according to Kubernetes resources. I can share with you a pod-template.yaml template that I summarized based on the above issues:\\n\\n```yaml\\n\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-template\\nspec:\\n  serviceAccount: default\\n  containers:\\n  - name: flink-main-container\\n    image:\\n  imagePullSecrets:\\n  - name: regsecret\\n  hostAliases:\\n    - ip: \\\"192.168.0.1\\\"\\n      hostnames:\\n      - \\\"node1\\\"\\n    - ip: \\\"192.168.0.2\\\"\\n      hostnames:\\n      - \\\"node2\\\"\\n    - ip: \\\"192.168.0.3\\\"\\n      hostnames:\\n      - \\\"node3\\\"\\n\\n```\\n\\n### **Best Practices**\\n\\n  Many of RELX's big data components are based on Alibaba Cloud, such as Maxcompute and Alibaba Cloud Redis. At the same time, our Flink SQL jobs need to use some UDFs. At first, we adopted the method of using Flink Base image + maven dependency + upload udf jar, but in practice we encountered some problems such as version conflicts and class conflicts. At the same time, if it is a large-volume job, the development efficiency of this method is relatively low. Finally, we packaged the commonly used Flink connectors, udf and Flink base image at the company level into a company-level base image. New Flink SQL jobs can directly write Flink SQL after using this base image, which greatly improves development efficiency.\\n\\n**Let\u2019s share a simple step to create a basic image\uff1a**\\n\\n**1. Prepare the required JAR**\\n\\nPlace the commonly used Flink Connector Jar and the user Udf Jar in the same folder lib. The following are some commonly used connector packages in Flink 1.13.6\\n\\n```jar\\nbigdata-udxf-1.0.0-shaded.jar\\nflink-connector-jdbc_2.11-1.13.6.jar\\nflink-sql-connector-kafka_2.11-1.13.6.jar\\nflink-sql-connector-mysql-cdc-2.0.2.jar\\nhudi-flink-bundle_2.11-0.10.0.jar\\nververica-connector-odps-1.13-vvr-4.0.7.jar\\nververica-connector-redis-1.13-vvr-4.0.7.jar\\n```\\n\\n**2. Prepare Dockerfile**\\n\\nCreate a Dockerfile file and place the Dockerfile file in the same folder as the above folder\\n\\n```shell\\nFROM flink:1.13.6-scala_2.11COPY lib $FLINK_HOME/lib/\\n```\\n\\n**3. Create a basic image and push it to a private repository**\\n\\n```shell\\ndocker login --username=xxxdocker \\\\\\nbuild -t udf_flink_1.13.6-scala_2.11:latest \\\\\\n.docker tag udf_flink_1.13.6-scala_2.11:latest \\\\\\nk8s-harbor.xxx.com/streamx/udf_flink_1.13.6-scala_2.11:latestdocker \\\\\\npush k8s-harbor.xxx.com/streamx/udf_flink_1.13.6-scala_2.11:latest\\n```\\n\\n##  **Future Expectations**\\n\\n- **StreamPark supports Flink job metric monitoring**\\n\\n  It would be great if StreamPark could connect to Flink Metric data and display Flink\u2019s real-time consumption data at every moment on the StreamPark platform.\\n\\n- **StreamPark supports Flink job log persistence**\\n\\n  For Flink deployed to YARN, if the Flink program hangs, we can go to YARN to view the historical logs. However, for Kubernetes, if the program hangs, the Kubernetes pod will disappear and there will be no way to check the logs. Therefore, users need to use tools on Kubernetes for log persistence. It would be better if StreamPark supports the Kubernetes log persistence interface.\\n\\n- **Improvement of the problem of too large image**\\n\\n  StreamPark's current image support for Flink on Kubernetes jobs is to combine the basic image and user code into a Fat image and push it to the Docker repository. The problem with this method is that it takes a long time when the image is too large. It is hoped that the basic image can be restored in the future. There is no need to hit the business code together every time, which can greatly improve development efficiency and save costs.\"},{\"id\":\"flink-development-framework-streampark\",\"metadata\":{\"permalink\":\"/blog/flink-development-framework-streampark\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/1-flink-framework-streampark.md\",\"source\":\"@site/blog/1-flink-framework-streampark.md\",\"title\":\"Apache StreamPark\u2122 - Powerful Flink Development Framework\",\"description\":\"Although the Hadoop system is widely used today, its architecture is complicated, it has a high maintenance complexity, version upgrades are challenging, and due to departmental reasons, data center scheduling is prolonged. We urgently need to explore agile data platform models. With the current popularization of cloud-native architecture and the integration between lake and warehous, we have decided to use Doris as an offline data warehouse and TiDB (which is already in production) as a real-time data platform. Furthermore, because Doris has ODBC capabilities on MySQL, it can integrate external database resources and uniformly output reports.\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"DataStream\",\"permalink\":\"/blog/tags/data-stream\"},{\"label\":\"FlinkSQL\",\"permalink\":\"/blog/tags/flink-sql\"}],\"readingTime\":10.225,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"flink-development-framework-streampark\",\"title\":\"Apache StreamPark\u2122 - Powerful Flink Development Framework\",\"tags\":[\"StreamPark\",\"DataStream\",\"FlinkSQL\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Apache StreamPark\u2122 Flink on Kubernetes practice\",\"permalink\":\"/blog/streampark-flink-on-k8s\"},\"nextItem\":{\"title\":\"Ziroom implements the best practice of one-key data input into the lake based on Apache StreamPark\u2122 + Paimon\",\"permalink\":\"/blog/streampark-flink-with-paimon-in-ziru\"}},\"content\":\"Although the Hadoop system is widely used today, its architecture is complicated, it has a high maintenance complexity, version upgrades are challenging, and due to departmental reasons, data center scheduling is prolonged. We urgently need to explore agile data platform models. With the current popularization of cloud-native architecture and the integration between lake and warehous, we have decided to use Doris as an offline data warehouse and TiDB (which is already in production) as a real-time data platform. Furthermore, because Doris has ODBC capabilities on MySQL, it can integrate external database resources and uniformly output reports.\\n\\n![](/blog/belle/doris.png)\\n\\n\x3c!-- truncate --\x3e\\n\\n# 1. Background\\n\\nAlthough the Hadoop system is widely used today, its architecture is complicated, it has a high maintenance complexity, version upgrades are challenging, and due to departmental reasons, data center scheduling is prolonged. We urgently need to explore agile data platform models. With the current popularization of cloud-native architecture and the integration between lake and warehous, we have decided to use Doris as an offline data warehouse and TiDB (which is already in production) as a real-time data platform. Furthermore, because Doris has ODBC capabilities on MySQL, it can integrate external database resources and uniformly output reports.\\n\\n![](/blog/belle/doris.png)\\n\\n<center style={{\\\"color\\\": \\\"gray\\\"}}>(Borrowing Doris's official architecture diagram here)</center>\\n\\n<br/><br/>\\n\\n# 2. Challenges Faced\\n\\nFor the data engine, we settled on using Spark and Flink:\\n\\n* Use Spark on K8s client mode for offline data processing.\\n* Use Flink on K8s Native-Application/Session mode for real-time task stream management.\\n\\nHere, there are some challenges we haven't fully resolved:\\n\\nThose who have used the Native-Application mode know that each time a task is submitted, a new image must be packaged, pushed to a private repository, and then the Flink Run command is used to communicate with K8s to pull the image and run the Pod. After the task is submitted, you need to check the log on K8s, but:\\n\\n1. How is task runtime monitoring handled?\\n2. Do you use Cluster mode or expose ports using NodePort to access Web UI?\\n3. Can the task submission process be simplified to avoid image packaging?\\n4. How can we reduce the pressure on developers?\\n\\n<br/><br/>\\n\\n# 3. Solving the Challenges\\n\\nAll of the above are challenges that need addressing. If we rely solely on the command line to submit each task, it becomes unrealistic. As the number of tasks increases, it becomes unmanageable. Addressing these challenges became inevitable.\\n\\n<br/>\\n\\n## Simplifying Image Building\\n\\nFirstly, regarding the need for a secondary build of the native Flink image: we utilized MinIO as external storage and mounted it directly on each host node using s3-fuse via DaemonSet. The jar packages we need to submit can all be managed there. In this way, even if we scale the Flink nodes up or down, S3 mounts can automatically scale.\\n\\n![](/blog/belle/k8s.png)\\n\\nFrom Flink version 1.13 onwards, Pod Template support has been added. We can use volume mounts in the Pod Template to mount host directories into each pod, allowing Flink programs to run directly on K8s without packaging them into images. As shown in the diagram above, we first mount S3 using the s3-fuse Pod to the `/mnt/data-s3fs` directory on Node 1 and Node 2, and then mount `/mnt/data-s3fs` into Pod A.\\n\\nHowever, because object storage requires the entire object to be rewritten for random writes or file appends, this method is only suitable for frequent reads. This perfectly fits our current scenario.\\n\\n<br/>\\n\\n## Introducing Apache StreamPark\u2122\\n\\nPreviously, when we wrote Flink SQL, we generally used Java to wrap SQL, packed it into a jar package, and submitted it to the S3 platform through the command line. This approach has always been unfriendly; the process is cumbersome, and the costs for development and operations are too high. We hoped to further streamline the process by abstracting the Flink TableEnvironment, letting the platform handle initialization, packaging, and running Flink tasks, and automating the building, testing, and deployment of Flink applications.\\n\\nThis is an era of open-source uprising. Naturally, we turned our attention to the open-source realm: among numerous open-source projects, after comparing various projects, we found that both Zeppelin and StreamPark provide substantial support for Flink and both claim to support Flink on K8s. Eventually, both were shortlisted for our selection. Here's a brief comparison of their support for K8s (if there have been updates since, please kindly correct).\\n\\n<table>\\n    <thead>\\n        <tr>\\n            <td>Feature</td>\\n            <td>Zeppelin</td>\\n            <td>StreamPark</td>\\n        </tr>\\n    </thead>\\n    <tbody>\\n        <tr>\\n            <td>Task Status Monitoring</td>\\n            <td>Somewhat limited, not suitable as a task status monitoring tool.</td>\\n            <td>Highly capable</td>\\n        </tr>\\n        <tr>\\n            <td>Task Resource Management</td>\\n            <td>None</td>\\n            <td>Exists, but the current version is not very robust.</td>\\n        </tr>\\n        <tr>\\n            <td>Local Deployment</td>\\n            <td>On the lower side. In on K8s mode, you can only deploy Zeppelin in K8s. Otherwise, you need to connect the Pod and external network, which is rarely done in production.</td>\\n            <td>Can be deployed locally</td>\\n        </tr>\\n        <tr>\\n            <td>Multi-language Support</td>\\n            <td>High - Supports multiple languages such as Python/Scala/Java.</td>\\n            <td>Average - Currently, K8s mode and YARN mode support FlinkSQL, and based on individual needs, you can use Java/Scala to develop DataStream.</td>\\n        </tr>\\n        <tr>\\n            <td>Flink WebUI Proxy</td>\\n            <td>Currently not very comprehensive. The main developer is considering integrating Ingress.</td>\\n            <td>Good - Currently supports ClusterIp/NodePort/LoadBalance modes.</td>\\n        </tr>\\n        <tr>\\n            <td>Learning Curve</td>\\n            <td>Low cost. Needs to learn additional parameters, which differ somewhat from native FlinkSQL.</td>\\n            <td>No cost. In K8s mode, FlinkSQL is supported in its native SQL format; also supports Custom-Code (user writes code for developing Datastream/FlinkSQL tasks).</td>\\n        </tr>\\n        <tr>\\n            <td>Support for Multiple Flink Versions</td>\\n            <td>Supported</td>\\n            <td>Supported</td>\\n        </tr>\\n        <tr>\\n            <td>Intrusion into Native Flink Image</td>\\n            <td>Invasive. You need to pre-deploy the jar package in the Flink image, which will start in the same Pod as JobManager and communicate with the zeppelin-server.</td>\\n            <td>Non-invasive, but it will generate many images that need to be cleaned up regularly.</td>\\n        </tr>\\n        <tr>\\n            <td>Multi-version Code Management</td>\\n            <td>Supported</td>\\n            <td>Supported</td>\\n        </tr>\\n    </tbody>\\n</table>\\n\\n<center style={{\\\"color\\\": \\\"gray\\\"}}>(PS: This comparison is based on our perspective as evaluators. We hold the utmost respect for the developers of both platforms.)</center>\\n\\n<br/>\\n\\nDuring our research process, we communicated with the main developers of both tools multiple times. After our repeated studies and assessments, we eventually decided to adopt StreamPark as our primary Flink development tool for now.\\n\\n<video src=\\\"http://assets.streamxhub.com/streamx-video.mp4\\\" controls=\\\"controls\\\" width=\\\"100%\\\" height=\\\"100%\\\"></video>\\n\\n<center style={{\\\"color\\\": \\\"gray\\\"}}>(StreamPark's official splash screen)</center>\\n\\n<br/>\\n\\nAfter extended development and testing by our team, StreamPark currently boasts:\\n\\n* Comprehensive <span style={{\\\"color\\\": \\\"red\\\"}}>SQL validation capabilities</span>\\n* It has achieved <span style={{\\\"color\\\": \\\"red\\\"}}>automatic build/push for images</span>\\n* Using a custom class loader and through the Child-first loading method, it <span style={{\\\"color\\\": \\\"red\\\"}}>addresses both YARN and K8s operational modes</span> and <span style={{\\\"color\\\": \\\"red\\\"}}>supports the seamless switch between multiple Flink versions</span>\\n* It deeply integrates with Flink-Kubernetes, returning a WebUI after task submission, and via remote REST API + remote K8s, it can <span style={{\\\"color\\\": \\\"red\\\"}}>track task execution status</span>\\n* It supports versions like <span style={{\\\"color\\\": \\\"red\\\"}}>Flink 1.12, 1.13, 1.14, and more</span>\\n\\nThis effectively addresses most of the challenges we currently face in development and operations.\\n\\n<video src=\\\"http://assets.streamxhub.com/streamx-1.2.0.mp4\\\" controls=\\\"controls\\\" width=\\\"100%\\\" height=\\\"100%\\\"></video>\\n\\n<center style={{\\\"color\\\": \\\"gray\\\"}}>(Demo video showcasing StreamPark's support for multiple Flink versions)</center>\\n\\n<br/>\\n\\nIn its latest release, version 1.2.0, StreamPark provides robust support for both K8s-Native-Application and K8s-Session-Application modes.\\n\\n<video src=\\\"http://assets.streamxhub.com/streamx-k8s.mp4\\\" controls=\\\"controls\\\" width=\\\"100%\\\" height=\\\"100%\\\"></video>\\n\\n<center style={{\\\"color\\\": \\\"gray\\\"}}>(StreamPark's K8s deployment demo video)</center>\\n\\n<br/>\\n\\n### K8s Native Application Mode\\n\\nWithin StreamPark, all we need to do is configure the relevant parameters, fill in the corresponding dependencies in the Maven POM, or upload the dependency jar files. Once we click on 'Apply', the specified dependencies will be generated. This implies that we can also compile all the UDFs we use into jar files, as well as various connector.jar files, and use them directly in SQL. As illustrated below:\\n\\n![](/blog/belle/dependency.png)\\n\\nThe SQL validation capability is roughly equivalent to that of Zeppelin:\\n\\n![](/blog/belle/sqlverify.png)\\n\\nWe can also specify resources, designate dynamic parameters within Flink Run as Dynamic Options, and even integrate these parameters with a Pod Template.\\n\\n![](/blog/belle/pod.png)\\n\\nAfter saving the program, when clicking to run, we can also specify a savepoint. Once the task is successfully submitted, StreamPark will, based on the FlinkPod's network Exposed Type (be it loadBalancer, NodePort, or ClusterIp), return the corresponding WebURL, seamlessly enabling a WebUI redirect. However, as of now, due to security considerations within our online private K8s cluster, there hasn't been a connection established between the Pod and client node network (and there's currently no plan for this). Hence, we only employ NodePort. If the number of future tasks increases significantly, and there's a need for ClusterIP, we might consider deploying StreamPark in K8s or further integrate it with Ingress.\\n\\n![](/blog/belle/start.png)\\n\\nNote: If the K8s master uses a vip for load balancing, the Flink 1.13 version will return the vip's IP address. This issue has been rectified in the 1.14 version.\\n\\nBelow is the specific submission process in the K8s Application mode:\\n\\n![](/blog/belle/flow.png)\\n\\n<center style={{\\\"color\\\": \\\"gray\\\"}}>(The above is a task submission flowchart, drawn based on personal understanding. If there are inaccuracies, your understanding is appreciated.)</center>\\n\\n<br/>\\n\\n### K8s Native Session Mode\\n\\nStreamPark also offers robust support for the <span style={{\\\"color\\\": \\\"red\\\"}}> K8s Native-Session mode</span>, which lays a solid technical foundation for our subsequent offline FlinkSQL development or for segmenting certain resources.\\n\\nTo use the Native-Session mode, one must first use the Flink command to create a Flink cluster that operates within K8s. For instance:\\n\\n```shell\\n./kubernetes-session.sh \\\\\\n-Dkubernetes.cluster-id=flink-on-k8s-flinkSql-test \\\\\\n-Dkubernetes.context=XXX \\\\\\n-Dkubernetes.namespace=XXXX \\\\\\n-Dkubernetes.service-account=XXXX \\\\\\n-Dkubernetes.container.image=XXXX \\\\\\n-Dkubernetes.container.image.pull-policy=Always \\\\\\n-Dkubernetes.taskmanager.node-selector=XXXX \\\\\\n-Dkubernetes.rest-service.exposed.type=Nodeport\\n```\\n\\n![](/blog/belle/flinksql.png)\\n\\nAs shown in the image above, we use that ClusterId as the Kubernetes ClusterId task parameter for StreamPark. Once the task is saved and submitted, it quickly transitions to a 'Running' state:\\n\\n![](/blog/belle/detail.png)\\n\\nFollowing the application info's WebUI link:\\n\\n![](/blog/belle/dashboard.png)\\n\\nIt becomes evident that StreamPark essentially uploads the jar package to the Flink cluster through REST API and then schedules the task for execution.\\n\\n<br/>\\n\\n### Custom Code Mode\\n\\nTo our delight, StreamPark also provides support for coding DataStream/FlinkSQL tasks. For special requirements, we can achieve our implementations in Java/Scala. You can compose tasks following the scaffold method recommended by StreamPark or write a standard Flink task. By adopting this approach, we can delegate code management to git, utilizing the platform for automated compilation, packaging, and deployment. Naturally, if functionality can be achieved via SQL, we would prefer not to customize DataStream, thereby minimizing unnecessary operational complexities.\\n\\n<br/><br/>\\n\\n# 4. Feedback and Future Directions\\n\\n## Suggestions for Improvement\\n\\nStreamPark, similar to any other new tools, does have areas for further enhancement based on our current evaluations:\\n\\n* **Strengthening Resource Management**: Features like multi-file system jar resources and robust task versioning are still awaiting additions.\\n* **Enriching Frontend Features**: For instance, once a task is added, functionalities like copying could be integrated.\\n* **Visualization of Task Submission Logs**: The process of task submission involves loading class files, jar packaging, building and submitting images, and more. A failure at any of these stages could halt the task. However, error logs are not always clear, or due to some anomaly, the exceptions aren't thrown as expected, leaving users puzzled about rectifications.\\n\\nIt's a universal truth that innovations aren't perfect from the outset. Although minor issues exist and there are areas for improvement with StreamPark, its merits outweigh its limitations. As a result, we've chosen StreamPark as our Flink DevOps platform. We're also committed to collaborating with its main developers to refine StreamPark further. We wholeheartedly invite others to use it and contribute towards its advancement.\\n\\n<br/>\\n\\n## Future Prospects\\n\\n* We'll keep our focus on Doris and plan to unify business data with log data in Doris, leveraging Flink to realize lakehouse capabilities.\\n* Our next step is to explore integrating StreamPark with DolphinScheduler 2.x. This would enhance DolphinScheduler's offline tasks, and gradually we aim to replace Spark with Flink for a unified batch-streaming solution.\\n* Drawing from our own experiments with S3, after building the fat-jar, we're considering bypassing image building. Instead, we'll mount PVC directly to the Flink Pod's directory using Pod Template, refining the code submission process even further.\\n* We plan to persistently implement StreamPark in our production environment. Collaborating with community developers, we aim to boost StreamPark's Flink stream development, deployment, and monitoring capabilities. Our collective vision is to evolve StreamPark into a holistic stream data DevOps platform.\\n\\nResources:\\n\\nStreamPark GitHub: [https://github.com/apache/incubator-streampark](https://github.com/apache/incubator-streampark) <br/>\\nDoris GitHub: [https://github.com/apache/doris](https://github.com/apache/doris)\\n\\n![](/blog/belle/author.png)\"},{\"id\":\"streampark-flink-with-paimon-in-ziru\",\"metadata\":{\"permalink\":\"/blog/streampark-flink-with-paimon-in-ziru\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/10-streampark-flink-with-paimon-in-ziru.md\",\"source\":\"@site/blog/10-streampark-flink-with-paimon-in-ziru.md\",\"title\":\"Ziroom implements the best practice of one-key data input into the lake based on Apache StreamPark\u2122 + Paimon\",\"description\":\"Introduction\uff1aThis article mainly introduces the architecture upgrade and evolution of the self-migrating MySQL data to Hive, the original architecture involves many components, complex links, and encounters many challenges, and effectively solves the dilemmas and challenges encountered in data integration after using the combination of StreamPark + Paimon, and shares the specific practical solutions of StreamPark + Paimon in practical applications, as well as the advantages and benefits brought by this rookie combination solution.\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"}],\"readingTime\":13.36,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-flink-with-paimon-in-ziru\",\"title\":\"Ziroom implements the best practice of one-key data input into the lake based on Apache StreamPark\u2122 + Paimon\",\"tags\":[\"StreamPark\",\"Production Practice\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Apache StreamPark\u2122 - Powerful Flink Development Framework\",\"permalink\":\"/blog/flink-development-framework-streampark\"},\"nextItem\":{\"title\":\"Apache StreamPark\u2122 helps Tianyancha real-time platform construction\uff5cMultiple-fold increase in efficiency\",\"permalink\":\"/blog/streampark-usercase-tianyancha\"}},\"content\":\"![](/blog/ziru/new_cover.png)\\n\\n**Introduction**\uff1aThis article mainly introduces the architecture upgrade and evolution of the self-migrating MySQL data to Hive, the original architecture involves many components, complex links, and encounters many challenges, and effectively solves the dilemmas and challenges encountered in data integration after using the combination of StreamPark + Paimon, and shares the specific practical solutions of StreamPark + Paimon in practical applications, as well as the advantages and benefits brought by this rookie combination solution.\\n\\nStreamPark: https://github.com/apache/streampark\\n\\nPaimon: https://github.com/apache/paimon\\n\\nWelcome to follow, star, fork, and participate in contributions\\n\\nContributor\uff5cBeijing Ziru Information Technology Co., Ltd.\\n\\nAuthors of the article\uff5cLiu Tao, Liang Yansheng, Wei Linzi\\n\\nArticle compilation\uff5cYang Linwei\\n\\nContent proofreading\uff5cPan Yuepeng\\n\\n\x3c!-- truncate --\x3e\\n\\n## **1.Data integration business background**\\n\\nThe data integration scenario of Ziroom's rental business mainly comes from the need to synchronize MySQL tables of each business line to Hive tables. This requirement includes more than 4,400 MySQL business tables synchronized every day and more than 8,000 Hive ETL processing tasks. The amount of new data generated every day is 50T, and these numbers are still growing. According to the freshness requirements of the data, it is divided into two types: low freshness (T+1 day) and high freshness (T+10 minutes). More than 4,000 low freshness data tables are synchronously scheduled every day, and more than 400 high freshness data tables are synchronously scheduled every day. Freshness data table to ensure the timeliness and accuracy of data.\\n\\nZiroom's data integration solutions can be mainly divided into two types according to business usage scenarios:\\n\\n- **Low freshness**: The timeliness requirement of low freshness for data is **T+1day**, and the Hive jdbc handler is used to pull the full amount of MySQL data to Hive at 00:00 every day. The basic The process is shown in the figure below:\\n\\n  ![](/blog/ziru/low_freshness.png)\\n\\n- **High freshness**: In this scenario, the required data effectiveness is **T+10minutes**. We reused the snapshot pull method of the low freshness scenario to obtain the full amount of data and initialized it to MySQL. Synchronously use Canal to parse the logs and collect them into Kafka, then use Flink to read the data in kafka and write it to HDFS, and finally use Airflow for scheduling to merge the incremental data into Hive. The basic logic is as follows:\\n\\n  ![](/blog/ziru/high_freshness.png)\\n\\nHowever, there are many challenges and pressures in the current architecture. Firstly, the operation and maintenance costs are high, and secondly, the computing pressure, storage pressure and network pressure are all very high. In addition, although the resources are underutilized during the system running time from 0:00 to 1:00, other time periods face resource shortages. In this regard, Ziroom decided to update the data integration architecture to improve the efficiency and stability of the system.\\n\\n## **2.Challenges encountered**\\n\\nIn the above two scenarios, we encountered the following challenges during the data integration process:\\n\\n- **Network bandwidth overload problem**: Since the pull task reached **4000+**, too many mirror full data pulls put great pressure on the database network bandwidth.\\n\\n- **Inefficient resource utilization**: After the upstream data is synchronized from MySQL to the ODS layer table, the downstream processing table can be started. As a result, the CPU and memory resources of the Hadoop cluster are not available between 0:00 and 1:00. be fully utilized.\\n\\n- **High maintenance costs**: When the database table structure changes, the Airflow script needs to be modified simultaneously. Otherwise, incomplete fields will appear, causing online data anomalies.\\n\\n- **Difficulty in troubleshooting**: The data link is long. When data anomalies occur, troubleshooting costs are high. The problem may occur in any link in Canal, Kafka, Flink, and Airflow scheduling, resulting in a long recovery time. .\\n\\n- **Flink jobs are difficult to manage in a unified manner**: Flink itself does not provide good deployment and development capabilities. As the number of Flink tasks increases, the time cost of management and maintenance also increases.\\n\\nIn order to solve the above problems, after a series of investigations, we decided to adopt the \\\"**StreamPark+Paimon**\\\" strategy. So what are the reasons for choosing them? We can first look at their characteristics.\\n\\n### **Paimon\u2019s core features**\\n\\n**After research and comprehensive evaluation of several data lake frameworks such as Apache Hudi/Iceberg/Paimon, we decided to use Apache Paimon**. Apache Paimon is a streaming data lake storage technology that can provide users with high throughput and low cost. Delayed data ingestion, streaming subscription and real-time query capabilities support the use of Flink and Spark to build a real-time Lakehouse architecture, support batch/stream data processing operations, innovatively combine the Lake format with the LSM structure, and provide real-time streaming updates. Introducing into the Lake architecture has the following advantages:\\n\\n- **Unified batch and stream processing**: Paimon supports batch writing, batch reading and streaming operations, providing flexible data processing methods.\\n\\n- **Data Lake Features**: As a data lake storage system, Paimon has the characteristics of low cost, high reliability and scalable metadata.\\n\\n- **Rich merging engines**: Paimon provides a variety of merging engines, and you can choose to retain the latest data, perform partial updates, or perform aggregation operations according to your needs.\\n\\n- **Automatically generate change logs**: Paimon supports a variety of Changelog producers and can automatically generate correct and complete change logs to simplify streaming task analysis.\\n\\n- **Rich table types**: Paimon supports primary key tables and append-only tables, as well as multiple table types such as internal tables, external tables, partitioned tables, and temporary tables.\\n\\n- **Support table structure change synchronization**: When the data source table structure changes, Paimon can automatically identify and synchronize these changes.\\n\\nPaimon can be used in conjunction with Apache Spark. Our scenario is Paimon combined with Flink. In this way, \\\"**How to manage 4000+ Flink data synchronization jobs**\\\" will be a new problem we face. After a comprehensive investigation of related projects and a comprehensive evaluation of various dimensions, **we decided to use StreamPark**. So why did we choose StremaPark?\\n\\n### **StreamPark\u2019s core features**\\n\\nApache StreamPark is a stream processing development and management framework that provides a set of fast APIs for developing Flink/Spark jobs. In addition, it also provides a one-stop stream processing job development and management platform, covering the entire life cycle from stream processing job development to launch. Cycles are supported. StreamPark mainly includes the following core features:\\n\\n- **Stream processing application development framework**: Based on StreamPark, developers can easily build and manage stream processing applications, and better utilize Apache Flink\xae to write stream processing applications.\\n\\n- **Perfect management capabilities**: StreamPark provides a one-stop streaming task development and management platform that supports the full life cycle of Flink/Spark from application development to debugging, deployment, operation and maintenance, allowing Flink/Spark jobs to Make it simple.\\n\\n- **High degree of completion**: StreamPark supports multiple versions of Flink, allowing flexible switching of one platform. It also supports Flink\u2019s deployment mode, effectively solving the problem of too cumbersome Flink on YARN/K8s deployment. Through automated processes, It simplifies the process of building, testing and deploying tasks and improves development efficiency.\\n\\n- **Rich management API**: StreamPark provides APIs for job operations, including job creation, copy, build, deployment, stop and start based on checkpoint/savepoint, etc., making it easy to implement external system calls to Apache Flink\xae tasks. .\\n\\n## **3. StreamPark + Paimon Practice**\\n\\nNext, we will continue to share how Ziroom optimized the architecture based on **StreamPark + Paimon**. Let\u2019s first look at the comparison before and after the architecture upgrade.\\n\\n### **3.1 Before architecture upgrade**\\n\\nThe system interaction process of the data integration module before the transformation is as follows:\\n\\n![](/blog/ziru/system_interaction_process.png)\\n\\n**Step1** (User initiates access application): First, the user selects a table on the data access platform, and then clicks the Apply for Access button:\\n\\n![](/blog/ziru/data_access_platform.png)\\n\\n**Step2** (Initiate OA system approval process): When the OA system receives the access application, it will initiate workflow approval. If the approval fails, the application will be rejected. Only if the approval is passed will the next step be continued.\\n\\n**Step3** (The data access platform processes the approval event): The data access platform calls the Canal interface to deploy the Canal task and transfer the Binlog data in the table to Kafka:\\n\\n![](/blog/ziru/canal_job.png)\\n\\n**Step4** (Flink task deployment): Manually use the Flink Session Submit UI to deploy the Flink template job, which is responsible for parsing the Binlog change data in Kafka, writing the data to HDFS, and mapping it into a Hive external incremental table:\\n\\n![](/blog/ziru/flink_job.png)\\n\\n**Step5** (Airflow scheduling initialization table): Create Hive mapping incremental table and full table and use Airflow scheduling to complete the first initialization:\\n\\n![](/blog/ziru/first_initialization.png)\\n\\n**Step6** (Merge data into Hive full table): Configure scheduling to merge data from Hive external incremental tables into Hive full table through Hive Merge method\\n\\n![](/blog/ziru/data_merging.png)\\n\\n### **3.2 After the architecture upgrade**\\n\\nThe modified system interaction flow chart is as follows:\\n\\n![](/blog/ziru/after_the_transformation.png)\\n\\nComparing the interaction flow charts before and after the transformation, it can be seen that the process of the first two steps (table access application and approval) is the same, but the only difference is the event monitoring and processing method after the approval is passed.\\n\\n- **Before transformation**: Call the Canal interface to deploy Canal tasks (old logic);\\n\\n- **After transformation**: Call StreamPark\u2019s API interface to complete Flink Paimon\u2019s task deployment.\\n\\nNext, let\u2019s look at using Paimon to complete data integration.\\n\\n### **3.3 Paimon implements one-key Hive input**\\n\\nAfter Apache Paimon version 0.5, CDC data integration capabilities are provided. Data from MySQL, Kafka, Mongo, etc. can be easily ingested into Paimon in real time through the officially provided paimon-action jar. We are using paimon- flink-action uses **mysql-sync-database (whole database synchronization)** and selects the tables to be synchronized through the \\\"**\u2014including_tables**\\\" parameter. This synchronization mode effectively saves a lot of resource overhead. Compared with starting a Flink task for each table, it avoids a lot of waste of resources.\\n\\npaimon-flink-action provides the function of automatically creating Paimon tables and supports Schema Evolution (for example, when the MySQL table fields change, the Paimon table will change accordingly without additional operations). The entire operation process is efficient and smooth, which solves the unnecessary operation and maintenance costs caused by adding fields to the original architecture. The specific use of paimon-action is as follows. For more information, please refer to the Paimon official website documentation.\\n\\n```shell\\n<FLINK_HOME>/bin/flink run \\\\\\n/path/to/paimon-flink-action-0.8-SNAPSHOT.jar \\\\\\nmysql_sync_table\\n--warehouse <warehouse-path> \\\\\\n--database <database-name> \\\\\\n--table <table-name> \\\\\\n[--partition_keys <partition_keys>] \\\\\\n[--primary_keys <primary-keys>] \\\\\\n[--type_mapping <option1,option2...>] \\\\\\n[--computed_column <'column-name=expr-name(args[, ...])'> [--computed_column ...]] \\\\\\n[--metadata_column <metadata-column>] \\\\\\n[--mysql_conf <mysql-cdc-source-conf> [--mysql_conf <mysql-cdc-source-conf> ...]] \\\\\\n[--catalog_conf <paimon-catalog-conf> [--catalog_conf <paimon-catalog-conf> ...]] \\\\\\n[--table_conf <paimon-table-sink-conf> [--table_conf <paimon-table-sink-conf> ...]]\\n```\\n\\nThe introduction of Paimon shortens the entire data access link, eliminates dependence on Canal, Kafka, and Airflow, allowing MySQL to directly connect to Hive at minute speeds, and the overall environment is clean and efficient. In addition, Paimon is fully compatible with Hive-side data reading, with extremely low conversion costs and good compatibility with the use of original architecture scripts. Paimon also supports the Tag function, which can be regarded as a lightweight snapshot, significantly reducing storage costs.\\n\\n### **3.4 StreamPark + Paimon implementation practice**\\n\\nStreamPark has better support for JAR type jobs in version 2.1.2, making Paimon type data integration jobs simpler. The following example demonstrates how to use StreamPark to quickly develop Paimon data migration type jobs:\\n\\n//Video link (StreamPark deployment Paimon data integration job example)\\n\\nIt is true that StreamPark has specifically supported Paimon data integration jobs after version 2.1.2, but this method of manually creating jobs and entering job parameters still does not meet our actual needs. We need more flexible job creation, which can be done through Quickly complete job creation and startup by calling API... After our research, we found that StreamPark has completely opened various operation APIs for jobs, such as job copy, creation, deployment, start, stop, etc. In this way, we Through scheduled scheduling, it can be easily combined with StreamPark to quickly complete the development and operation of jobs. The specific methods are as follows:\\n\\n**Step1**\uff1aFirst, the api copy template interface will be called and parameters will be passed in. The relevant screenshots are as follows:\\n\\n![](/blog/ziru/api_copy.png)\\n\\nIn this way, the job will be created quickly, and the parameters will be passed in through the copy interface. The specific parameters are as follows:\\n\\n![](/blog/ziru/parameter.png)\\n\\n**Step2**\uff1aNext, call the API to build the job image:\\n\\n![](/blog/ziru/job_mirroring.png)\\n\\n**Step3**\uff1aContinue to call the API to start the job:\\n\\n![](/blog/ziru/start_the_job.png)\\n\\nFinally, after the task is successfully started, you can see the relevant status information of the task and the resource overview of the overall task on the StreamPark platform:\\n\\n![](/blog/ziru/resource_overview.png)\\n\\nYou can also click Application Name to schedule the Flink web UI:\\n\\n![](/blog/ziru/flink_ui.png)\\n\\nFinally, you can directly query the Paimon table on the Hive side to obtain the required data. The Paimon table is a data synchronization target table with low latency processed by Flink. The screenshot of Hive query Paimon table in Hue is as follows\\n\\n![](/blog/ziru/hive_query_paimon.png)\\n\\nThrough the above steps, the business party can easily initiate an access application. After the approval process, a Flink job is created and deployed through StreamPark. The data is entered into the Paimon table through the Flink job, allowing users to easily perform query operations on the Hive side. The entire process is simplified. User operation improves the efficiency and maintainability of data access.\\n\\n## **4. Benefits**\\n\\nBy using Paimon and StreamPark, Ziroom has brought the following advantages and benefits:\\n\\n- **Network resources and database pressure optimization**: By obtaining data snapshots directly from the Paimon table, the problem of network resource constraints and excessive database pressure caused by pulling data from the business database in the early morning is solved, while data storage costs are reduced.\\n\\n- **Job management interface improves efficiency**: Using StreamPark's job management interface easily solves the problem of manual deployment of Flink tasks, eliminates the situation where tasks are not deployed in time due to personnel dependence on time, improves efficiency, and reduces communication costs. Improved the management efficiency of Flink jobs.\\n\\n- **Reduce development and maintenance costs**: It solves the problems of high maintenance costs and slow problem location caused by long links in the previous solution. It also solves the problem of field inconsistency caused by field changes and realizes the unified flow and batch of data access. , reducing development and maintenance costs.\\n\\n- **Reduced usage costs of data integration scheduling resources and computing resources**: No longer relies on external scheduling systems for incremental data merging, reducing the use of scheduling resources. It no longer relies on Hive resources for merge operations, reducing the cost of merging incremental data on Hive computing resources.\\n\\n## **5. Conclusion & Expectations**\\n\\nWe would like to sincerely thank the **Apache StreamPark** community for their generous help as we use the StreamPark API. Their professional service spirit and user-oriented attitude allow us to use this powerful framework more efficiently and smoothly. As an excellent framework, StreamPark not only has excellent functions, but also supports a series of operations such as task copying, creation, deployment, start, stop and status monitoring, and has significantly played a significant role in simplifying the management and maintenance of Flink tasks. its value.\\n\\nAt the same time, we would like to express our gratitude to the **Apache Paimon** community for their patience and professional guidance during the testing phase of Ziroom MySQL into Paimon. This is an important support for us to successfully complete the test. As a project with great potential, Paimon has demonstrated extraordinary qualities both in terms of feature implementation and team collaboration.\\n\\nFinally, we are full of expectations and confidence in the future of Apache StreamPark and Apache Paimon, and firmly believe that they will become excellent projects for Apache in the future. Their excellent functions and harmonious community cooperation model have laid a solid foundation for their maturity in the open source community. We expect that the StreamPark and Paimon communities will continue to maintain a professional attitude and good teamwork spirit, continue to advance the development of the project, and become a model for new Apache projects in recent years, so as to have a greater impact on the broader developer community.\"},{\"id\":\"streampark-usercase-tianyancha\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-tianyancha\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/11-streampark-usercase-tianyancha.md\",\"source\":\"@site/blog/11-streampark-usercase-tianyancha.md\",\"title\":\"Apache StreamPark\u2122 helps Tianyancha real-time platform construction\uff5cMultiple-fold increase in efficiency\",\"description\":\"Introduction: This paper mainly introduces the challenges of job development and management faced by Tianyancha in the operation and maintenance of nearly 1,000 Flink jobs in the real-time computing business, and solves these challenges by introducing Apache StreamPark. It introduces some of the problems encountered in the process of introducing StreamPark, and how to solve these problems and successfully land the project, and finally greatly reduces operation and maintenance costs and significantly improve human efficiency.\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"}],\"readingTime\":10.05,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-tianyancha\",\"title\":\"Apache StreamPark\u2122 helps Tianyancha real-time platform construction\uff5cMultiple-fold increase in efficiency\",\"tags\":[\"StreamPark\",\"Production Practice\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Ziroom implements the best practice of one-key data input into the lake based on Apache StreamPark\u2122 + Paimon\",\"permalink\":\"/blog/streampark-flink-with-paimon-in-ziru\"},\"nextItem\":{\"title\":\"Apache StreamPark\u2122 Cloud Native Platform Practice at Joymaker\",\"permalink\":\"/blog/streampark-usercase-joymaker\"}},\"content\":\"![](/blog/tianyancha/new_cover.png)\\n\\n**Introduction**: This paper mainly introduces the challenges of job development and management faced by Tianyancha in the operation and maintenance of nearly 1,000 Flink jobs in the real-time computing business, and solves these challenges by introducing Apache StreamPark. It introduces some of the problems encountered in the process of introducing StreamPark, and how to solve these problems and successfully land the project, and finally greatly reduces operation and maintenance costs and significantly improve human efficiency.\\n\\nGithub: https://github.com/apache/streampark\\n\\nWelcome to follow, star, fork, and participate in contributions\\n\\nContributor | Beijing Tianyancha\\n\\nAuthor | Li Zhilin\\n\\nArticle compilation\uff5cYang Linwei\\n\\nContent proofreading\uff5cPan Yuepeng\\n\\n\x3c!-- truncate --\x3e\\n\\nTianyancha is China's leading business search platform. Since its inception in 2014, Tianyancha has continued to maintain rapid growth, with an industry penetration rate of more than 77%, a monthly active user base of 35 million, and a cumulative user base of 600 million+, making Tianyancha a super symbol in the business field, and one of the first companies to obtain the qualification of enterprise credit collection filed with the Central Bank. After years of deep cultivation, the platform has included 340 million social entities nationwide and 640 million globally, with more than 1,000 kinds of business information dimensions updated in real time, making it the preferred brand for business inquiries by both corporate clients and individual users.\\n\\n![](/blog/tianyancha/enterprise.png)\\n\\nIn this paper, we will introduce the challenges faced by the real-time computing business of Tianyancha, how these challenges are addressed by the Apache StreamPark platform, as well as the significant benefits and future plans.\\n\\n## **Business background and challenges**  \\n\\nTianyancha has a huge user base and diverse business dimensions, we use Apache Flink, a powerful real-time computing engine, to provide users with better products and service experience, our real-time computing business mainly covers the following scenarios:\\n\\n- Real-time data ETL processing and data transmission.\\n\\n- C-end dimensional data statistics, including user behavior, PV/UV analysis.\\n\\n- Real-time correlation analysis of C-end business data with business information, risk information, property rights, equity, shareholders, cases and other information.\\n\\nWith the rapid development of the business of Tianyancha, real-time operations have increased to 800+, and will soon be thousands in the future. Facing such a large volume of operations, we have encountered challenges in development and management, which are roughly as follows:\\n\\n- **Difficulty in job operation and maintenance**: Jobs cover multiple Flink versions, running and upgrading multi-version Flink jobs are costly due to API changes, and job submission and parameter tuning rely on complex scripts, which greatly increases the maintenance difficulty and cost.\\n\\n- **Inefficient job development**: From local IDE debugging to test environment deployment, it takes time-consuming and inefficient steps such as compilation, file transfer and command execution.\\n\\n- **Lack of automatic retry mechanism**: Multi-table correlation operation of real-time business is susceptible to network and IO fluctuations, which leads to Flink program failure and requires manual or additional program intervention for recovery, increasing development and maintenance costs.\\n\\n- **High Maintenance Costs**: Lack of centralized logging, version control, and configuration management tools makes maintaining a large number of JAR packages, commit scripts, and Flink client configurations increasingly complex as the task volume rises.\\n\\n![](/blog/tianyancha/challenges.png)\\n\\n## **StreamPark on-the-ground practice**  \\n\\nIn order to solve the above problems, our company decided to introduce StreamPark, we started to pay attention to the StreamPark project when it was first open-sourced, and the project name was StreamX at that time, and since then we have been closely following the development of the project, witnessing its development from open-sourcing to joining the incubation of the Apache Software Foundation, and we have seen that its community activity, user reputation, and the number of times of release have all been very good, which strengthens our confidence in StreamPark. Its community activity, user reputation, number of releases, etc. are all very good, and there are a lot of real use cases in enterprises, which strengthens our confidence in StreamPark. Therefore, introducing StreamPark is a natural thing to do, and we are especially interested in the following capabilities that can solve our pain points:\\n\\n- **Git-style project management**: provides a convenient version control and deployment process for jobs.\\n\\n- **Powerful job O&M capabilities**: significantly improves the efficiency of job management and maintenance.\\n\\n- **Precise second-level alarm system**: provides real-time monitoring and rapid response capability.\\n\\n- **One-stop task development platform**: provides strong protection for development, testing and deployment\\n\\n### **1. Git-style project management**\\n\\nTianyancha more than 90% are JAR jobs, management of hundreds of JAR jobs is a headache, in order to facilitate the maintenance and management of the previous we named the JAR file name through the \u201cdeveloper name-project name-module-environment\u201d, which brings another problem, there is no way to intuitively differentiate between the maintenance of the operation in that There is no way to visually distinguish which module the job is maintained in, and if a non-owner maintains the job, he or she has to go through the project documentation and code to see the logic, and needs to find the project code and the corresponding branch, the startup script of the job, etc., which increases the workload of the developer dramatically.\\n\\nIn StreamPark, we were pleasantly surprised to find a project management feature, similar to the Git integration, that solved our pain point by simplifying the naming convention to isolate projects, people, branches and environments. Now, we can instantly locate the code and documents of any job, which greatly improves the maintainability and development efficiency of jobs, and significantly improves our operation and maintenance efficiency.\\n\\n![](/blog/tianyancha/git.png)\\n\\n### **2. Seconds-accurate alerts and automated O&M**\\n\\nBelow is one of our Flink online tasks, during the query process to access the third-party side, Connection Time out and other errors occurred (Note: itself Flink set the restart policy interval restart three times, but ultimately failed)\\n\\n![](/blog/tianyancha/alarm.png)\\n\\nAs shown above, when the job alert was received, and our fellow developers responded to 1 in the group, we realized that the **job had been successfully pulled up by StreamPark, and it took less than 15 seconds to resubmit the job and run it**!\\n\\n### **3. Platform user rights control**\\n\\nStreamPark provides a complete set of permission management mechanism, which allows us to manage the fine-grained permissions of jobs and users on the StreamPark platform, effectively avoiding possible misoperation due to excessive user permissions, and reducing the impact on the stability of job operation and the accuracy of the environment configuration, which is very necessary for enterprise-level users.\\n\\n![](/blog/tianyancha/permissions.png)\\n\\n### **4. Flink SQL online development**\\n\\nFor many development scenarios where Flink SQL jobs need to be submitted, we can now completely eliminate the tedious steps of writing code in IDEA. Simply use the StreamPark platform,**which makes it easy to develop Flink SQL jobs**, and here's a simple example.\\n\\n![](/blog/tianyancha/sql_job.png)\\n\\n### **5. Well-established operations management capabilities**\\n\\nStreamPark also has more complete job operation records, for some abnormal jobs at the time of submission, you can now view the failure log completely through the platform, including historical log information.\\n\\n![](/blog/tianyancha/historical_logs.png)\\n\\nOf course, StreamPark also provides us with a more complete and user-friendly task management interface to visualize how tasks are running.\\n\\n![](/blog/tianyancha/task_management.png)\\n\\nOur internal real-time projects are highly abstracted code development jobs, and each time we develop a new job, we only need to modify some external incoming parameters. With the StreamPark replicated job feature, we can say goodbye to a lot of repetitive work, and just replicate the existing job on the platform, and slightly modify the running parameters of the program in order to submit a new job!\\n\\n![](/blog/tianyancha/successful.png)\\n\\nOf course, there are more, not to list here, StreamPark in the use of simple enough to use, within minutes you can get started to use, the function is focused enough to be reliable, the introduction of StreamPark to solve the problem we face in managing Flink jobs, bringing real convenience, significantly improve the efficiency of the people, to give the creative team a great praise!\\n\\n## **Problems encountered**\\n\\nIn the StreamPark landing practice, we also encountered some problems, which are documented here, expecting to bring some inputs and references to the users in the community.\\n\\n### **Huawei Cloud has compatibility issues with open source Hadoop**\\n\\nOur jobs are in Flink on Yarn mode and deployed in Huawei Cloud. During the process of using StreamPark to deploy jobs, we found that the jobs can be successfully deployed to Huawei Hadoop cluster, but the request for Yarn ResourceManager was rejected when getting the job status information, so we communicated with the community in time to find a solution, and logged the issue.\\nhttps://github.com/apache/incubator-streampark/issues/3566\\n\\nWe communicated with the community to seek a solution, and recorded the issue: . Finally, with the assistance of Mr. Huajie, a community PMC member, and Huawei Cloud students, we successfully resolved the issue: StreamPark could not access the cloud ResourceManager properly due to the internal security authentication mechanism of the cloud product itself.\\n\\nSolution (for reference only - add different dependencies according to different environments)\\n\\nRemove two HADOOP related packages from the STREAMPARK_HOME/lib directory:\\n\\n```shell\\nhadoop-client-api-3.3.4.jar\\nhadoop-client-runtime-3.3.4.jar\\n```\\n\\nReplace the following Huawei Cloud HADOOP dependencies.\\n\\n```shell\\ncommons-configuration2-2.1.1.jar\\ncommons-lang-2.6.0.wso2v1.jar\\nhadoop-auth-3.1.1-hw-ei-xxx.jar\\nhadoop-common-3.1.1-hw-ei-xxx.jar\\nhadoop-hdfs-client-3.1.1-hw-ei-xxx.jar\\nhadoop-plugins-8.1.2-xxx.jar\\nhadoop-yarn-api-3.1.1-hw-ei-xxx.jar\\nhadoop-yarn-client-3.1.1-hw-ei-xxx.jar\\nhadoop-yarn-common-3.1.1-hw-ei-xxx.jar\\nhttpcore5-h2-5.1.5.jar\\njaeger-core-1.6.0.jar\\nmysql-connector-java-8.0.28.jar\\nopentracing-api-0.33.0.jar\\nopentracing-noop-0.33.0.jar\\nopentracing-util-0.33.0.jar\\nprotobuf-java-2.5.0.jar\\nre2j-1.7.jar\\nstax2-api-4.2.1.jar\\nwoodstox-core-5.0.3.jar\\n```\\n\\n### **Other Bugs**\\n\\nIn the process of intensive use, we also found some problems. In order to better improve and optimize StreamPark's functionality, we have come up with some specific suggestions and solutions, such as:\\n\\n- **Dependency conflict**: conflict resolution when loading Flink dependencies (see: Pull Request #3568).\\n\\n- **Service Stability**: Intercept user program JVM exits to prevent the StreamPark platform from exiting abnormally due to user programs (see: Pull Request #3659).\\n\\n- **Resource Optimization**: In order to reduce the burden on resources, we limit the number of concurrent StremaPark builds, and control the maximum number of builds through configuration parameters (see: Pull Request #3696).\\n\\n## **Benefits derived**\\n\\nApache StreamPark has brought us significant benefits,**mainly in its one-stop service capability, which enables business developers to complete the development, compilation, submission and management of Flink jobs** on a unified platform. It greatly saves our time on Flink job development and deployment, significantly improves development efficiency, and realizes full-process automation from user rights management to Git deployment, task submission, alerting, and automatic recovery, effectively solving the complexity of Apache Flink\xae operation and maintenance.\\n\\n![](/blog/tianyancha/earnings.png)\\n\\nCurrently on the Tianyancha platform, specific results from StreamPark include:\\n\\n- Real-time job go-live and test deployment processes are simplified by **70%**.\\n\\n- The operation and maintenance costs of real-time jobs have been reduced by **80%**, allowing developers to focus more on code logic.\\n\\n- Alert timeliness has improved by more than **5x**, from minutes to seconds, enabling task exceptions to be sensed and handled within **5 seconds**.\\n\\n- StreamPark automatically recovers from task failures without human intervention.\\n\\n- The integration of GitLab with StreamPark simplifies the process of compiling projects and submitting jobs, dramatically reducing development and maintenance costs.\\n\\n## **Future planning**\\n\\nIn the future, we plan to migrate all 300+ Flink jobs deployed on our internal real-time computing platform and 500+ Flink jobs maintained in other ways to StreamPark for unified management and maintenance. When we encounter some optimization or enhancement points in the future, we will communicate with the community developers in a timely manner and contribute to the community, so as to make StreamPark easier and more usable! We will make StreamPark easier and better to use.\\n\\nAt the end of this article, we would like to thank the Apache Streampark community, especially Mr. Huajie, a member of the PMC, for his continuous follow-up and valuable technical support during the process of using StreamPark, and we deeply feel the serious attitude and enthusiasm of the Apache StreamPark community. We are also very grateful to Huawei Cloud students for their support and troubleshooting on cloud deployment.\\n\\nWe look forward to continuing our cooperation with the StreamPark community in the future to jointly promote the development of real-time computing technology, and hope that StreamPark will do better and better in the future, and graduate to become a representative of the new Apache project soon!\"},{\"id\":\"streampark-usercase-joymaker\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-joymaker\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/12-streampark-usercase-joymaker.md\",\"source\":\"@site/blog/12-streampark-usercase-joymaker.md\",\"title\":\"Apache StreamPark\u2122 Cloud Native Platform Practice at Joymaker\",\"description\":\"Introduction: This article mainly introduces Joymaker's application of big data technology architecture in practice, and explains why Joymaker chose \\\"Kubernetes + StreamPark\\\" to continuously optimise and enhance the existing architecture. It not only systematically describes how to deploy and apply these key technologies in the actual environment, but also explains the practical use of StreamPark in depth, emphasising the perfect integration of theory and practice. I believe that by reading this article, readers will help to understand and master the relevant technologies, and will be able to make progress in practice, which will lead to significant learning results.\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"}],\"readingTime\":29.57,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-joymaker\",\"title\":\"Apache StreamPark\u2122 Cloud Native Platform Practice at Joymaker\",\"tags\":[\"StreamPark\",\"Production Practice\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Apache StreamPark\u2122 helps Tianyancha real-time platform construction\uff5cMultiple-fold increase in efficiency\",\"permalink\":\"/blog/streampark-usercase-tianyancha\"},\"nextItem\":{\"title\":\"China Union's Flink Real-Time Computing Platform Ops Practice\",\"permalink\":\"/blog/streampark-usercase-chinaunion\"}},\"content\":\"![](/blog/joymaker/new_cover.png)\\n\\n**Introduction**: This article mainly introduces Joymaker's application of big data technology architecture in practice, and explains why Joymaker chose \\\"Kubernetes + StreamPark\\\" to continuously optimise and enhance the existing architecture. It not only systematically describes how to deploy and apply these key technologies in the actual environment, but also explains the practical use of StreamPark in depth, emphasising the perfect integration of theory and practice. I believe that by reading this article, readers will help to understand and master the relevant technologies, and will be able to make progress in practice, which will lead to significant learning results.\\n\\nGithub: https://github.com/apache/streampark\\n\\nWelcome to follow, Star, Fork and participate in contributing!\\n\\nContributed by | Joymaker\\n\\nAuthor | Du Yao\\n\\nArticle Organiser | Yang Linwei\\n\\nProofreader | Pan Yuepeng\\n\\n\x3c!-- truncate --\x3e\\n\\nJoymaker, is a global game development and publishing company; the company's products focus on two categories: MMORPG and MMOACT; Joy's product philosophy: \\\"It is better to make a product that 100 people scream than a product that 10,000 people applaud\\\"; Joy's products are always of high quality. RO Legend of Wonderland: Love is Like the First Time\\\" was launched in Southeast Asia, and reached the milestone of 10 million reservations during the booking period, achieved the first place in the best-seller list of five countries on the first day of the launch, and was released in Japan, Korea, the United States, Hong Kong, Macao, Taiwan, and other places, and has achieved good results.\\n\\n## **1. Operational Context and Challenges** \\n\\nThe big data base currently used by Joymaker Entertainment is Tencent Cloud's EMR, and the number warehouse architecture is mainly divided into the following categories:\\n\\n- **Offline Warehouse Architecture**: Hadoop + Hive + Dolphinscheduler + Trino\\n\\n- **Real-time Warehouse Architecture**: Kafka + Flink + StreamPark + Doris\\n\\n- **Streaming Warehouse Architecture (in planning)**: Paimon + Flink + StreamPark + Doris\\n\\nAmong them, the streaming warehouse architecture is one of our key plans for 2024, with the goal of improving big data analytics and the value output of various data products based on the elasticity feature of Kubernetes. However, due to earlier architectural reasons (mainly the mixed deployment of Apache Doris and EMR core nodes), it made us encounter many problems, such as:\\n\\n- **Resource contention**: Under the YARN resource scheduling platform, Apache Doris, Apache Flink real-time tasks, Hive's offline tasks, and Spark's SQL tasks often contend for CPU and memory resources, resulting in Doris query stability degradation and frequent query errors.\\n\\n- **Insufficient resources**: YARN's machine resource usage rate is between 70% and 80% most of the time, especially in the scenario of a large number of complements, the cluster resources are often insufficient.\\n\\n- **Manual Intervention**: Real-time Flink has more and more tasks, the data of each game has obvious peaks and valleys changes in a day, especially the hangout activities carried out on weekends, the data volume doubles, and even increases to more than 10 billion entries per day, and this requires frequent manual adjustment of parallelism to adapt to the computational pressure in order to ensure the data consumption capacity.\\n\\n## **2. Solution Selection** \\n\\nIn order to solve the above problems, we are looking for components that can perform \\\"**resource elasticity scaling**\\\" and \\\"**task management**\\\", and considering the trend of big data cloud-native, it is imminent to adopt cloud-native architecture, which can reduce the burden of big data operation and maintenance and provide better and more stable capabilities such as high availability, load balancing, grey update, etc. under Kubernetes architecture. The Kubernetes architecture reduces the burden on big data operations and provides better and more stable capabilities such as high availability, load balancing, and grey scale updates.\\n\\nAt the same time, simplifying the development and operation and maintenance of Apache Flink real-time tasks is also a problem that has to be faced. Since mid-2022, we have focused on researching Flink-related open-source and commercialisation-related projects, and in the process of technology selection, the company has conducted in-depth investigations mainly from several key dimensions, including:**Kubernetes containerised deployment, lightweightness, multi-version support for Flink, and functionality. Flink version support, completeness of features, multiple deployment modes, support for CI/CD, stability, maturity, etc. And finally chose Apache StreamPark**.\\n\\n![](/blog/joymaker/cover.png)\\n\\nIn the process of using StreamPark, we have carried out in-depth application and exploration of it, and have successfully achieved 100+ Flink tasks easily deployed to Kubernetes, which can be very convenient for job management.\\n\\nNext, will detail how we use StreamPark to achieve Flink on kubernetes in the production environment of the practice process, although the content is longer, but full of dry goods, the content is mainly divided into the following parts:\\n\\n- **Environmental Preparation**: This part introduces the installation and configuration process of Kubernetes, StreamPark, Maven, Docker, kubectl tools and Flink.\\n\\n- **StreamPark Use Chapter**: This part introduces how to use StreamPark to start session clusters, as well as the process of deploying SQL and JAR jobs.\\n\\n- **Flink Tasks Application Scenarios**: In this section, we will share the practical scenarios of Joyful Hub's actual use of Flink tasks.\\n\\n\\n## **3. Practical - Environment Preparation** \\n\\nNext we will start from the installation of a detailed introduction to the StreamPark landing practice, on the Tencent cloud products, some of the steps, will be briefly introduced at the end of the article, here mainly share the core steps.\\n\\n### **3.1. Preparing the Kubernetes Environment**\\n\\nJoymaker's Kubernetes environment uses the Tencent Cloud container service product, in which the Kubernetes namespace is mainly divided as follows:\\n\\n- **Apache Doris**: Doris cluster + 3 FE (standard nodes) + CN (1 supernode).\\n\\n- **Apache Flink**: The Flink cluster consists of a supernode, which is the equivalent of an oversized resource machine that is very easy to use and can quickly and elastically scale to tens of thousands of pods.\\n\\n- **BigData**: contains various Big Data components.\\n\\nIn addition, we have two native nodes that are mainly used to deploy StreamPark, Dolphinscheduler, and Prometheus + Grafana. screenshots related to the cluster information are shown below:\\n\\n![](/blog/joymaker/clusters.png)\\n\\n### **3.2. Mounting StreamPark**\\n\\nWe are using the latest stable version of StreamPark 2.1.4, which fixes some bugs and adds some new features, such as support for Flink 1.18, Jar task support via pom or uploading dependencies and so on.\\n\\nAlthough StreamPark supports Docker and Kubernetes deployment, for the sake of saving time and the need to containerise all big data components in a top-level design, we currently chose **to quickly build StreamPark on a CVM in the cloud**, and the following is the installation and deployment script for StreamPark:\\n\\n```shell\\n#Use root user to enter the installation directory (java environment is required first) and k8s environment on the same network segment!!!! You can save yourself the trouble.\\ncd /data \\n    \\n#Download the binary installer.\\nwget https://dlcdn.apache.org/incubator/streampark/2.1.4/apache-streampark_2.12-2.1.4-incubating-bin.tar.gz   \\n\\n#Compare it with the values provided to ensure the package is complete.\\nsha512sum apache-streampark_2.12-2.1.2-incubating-bin.tar.gz  #unzip it\\ntar -xzvf apache-streampark_2.12-2.1.2-incubating-bin.tar.gz  #rename it\\nmv apache-streampark_2.12-2.1.2-incubating-bin streampark_2.12-2.1.2 #create softlinks\\nln -s streampark_2.12-2.1.2  streampark\\n\\n# Modify the database configuration\\nvim /data/streampark/conf/application-mysql.ymlspring:\\n  datasource:\\n    username: root\\n    password: xxxxx\\n    driver-class-name: com.mysql.cj.jdbc.Driver\\n    url: jdbc:mysql://10.16.x.x:3306/streampark_k8s?...\\n\\n# Modify streampark kernel configuration You can modify the port (because k8s nodeport port range 30000-32767) so you can configure this range to save the port for external development and then configure or account password We use cos for storage so we do not configure hdfs.\\nvim /data/streampark/conf/application.yml      workspace:\\n   local: /data/streampark/streampark_workspace #create the required directory\\n\\n\\nmkdir -p  vim /data/streampark/conf/application.yml\\n\\n#Initialise the database tables for streampark. Requires a mysql client in the environment. After execution, you can go to mysql and see if the tables exist.\\nmysql -u root -p xxxxx < /data/streampark/script/schema/mysql-schema.sql\\n\\n#Add the dependent mysql-connector package because it's not under the streampark lib. Under /data/streampark/lib run.\\nwget https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.0.33/mysql-connector-j-8.0.33.jar \\n\\n#Start and log in. You can verify the java environment first. java -version to make sure it's ok. Execute under /data/streampark/lib.\\n ./startup.sh \\n```\\n\\n### **3.3. Mounting Maven**\\n\\nTo configure Flink tasks on StreamPark, you can download the dependencies via maven, here is the Maven installation configuration:\\n\\n```shell\\n#Download maven as root user.\\ncd /data && wget https://dlcdn.apache.org/maven/maven-3/3.9.6/binaries/apache-maven-3.9.6-bin.tar.gz\\n\\n#Verify that the installation package is complete.\\nsha512sum apache-maven-3.9.6-bin.tar.gz \\n\\n#Unzip Softlink.\\ntar -xzvf apache-maven-3.9.6-bin.tar.gz \\nln -s apache-maven-3.9.6 maven\\n\\n#Set up a maven mirror to speed up downloads from the AliCloud repository.\\nvim /data/maven/conf/settings.xml\\n\\n<mirror>\\n   <id>alimaven</id>\\n  <mirrorOf>central</mirrorOf>\\n  <name>aliyun maven</name>\\n  <url>https://maven.aliyun.com/repository/central</url>\\n</mirror>\\n\\n<mirror>\\n  <id>alimaven</id>\\n  <name>aliyun maven</name>\\n  <url>http://maven.aliyun.com/nexus/content/groups/public/</url>\\n  <mirrorOf>central</mirrorOf>\\n </mirror>\\n\\n#Environment Variables Finally add.\\nvim /etc/profile\\nexport MAVEN_HOME=/data/maven\\nPATH=$MAVEN_HOME/bin:$PATH\\n\\n#Validate.\\nsource /etc/profile  && mvn -version\\n```\\n\\n### **3.4 Mounting Docker**\\n\\nBecause StreamPark uses Kubernetes, you can use Docker to build and upload images to your company's popular Harbor image service, here's the Docker install script\\n\\n```shell\\n#Uninstall the old version If available\\nsudo yum remove docker \\\\\\n         docker-client \\\\\\n         docker-client-latest \\\\\\n         docker-common \\\\\\n         docker-latest \\\\\\n         docker-latest-logrotate \\\\\\n         docker-logrotate \\\\\\n         docker-engine\\n\\n#Download the repository\\nwget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo      #AliCloud docker-ce image repository\\n\\n#Update the package index \\nyum makecache     \\n\\n#Download and install\\nyum install docker-ce docker-ce-cli containerd.io -y # docker-ce community edition ee enterprise edition\\n\\n#Start View \\nsystemctl start docker && docker version\\n\\n#Boot\\nsystemctl enable docker\\n```\\n\\n### **3.5. Kubectl Tool Installation**\\n\\nIn order to interact with the Kubernetes cluster, view configurations, access logs, etc., we also installed the kubectl tool. Before installing kubectl, you first need to get the access credentials for the Kubernetes cluster from here, as shown in the following figure:\\n\\n![](/blog/joymaker/access_credential.png)\\n\\nNext, download a copy of the configuration to the root user directory, specify the directory and rename it:\\n\\n```shell\\n# 1\uff09Create the default hidden directory.\\nmkdir -p /root/.kube   \\n\\n# 2\uff09Upload k8s intranet access credentials.                 \\nrz cls-dtyxxxxl-config\\n\\n# 3\uff09Modify the default name of the credentials.               \\nmv cls-dtyxxxxl-config config  \\n\\n# 4\uff09Place the credentials in the specified location.\\nmv config /root/.kube \\n```\\n\\nYou can install the kubetcl client as follows (you can also install **kubectx** to make it easier to access multiple kubernetes clusters and switch between fixed namespaces):\\n\\n```shell\\n# 1\uff09Download the latest version.\\ncurl -LO \\\"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\\\"\\n\\n# 2\uff09Add execute permissions.\\nchmod +x kubectl\\n\\n# 3\uff09Move to a common tool command location.\\nmv ./kubectl /usr/local/bin/kubectl\\n\\n# 4\uff09This can be configured in /etc/profile or in .bashrc in the root user's home directory.\\nvim /root/.bashrc\\nexport JAVA_HOME=/usr/local/jdk1.8.0_391\\nPATH=/usr/local/bin:$JAVA_HOME/bin:$PATH\\n\\n# 5\uff09Verify the client and cluster keys.\\nkubectl cluster-info\\n```\\n\\nFinally, you also need to create a Flink-specific account credentials (which will be used later):\\n\\n```shell\\n# 1\uff09Create namespace.\\nkubectl create namespace flink\\n\\n# 2\uff09Create account for flink to access k8s Remember to bring the namespace!\\nkubectl  create serviceaccount flink-service-account -n flink\\n\\n# 3\uff09Bind some permissions for container operations to this account Remember with namespaces!!!\\nkubectl  create clusterrolebinding flink-role-binding-flink --clusterrole=edit --serviceaccount=flink:flink-service-account -n flink  \\n```\\n\\n### **3.6. Installation Configuration Flink**\\n\\nWe chose the Flink1.17-scala2.12 Java11 image for our Kubernetes environment because some of the dependencies of Flink 1.18 were not easy to find.\\n\\nDownload and extract the flink-1.17.2-bin-scala_2.12.tgz installer with the following script:\\n\\n```shell\\ncd /data && wget https://dlcdn.apache.org/flink/flink-1.17.2/flink-1.17.2-bin-scala_2.12.tgz\\n\\n#Integrity checksum.\\nsha512sum flink-1.17.2-bin-scala_2.12.tgz\\n\\n#Unzip\\ntar -xzvf flink-1.17.2-bin-scala_2.12.tgz\\n\\n#Rename\\nmv flink-1.17.2-bin-scala_2.12 flink-1.17.2\\n\\n#Soft connect.\\nln -s flink-1.17.2 flink\\n\\n#Store old binary packages.\\nmv flink-1.17.2-bin-scala_2.12.tgz /data/softbag   \\n```\\n\\nAt this point, we have set up the environment, the following continue to step by step detailed explanation of the use of StreamPark.\\n\\n## **4. Practical - StreamPark in Action**\\n\\nReaders who want to get a quick overview of StreamPark running jobs to Kubernetes can watch the video below:\\n\\n// Video link (Flink On Kubernetes Application Hands-On Tutorial)\\n\\n// Video link (Flink On Kubernetes Session Hands-On Tutorial)\\n\\n### **4.1. Configure Flink Home**\\n\\nAfter logging in to StreamPark, switch to the Flink Home menu and configure the directory where we extracted the Flink installation package earlier, as shown in the screenshot below:\\n\\n![](/blog/joymaker/flink_home.png)\\n\\n### **4.2. Establish Flink Session**\\n\\nThen switch to FlinkCluster and Add New to add a new Flink cluster:\\n\\n![](/blog/joymaker/flink_cluster.png)\\n\\nConfiguration details are as follows, here is the first part of the configuration:\\n\\n**Configuration details**: cluster name and Kubernetes cluster ID We usually write the same, fill in the correct Kubernetes service account, here the image is used in tencent cloud tcr, and use lb as the external access to the Flink UI. (The slots here are usually set to 2, and the task parallelism is usually a multiple of 2, so that the session cluster has no idle resources).\\n\\n![](/blog/joymaker/first_configuration.png)\\n\\nThe second part of the configuration content:\\n\\n**Configuration details**: We take the Session mode, a JobManager manages a lot of tasks, so the resources can be slightly larger, we give 2G, after observing can be satisfied, but due to the task is more, the memory allocation of JobManager metaspace consumption is relatively large, so you can be the default of 256M, change to 500M, after After observation, a session manages less than 10 tasks, this configuration is reasonable.\\n\\nAfter TaskManager memory session configuration is given, it can not be modified in StreamPark programme configuration page, so you can estimate the amount of session project data, if the amount of data is large, you can adjust \\\"CPU : Memory\\\" to \\\"1 core : 2G\\\", of course, you can also adjust \\\"CPU : Memory\\\" to \\\"1 core : 2G\\\", of course, you can also change the default to 256M. 2G\\\", of course, can also be larger, because the data processing and the number of TaskManager related, that is, the degree of parallelism, so you can basically avoid some OOM problems, we \\\"1 : 2\\\" session cluster project a day to deal with the data volume of six billion, 18 Parallelism is for reference only. The default cpu: memory 1 : 1 is basically ok.\\n\\n![](/blog/joymaker/second_configuration.png)\\n\\nThe last part reads as follows:\uff1a\\n\\n![](/blog/joymaker/third_configuration.png)\\n\\nThe exact textual content is posted here:\\n\\n```shell\\n#Access method reuse lb+port lb can be created in advance without k8s creation so that the ip is fixed. It is more convenient to reuse.\\n-Drest.port=8091\\n-Dkubernetes.rest-service.annotations=service.kubernetes.io/qcloud-share-existed-lb:true,service.kubernetes.io/tke-existed-lbid:lb-iv9ixxxxx\\n-Dkubernetes.rest-service.exposed.type=LoadBalancer\\n\\n#cp sp access tencent cloud cos using s3 protocol plugin way. \\n-Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.17.2.jar\\n-Dcontainerized.taskmanager.env.EANABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.17.2.jar\\n-Dstate.checkpoints.dir=s3://k8s-bigdata-xxxxxx/flink-checkpoints\\n-Dstate.savepoints.dir=s3://k8s-bigdata-xxxxx/flink-savepoints\\n\\n#The task scheduling policy in #k8s, the node selection policy, because the flink namespace is logical, and the node selection policy can be made to run on a flink-specific physical supernode.\\n-Dkubernetes.jobmanager.node-selector=usefor:flink\\n-Dkubernetes.taskmanager.node-selector=usefor:flink\\n\\n#Enable high availability for jobmanager Use k8s implementation, use cos directory storage, no hdfs overall.\\n-Dhigh-availability.type=kubernetes\\n-Dhigh-availability.storageDir=s3://k8s-bigdata-xxx/flink-recovery\\n-Dkubernetes.cluster-id=streampark-share-session\\n-Dkubernetes.jobmanager.replicas=2\\n\\n#pod image pull policy and flink cancel cp reservation Time zone\\n-Dkubernetes.container.image.pull-policy=Always\\n-Dexecution.checkpointing.externalized-checkpoint-retention=RETAIN_ON_CANCELLATION\\n-Dtable.local-time-zone=Asia/Shanghai\\n\\n#Mirror pull cache acceleration as explained below.\\n-Dkubernetes.taskmanager.annotations=eks.tke.cloud.tencent.com/use-image-cache: imc-6iubofdt\\n-Dkubernetes.jobmanager.annotations=eks.tke.cloud.tencent.com/use-image-cache: imc-6iubofdt  \\n```\\n\\nNote: Flink basic information can be configured in the conf file, and more general information is recommended to be written in the session, StreamPark is more appropriate to fill in the parallelism/cp/sp/fault-tolerance/main parameter, and so on.\\n\\nFinally, you can start the session cluster. You can use the kubectl command to check whether the Flink kubernetes session cluster has started successfully:\\n\\n![](/blog/joymaker/session_cluster.png)\\n\\n### **4.3. Task Configuration**\\n\\nHere we will continue to demonstrate the use of StreamPark to submit three types of tasks, namely **SQL tasks, Jar tasks, and Application mode tasks**.\\n\\n- **Submitting a Flink SQL job**\\n\\nA screenshot of the configuration of a Flink Sql job is shown below:\\n\\n![](/blog/joymaker/sql_configuration.png)\\n\\nThe kubernetes clusterid should just be the session name of the corresponding project:\\n\\n![](/blog/joymaker/kubernetes_configuration.png)\\n\\nThe dynamic parameters are configured as follows:\\n\\n```shell\\n-Dstate.checkpoints.dir=s3://k8s-bigdata-12xxxxx/flink-checkpoints/ro-cn-sync\\n-Dstate.savepoints.dir=s3://k8s-bigdata-12xxxxxxx/flink-savepoints/ro-cn-sync\\n-Dkubernetes.container.image.pull-policy=Always\\n-Dkubernetes.service-account=flink-service-account\\n-Dexecution.checkpointing.interval=25s\\n-Dexecution.checkpointing.mode=EXACTLY_ONCE\\n-Dtable.local-time-zone=Asia/Shanghai\\n-Drestart-strategy.fixed-delay.attempts=10\\n-Drestart-strategy.fixed-delay.delay=3min  \\n```\\n\\n- **Submit Flink JAR Tasks**\\n\\nThe Jar task is basically the same as the Sql task, the configuration screenshot is below:\\n\\n\\n![](/blog/joymaker/jar_configuration.png)\\n\\nDynamic parameters are as follows (if there is a configuration template, here can be more streamlined, because StreamPark supports very good task replication, so you can write a standard Jar and SQL task demo, the rest of the tasks are replicated, which can greatly improve the efficiency of task development):\\n\\n```shell\\n-Dstate.checkpoints.dir=s3://k8s-bigdata-1xxxx/flink-checkpoints/ro-cn-shushu\\n-Dstate.savepoints.dir=s3://k8s-bigdata-1xxxx/flink-savepoints/ro-cn-shushu\\n-Dkubernetes.container.image.pull-policy=Always\\n-Dkubernetes.service-account=flink-service-account\\n-Dexecution.checkpointing.interval=60s\\n-Dexecution.checkpointing.mode=EXACTLY_ONCE\\n-Dstate.checkpoints.num-retained=2\\n-Drestart-strategy.type=failure-rate\\n-Drestart-strategy.failure-rate.delay=3min\\n-Drestart-strategy.failure-rate.failure-rate-interval=30min\\n-Drestart-strategy.failure-rate.max-failures-per-interval=8\\n```\\n\\n- **Submitting a Flink Application task**\\n\\nOf course, you can also deploy Application mode tasks to Kubernetes, so here's an example, starting with the first part of the configuration:\\n\\n![](/blog/joymaker/application_first_configuration.png)\\n\\nThis is the second part of the configuration as follows:\\n\\n![](/blog/joymaker/application_second_configuration.png)\\n\\nThe third part of the configuration is as follows:\\n\\n![](/blog/joymaker/application_third_configuration.png)\\n\\nDynamic parameters:\\n\\n![](/blog/joymaker/dynamic_parameter.png)\\n\\nDetailed configuration content:\uff1a\\n\\n```shell\\n-Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.17.2.jar \\n-Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-s3-fs-hadoop-1.17.2.jar\\n-Dstate.checkpoints.dir=s3://k8s-bigdata-xxx/flink-checkpoints/Kafka2Rocketmq2Kafka\\n-Dstate.savepoints.dir=s3://k8s-bigdata-xxx/flink-savepoints/Kafka2Rocketmq2Kafka\\n-Dkubernetes.container.image.pull-policy=Always\\n-Dkubernetes.service-account=flink-service-account\\n-Dexecution.checkpointing.interval=60s\\n-Dexecution.checkpointing.mode=EXACTLY_ONCE\\n-Dstate.checkpoints.num-retained=2\\n-Drest.port=8092\\n-Dkubernetes.jobmanager.node-selector=usefor:flink\\n-Dkubernetes.taskmanager.node-selector=usefor:flink\\n-Dkubernetes.rest-service.annotations=service.kubernetes.io/qcloud-share-existed-lb:true,service.kubernetes.io/tke-existed-lbid:lb-xxx\\n-Dkubernetes.rest-service.exposed.type=LoadBalancer\\n-Dfs.allowed-fallback-filesystems=s3\\n```\\n\\n### **4.4. Build Tasks**\\n\\nThe task is then constructed, and each step of the task construction can be clearly seen here:\\n\\n![](/blog/joymaker/task_construction.png)\\n\\nEventually you can run it and observe that the task is running:\\n\\n![](/blog/joymaker/running.png)\\n\\n### **4.5. Notification Alarm Configuration (optional)**\\n\\nOf course, StreamPark supports multiple notification alert methods (e.g. mailboxes and flybooks, etc.), and we use the mailbox method, which can be configured directly from the interface:\\n\\n![](/blog/joymaker/alarm_configuration.png)\\n\\nSo far, we have shared how to successfully go about deploying different types of Flink tasks running in kubernetes in StremPark, very silky smooth!\\n\\n## **5. Practical - Homework Application Scenarios**\\n\\nNext, we will continue to dissect the job, from the tuning, hands-on and other scenarios to give us a better understanding of the application of Flink tasks.\\n\\n### **5.1. Tuning Recommendations**\\n\\n**Recommendation 1 (time zone)**: containers need to be in utc+8 to easily see the logs, we can add the following in the conf configuration:\\n\\n```shell\\nenv.java.opts.jobmanager: -Duser.timezone=GMT+08\\nenv.java.opts.taskmanager: -Duser.timezone=GMT+08\\n```\\n\\n**Recommendation 2 (StreamPark Variable Management)**: StreamPark provides variable management, which allows us to manage some configurations to improve security and convenience:\\n\\n![](/blog/joymaker/variable_management.png)\\n\\n**Proposal 3 (StreamPark Passing Parameters)**: On YARN, we used Flink's own toolkit to pass parameters, and it was difficult to pass parameters to the Flink environment, so we didn't look into it. We chose to pass in the program parameters through StreamPark, so that it can be used in both YARN and kubernetes, **while the application and session methods in kubernetes can also be used, it becomes more general and easy to change, highly recommended**!\\n\\n![](/blog/joymaker/incoming_parameter.png)\\n\\n**Recommendation 4 (Use Session pattern)**: Although the Yarn-application pattern provides a high degree of task isolation, each of its tasks requires a separate jobmanager resource, which leads to excessive resource consumption. In addition, merging tasks, while reducing resource consumption, can lead to problems such as high workload and unstable data. In contrast, the session pattern allows tasks to share a single jobmanager resource in a session, which can maximise the saving of computing resources, and although it may have a single-point problem, it can be solved by using two jobmanager instances to achieve high availability. More importantly, the session mode also makes it easier to view logs generated due to job failures, which helps improve the efficiency of troubleshooting. Therefore, given its advantages in terms of resource utilisation, high availability and task failure handling, choosing session mode becomes a more desirable option.\\n\\n### **5.2. Data CDC synchronisation**\\n\\n**[MySQL \u2192 Doris]**: The following code essentially demonstrates the Flink sql pipeline for **MySQL data synchronisation to Apache Doris**:\\n\\n```sql\\nCREATE TABLE IF NOT EXISTS `login_log_mysql` (\\n login_log_id      bigint not null,  \\n account_id        bigint ,  \\n long_account_id   string , \\n short_account_id  string , \\n game_code         string , \\n package_code      string , \\n channel_id        int    ,\\n login_at          int    ,  \\n login_ip          string ,    \\n device_ua         string ,\\n device_id         string ,\\n device_net        string ,\\n device_ratio      string , \\n device_os         string ,\\n device_carrier    string ,\\n ext               string ,        \\n created_at        int    ,  \\n updated_at        int    ,  \\n deleted_at        int ,\\n  PRIMARY KEY(`login_log_id`)\\n NOT ENFORCED\\n) with (\\n  'connector' = 'mysql-cdc',\\n  'hostname' = '10.xxx.xxx.xxx',\\n  'port' = '3306',\\n  'username' = 'http_readxxxxx',\\n  'password' = 'xxxx@)',\\n  'database-name' = 'player_user_center',\\n  -- 'scan.startup.mode' = 'latest-offset',\\n  'scan.startup.mode' = 'earliest-offset',\\n  'table-name' = 'login_log_202[3-9][0-9]{2}',\\n  'server-time-zone' = '+07:00'\\n);\\n\\ncreate table if not EXISTS login_log_doris(\\n  `date`           date       ,    \\n   login_log_id    bigint     ,\\n   account_id      bigint     ,\\n   long_account_id     string ,\\n   short_account_id    string ,\\n   game_code       string     ,\\n  `package_code`   string     ,\\n  `channel_id`     int        ,\\n   login_at        string     ,\\n   login_ip        string     ,\\n   device_ua       string     ,\\n   device_id       string     ,\\n   device_net      string     ,\\n   device_ratio    string     ,\\n   device_os       string     ,\\n   device_carrier  string     ,\\n   ext             string     ,\\n   created_at      string     ,\\n   updated_at      string     ,\\n   deleted_at      string     ,\\n   PRIMARY KEY(`date`,`login_log_id`)\\n NOT ENFORCED\\n) WITH (\\n   'connector' = 'doris',\\n    'jdbc-url'='jdbc:mysql://xxx.xx.xx.xx:9030,xxx.xxx.xxx.xxx:9030,xxx.xxx.xxx.xxx:9030',\\n    'load-url'='xxx.xx.xx.xx:8030;xxx.xx.xx.xx:8030;xxx.xx.xx.xx:8030',\\n    'database-name' = 'ro_sea_player_user_center',\\n    'table-name' = 'ods_login_log',\\n    'username' = 'root',\\n    'password' = 'root123',\\n    'sink.semantic' = 'exactly-once',\\n    'sink.max-retries' = '10'\\n);\\n\\ncreate view login_log_flink_trans as \\nselect  \\n to_date(cast(to_timestamp_ltz(login_at,0) as varchar) )  , \\n login_log_id     ,  \\n account_id       ,  \\n long_account_id  , \\n short_account_id , \\n game_code        , \\n package_code     , \\n channel_id,\\n cast(to_timestamp_ltz(login_at,0) as varchar) as login_at,\\n login_ip         ,    \\n device_ua        ,\\n device_id        ,\\n device_net       ,\\n device_ratio     , \\n device_os        ,\\n device_carrier   ,\\n ext              ,\\n cast(to_timestamp_ltz(created_at,0) as varchar) as created_at,\\n cast(to_timestamp_ltz(updated_at,0) as varchar) as updated_at,\\n cast(to_timestamp_ltz(deleted_at,0) as varchar) as deleted_at\\nfrom login_log_mysql;\\n\\ninsert into login_log_doris select * from login_log_flink_trans;\\n\\nCREATE TABLE IF NOT EXISTS `account_mysql` (\\n account_id        bigint ,  \\n open_id           string , \\n dy_openid         string , \\n dy_ios_openid     string , \\n ext               string ,        \\n last_channel      int    ,  \\n last_login_time   int    ,  \\n last_login_ip     string ,  \\n created_at        int    ,  \\n updated_at        int    ,  \\n deleted_at        string ,\\n PRIMARY KEY(`account_id`)\\n NOT ENFORCED\\n) with (\\n  'connector' = 'mysql-cdc',\\n  'hostname' = 'xxx.xx.xx.xx',\\n  'port' = '3306',\\n  'username' = 'http_readxxxx',\\n  'password' = 'xxxxx@)',\\n  'database-name' = 'player_user_center',\\n  -- 'scan.startup.mode' = 'latest-offset',\\n  'scan.startup.mode' = 'earliest-offset',\\n  'table-name' = 'account_[0-9]+',\\n  'server-time-zone' = '+07:00'\\n);\\n\\ncreate table if not EXISTS account_doris(\\n  `date`            date     ,    \\n   account_id       bigint   ,\\n   open_id          string   ,\\n   dy_openid        string   ,\\n   dy_ios_openid    string   ,\\n   ext              string   ,\\n  `last_channel`    int      ,\\n   last_login_time  string   ,\\n   last_login_ip    string   ,\\n   created_at       string   ,\\n   updated_at       string   ,\\n   deleted_at       string   ,\\n   PRIMARY KEY(`date`,`account_id`)\\n NOT ENFORCED\\n) WITH (\\n   'connector' = 'doris',\\n    'jdbc-url'='jdbc:mysql://xxx.xx.xx.xx:9030,xxx.xx.xx.xx:9030,xxx.xx.xx.xx:9030',\\n    'load-url'='xxx.xx.xx.xx:8030;xxx.xx.xx.xx:8030;xxx.xx.xx.xx:8030',\\n    'database-name' = 'ro_sea_player_user_center',\\n    'table-name' = 'ods_account_log',\\n    'username' = 'root',\\n    'password' = 'root123',\\n    'sink.semantic' = 'exactly-once',\\n    'sink.max-retries' = '10'\\n);\\n\\ncreate view account_flink_trans as \\nselect  \\n to_date(cast(to_timestamp_ltz(created_at,0) as varchar) )  , \\n account_id       ,  \\n open_id          , \\n dy_openid        , \\n dy_ios_openid    , \\n ext              ,\\n last_channel     ,\\ncast(to_timestamp_ltz(last_login_time,0) as varchar) as last_login_time,\\n last_login_ip    ,    \\n cast(to_timestamp_ltz(created_at,0) as varchar) as created_at,\\n cast(to_timestamp_ltz(updated_at,0) as varchar) as updated_at,\\n deleted_at\\nfrom account_mysql;\\n\\ninsert into account_doris select * from account_flink_trans;\\n```\\n\\n**[Game Data Synchronisation]**: pull kafka data collected by overseas **filebeat to domestic kafka** and process metadata processing:\\n\\n```sql\\n--Create a kafka source table for hmt.\\nCREATE TABLE kafka_in (\\n  `env` STRING comment 'Game environment hdf  /qc/cbt/obt/prod',\\n  `host` STRING comment 'Game server where logs are located',\\n  `tags` STRING comment 'Log Tags normal | fix',\\n  `log`  STRING comment 'File and offset of the log',\\n  `topic` STRING METADATA VIRTUAL comment 'kafka topic',\\n  `partition_id` BIGINT METADATA FROM 'partition' VIRTUAL comment 'The kafka partition where the logs are located', \\n  `offset` BIGINT METADATA VIRTUAL comment 'The offset of the kafka partition', \\n   `uuid`  STRING comment 'uuid generated by filebeat',\\n  `message` STRING comment 'tlog message',\\n  `@timestamp` STRING comment 'filebeat capture time',\\n  `ts` TIMESTAMP(3) METADATA FROM 'timestamp' comment 'kafka storage message time'\\n) WITH (\\n  'connector' = 'kafka',\\n  'topic' = 'ro_sea_tlog',\\n  'properties.bootstrap.servers' = 'sea-kafka-01:9092,sea-kafka-02:9092,sea-kafka-03:9092',\\n  'properties.group.id' = 'streamx-ro-sea-total',\\n  'properties.client.id' = 'streamx-ro-sea-total',\\n  'properties.session.timeout.ms' = '60000',\\n  'properties.request.timeout.ms' = '60000',\\n  'scan.startup.mode' = 'group-offsets',\\n  --'scan.startup.mode' = 'earliest-offset',\\\\\\n  'properties.fetch.max.bytes' = '123886080',\\n  'properties.max.partition.fetch.bytes' = '50388608',\\n  'properties.fetch.max.wait.ms' = '2000',\\n  'properties.max.poll.records' = '1000',\\n  'format' = 'json',\\n  'json.ignore-parse-errors' = 'false'\\n);\\n\\n--Creating the kafka official table for emr\\nCREATE TABLE kafka_out_sea (\\n`env` STRING comment 'Game environment qc/cbt/obt/prod',\\n`hostname` STRING comment 'Game server where logs are located',\\n`tags` STRING comment 'Log Tags normal | fix',\\n`log_offset`  STRING comment 'File and offset of the log',\\n`uuid`  STRING comment 'uuid generated by filebeat',\\n`topic` STRING comment 'kafka topic',\\n`partition_id` BIGINT comment 'The kafka partition where the logs are located', \\n`kafka_offset` BIGINT comment 'The offset of the kafka partition', \\n eventname string comment 'event name',\\n`message` STRING comment 'tlog message',\\n`filebeat_ts` STRING comment 'filebeat collection time',\\n`kafka_ts` TIMESTAMP(3) comment 'kafka stored message time',\\n`flink_ts` TIMESTAMP(3)  comment 'flink processed message time'\\n) WITH (\\n  'connector' = 'kafka',\\n  'topic' = 'ro_sea_tlog',\\n  --'properties.client.id' = 'flinkx-ro-sea-prod-v2',\\n  'properties.bootstrap.servers' = 'emr_kafka1:9092,emr_kafka2:9092,emr_kafka3:9092',\\n  'format' = 'json',\\n  --'sink.partitioner'= 'fixed',\\n  'sink.delivery-guarantee' = 'exactly-once',\\n  'sink.transactional-id-prefix' = 'ro_sea_kafka_sync_v10',\\n  'properties.compression.type' = 'zstd',\\n  'properties.transaction.timeout.ms' = '600000',\\n  'properties.message.max.bytes' = '13000000',\\n  'properties.max.request.size' = '13048576',\\n  --'properties.buffer.memory' = '83554432',\\n  'properties.acks' = '-1'\\n);\\n\\n--etl create target view\\ncreate view kafka_sea_in_view  as \\nselect `env`,JSON_VALUE(`host`,'$.name') as hostname,`tags`,`log` as log_offset,`uuid`,\\n       `topic`,`partition_id`,`offset` as kafka_offset,\\n     lower(SPLIT_INDEX(message,'|',0)) as eventname,`message`, \\n     CONVERT_TZ(REPLACE(REPLACE(`@timestamp`,'T',' '),'Z',''), 'UTC', 'Asia/Bangkok') as filebeat_ts, `ts` as kafka_ts ,CURRENT_TIMESTAMP as flink_ts\\nfrom kafka_in;\\n\\n--Write data to emr sea topic\\ninsert into kafka_out_sea\\nselect * from kafka_sea_in_view ;\\n```\\n\\n**[Real-time counting warehouse]**: shunt Jar program and ods -> dwd sql processing function, part of the code screenshot below:\\n\\n![](/blog/joymaker/jar.png)\\n\\nand the associated Flink sql:\\n\\n```sql\\n--2 dwd_moneyflow_log\\nCREATE TABLE kafka_in_money (\\n`env` STRING   comment 'Game environment qc/cbt/obt/prod',\\n`hostname` STRING comment 'Game server where logs are located',\\n`uuid` STRING comment 'Log unique id',\\n`message` STRING comment 'tlog message',\\n`filebeat_ts` STRING comment 'filebeat collection time'\\n) WITH (\\n  'connector' = 'kafka',\\n  'topic' = 'ro_sea_tlog_split_moneyflow',\\n  'properties.bootstrap.servers' = 'emr_kafka1:9092,emr_kafka2:9092,emr_kafka3:9092',\\n  'properties.group.id' = 'flinksql_kafka_in_moneyflow_v2',\\n  'scan.startup.mode' = 'group-offsets',\\n  --'scan.startup.mode' = 'earliest-offset',\\n  'properties.isolation.level' = 'read_committed',\\n  'format' = 'json',\\n  'json.ignore-parse-errors' = 'false'\\n);\\n\\nCREATE TABLE kafka_out_money(\\n  `date` string not NULL  COMMENT 'date',\\n  `vroleid` int NULL COMMENT 'role id',\\n  `moneytype` int NULL  COMMENT 'Currency type', \\n  `env` string NULL COMMENT 'Game environment hdf  /qc/cbt/obt/prod',\\n  `hostname` string NULL COMMENT 'The game server where the logs are located',\\n  `uuid`  string NULL COMMENT 'The uuid generated by the filebeat capture',\\n  `filebeat_ts` string NULL COMMENT 'filebeat capture time',\\n   flink_ts string NULL COMMENT 'message time written by flink',  \\n  `gamesvrid` int  NULL  COMMENT 'Logged in game server number', \\n  `dteventtime` string NULL  COMMENT  'The time of the game event, format YYYY-MM-DD HH:MM:SS', \\n  `vgameappid` string NULL COMMENT  'Game APPID', \\n  `platid` int NULL  COMMENT 'ios 0 /android 1', \\n  `izoneareaid` int NULL  COMMENT 'The zone id is used to uniquely identify a zone for split-server games; 0 for non-split-server games', \\n  `vopenid` string NULL COMMENT 'User's OPENID number', \\n  `vrolename` string NULL COMMENT 'Character name', \\n  `jobid` int  NULL  COMMENT 'Role Occupation 0=Wizard 1=......', \\n  `gender` int NULL  COMMENT 'Character Gender 0=Male 1=Female', \\n  `ilevel` int NULL  COMMENT 'Character base level', \\n  `ijoblevel` int NULL  COMMENT 'Character's career level', \\n  `playerfriendsnum` int NULL  COMMENT 'Number of player friends', \\n  `chargegold` int NULL  COMMENT 'Character's recharge experience (cumulative recharge),\\n  `iviplevel` int NULL  COMMENT 'Character's VIP level', \\n  `createtime` string NULL  COMMENT  'Account creation time',\\n  `irolece` int NULL  COMMENT 'Battle power/rating',\\n  `unionid` int NULL  COMMENT 'Guild ID',\\n  `unionname`  string NULL COMMENT 'Guild name', \\n  `regchannel` int NULL  COMMENT 'registration channel', \\n  `loginchannel` int NULL  COMMENT 'login channel', \\n  `sequence` int NULL  COMMENT 'Used to correlate a single action to generate multiple logs of different types of currency movement', \\n  `reason` int NULL  COMMENT 'Behavior (currency flow level 1 reason)',\\n  `subreason` int NULL  COMMENT 'Flow direction (item flow definition)', \\n  `imoney` int NULL  COMMENT 'Number of currency changes',\\n  `aftermoney` int NULL  COMMENT 'Number of aftermoney after action',\\n  `afterboundmoney` int NULL  COMMENT 'Number of money bound after the action',\\n  `addorreduce` int NULL  COMMENT 'Increase or decrease: 0 is increase; 1 is decrease', \\n  `serialnum` string NULL COMMENT  'The running number', \\n  `sourceid` int NULL  COMMENT 'Channel number', \\n  `cmd` string NULL COMMENT  'command word', \\n  `orderid` string NULL COMMENT  'Order id (contains meowgoincrease and meowgoincrease, also contains cash recharge order id)', \\n  `imoneytype` int NULL  COMMENT 'Currency type 2', \\n  `distincid` string NULL COMMENT  'Visitor ID', \\n  `deviceuid` string NULL COMMENT  'device ID', \\n  `guildjob` int NULL  COMMENT 'Guild Position', \\n  `regtime` string NULL  COMMENT  'Account registration time'\\n) WITH (\\n   'connector' = 'doris',\\n    'jdbc-url'='jdbc:mysql://xxx:9030,xxx:9030,xxx:9030',\\n    'load-url'='xx:8030;xxx:8030;xxx:8030',\\n    'database-name' = 'ro_sea',\\n    'table-name' = 'dwd_moneyflow_log',\\n    'username' = 'root',\\n    'password' = 'root123',\\n    'sink.semantic' = 'exactly-once'\\n);\\n\\ncreate view kafka_out_money_view1  as \\nselect  IF(to_date(SPLIT_INDEX(message,'|',2)) > CURRENT_DATE + interval '1' day,CURRENT_DATE,to_date(SPLIT_INDEX(message,'|',2))) as `date`,\\n        `env`,`hostname` as hostname,`uuid`,\\n    try_cast(SPLIT_INDEX(message,'|',1) as int) as gamesvrid               ,                                  \\n        SPLIT_INDEX(message,'|',2) as dteventtime             ,                                  \\n        SPLIT_INDEX(message,'|',3) as vgameappid              ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',4) as int) as platid                  ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',5)  as int) as izoneareaid             ,                                  \\n        SPLIT_INDEX(message,'|',6) as vopenid                 ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',7) as int) as vroleid                 ,                                  \\n        SPLIT_INDEX(message,'|',8) as vrolename               ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',9)  as int)  as jobid                   ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',10) as int) as gender                  ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',11) as int) as ilevel                  ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',12) as int) as ijoblevel               ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',13) as int) as playerfriendsnum        ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',14) as int) as chargegold              ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',15) as int) as iviplevel               ,                                  \\n        SPLIT_INDEX(message,'|',16) as createtime              ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',17) as int)  as irolece                 ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',18) as int) as unionid                 ,                                  \\n        SPLIT_INDEX(message,'|',19) as unionname               ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',20) as int) as regchannel              ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',21) as int) as loginchannel            ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',22) as int) as `sequence`                ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',23) as int) as `reason`                 ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',24) as int) as subreason               ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',25) as int) as moneytype               ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',26) as int) as imoney                  ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',27) as int) as aftermoney              ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',28) as int) as afterboundmoney         ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',29) as int) as addorreduce             ,                                  \\n        SPLIT_INDEX(message,'|',30) as serialnum               ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',31) as int) as sourceid                ,                                  \\n        SPLIT_INDEX(message,'|',32) as `cmd`                    ,                                  \\n        SPLIT_INDEX(message,'|',33) as orderid                 ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',34) as int) as imoneytype              ,                                  \\n        SPLIT_INDEX(message,'|',35) as distincid               ,                                  \\n        SPLIT_INDEX(message,'|',36) as deviceuid               ,                                  \\n        try_cast(SPLIT_INDEX(message,'|',37) as int) as guildjob                ,                                  \\n        SPLIT_INDEX(message,'|',38) as regtime ,\\n      filebeat_ts,CURRENT_TIMESTAMP  as flink_ts\\nfrom kafka_in_money;\\n\\ninsert into kafka_out_money\\nselect \\ncast(`date` as varchar) as `date`,\\n`vroleid`,\\n`moneytype`,\\n`env`,\\n`hostname`,\\n`uuid`,\\n`filebeat_ts`,\\ncast(`flink_ts` as varchar) as flink_ts,\\n`gamesvrid`,\\n`dteventtime`,\\n`vgameappid`,\\n`platid`,\\n`izoneareaid`,\\n`vopenid`,\\n`vrolename`,\\n`jobid`,\\n`gender`,\\n`ilevel`,\\n`ijoblevel`,\\n`playerfriendsnum`,\\n`chargegold`,\\n`iviplevel`,\\n`createtime`,\\n`irolece`,\\n`unionid`,\\n`unionname`,\\n`regchannel`,\\n`loginchannel`,\\n`sequence`,\\n`reason`,\\n`subreason`,\\n`imoney`,\\n`aftermoney`,\\n`afterboundmoney`,\\n`addorreduce`,\\n`serialnum`,\\n`sourceid`,\\n`cmd`,\\n`orderid`,\\n`imoneytype`,\\n`distincid`,\\n`deviceuid`,\\n`guildjob`,\\n`regtime`\\nfrom kafka_out_money_view1;\\n```\\n\\n**[Advertisement system]: Use Flink SQL to slice and dice a row of log data to parse out the data in each field and complete the business calculations :**\\n\\n```sql\\nCREATE TABLE IF NOT EXISTS ods_adjust_in (\\n  log STRING\\n) WITH (\\n  'connector' = 'kafka',\\n  'topic' = 'ad_adjust_callback',\\n  'properties.bootstrap.servers' = 'emr_kafka1:9092,emr_kafka2:9092,emr_kafka3:9092',\\n  'properties.group.id' = 'ods_adjust_etl_20231011',\\n  'properties.max.partition.fetch.bytes' = '4048576',\\n  'scan.startup.mode' = 'group-offsets',\\n  'properties.isolation.level' = 'read_committed',\\n  'format' = 'raw'\\n);\\n\\nCREATE TABLE IF NOT EXISTS ods_af_in (\\n  log STRING\\n) WITH (\\n  'connector' = 'kafka',\\n  'topic' = 'ad_af_callback',\\n  'properties.bootstrap.servers' = 'emr_kafka1:9092,emr_kafka2:9092,emr_kafka3:9092',\\n  'properties.max.partition.fetch.bytes' = '4048576',\\n  'properties.group.id' = 'ods_af_etl_20231011',\\n  'scan.startup.mode' = 'group-offsets',\\n  'properties.isolation.level' = 'read_committed',\\n  'format' = 'raw'\\n);\\n\\nCREATE TABLE IF NOT EXISTS ods_mmp_out (\\n  `date` DATE,\\n  `mmp_type` STRING,\\n  `app_id` STRING,\\n  `event_name` STRING,\\n  `event_time` TIMESTAMP(3),\\n  `mmp_id` STRING,\\n  `distinct_id` STRING,\\n  `open_id` STRING,\\n  `account_id` STRING, \\n  `os_name` STRING, -- platform\\n  `country_code` STRING,\\n  `install_time` TIMESTAMP(3),\\n  `bundle_id` STRING,\\n  `media` STRING,\\n  `channel` STRING,\\n  `campaign` STRING,\\n  `campaign_id` STRING,\\n  `adgroup` STRING,\\n  `adgroup_id` STRING,\\n  `ad` STRING,\\n  `ad_id` STRING,\\n  `flink_ts` TIMESTAMP(3) comment 'Message time processed by flink',\\n  `device_properties` STRING,\\n  `log` STRING\\n) WITH (\\n    'connector' = 'kafka',\\n    'topic' = 'ods_mmp_log',\\n    'properties.bootstrap.servers' = 'emr_kafka1:9092,emr_kafka2:9092,emr_kafka3:9092',\\n    'scan.topic-partition-discovery.interval'='60000',\\n    'properties.group.id' = 'mmp2etl-out',\\n    'format' = 'json'\\n);\\n\\nINSERT INTO ods_mmp_out\\nSELECT\\n   `date`\\n  ,'Adjust' as `mmp_type`\\n  ,`app_token` as `app_id`\\n  ,`event_name`\\n  ,`event_time`\\n  ,`adid` as `mmp_id`\\n  ,`distinct_id`\\n  ,`open_id`\\n  ,`account_id`\\n  ,`os_name`\\n  ,`country_code`\\n  ,`install_time`\\n  ,'' as `bundle_id`\\n  ,'' as `media`\\n  ,`network` as `channel`\\n  ,`campaign`\\n  ,REGEXP_EXTRACT(`campaign`, '([\\\\\\\\(=])([a-z0-9]+)', 2) as `campaign_id`\\n  ,`adgroup`\\n  ,REGEXP_EXTRACT(`adgroup`, '([\\\\\\\\(=])([a-z0-9]+)', 2) as `adgroup_id`\\n  ,`creative` as `ad`\\n  ,REGEXP_EXTRACT(`creative`, '([\\\\\\\\(=])([a-z0-9]+)', 2) as `ad_id`\\n  ,`flink_ts`\\n  ,`device_properties`\\n  ,`log`\\nFROM (\\n  SELECT\\n     to_date(FROM_UNIXTIME(cast(JSON_VALUE(log,'$.created_at') as bigint))) as `date`\\n    ,JSON_VALUE(log,'$.app_token') as `app_token`\\n    ,JSON_VALUE(log,'$.adid') as `adid`\\n    ,LOWER(REPLACE( TRIM( COALESCE(JSON_VALUE(log,'$.event_name'), JSON_VALUE(log,'$.activity_kind')) ), ' ', '_')) as `event_name`\\n    ,TO_TIMESTAMP(FROM_UNIXTIME(cast(JSON_VALUE(log,'$.created_at') as bigint))) as `event_time`\\n    ,TO_TIMESTAMP(FROM_UNIXTIME(cast(JSON_VALUE(log,'$.installed_at') as bigint))) as `install_time`\\n    ,LOWER(CASE WHEN (JSON_VALUE(log,'$.network_name') = 'Unattributed') THEN COALESCE(JSON_VALUE(log,'$.fb_install_referrer_publisher_platform'),'facebook') ELSE JSON_VALUE(log,'$.network_name') END) AS `network`\\n    ,LOWER(CASE WHEN (JSON_VALUE(log,'$.network_name') = 'Unattributed') THEN (concat(JSON_VALUE(log,'$.fb_install_referrer_campaign_group_name'), '(', JSON_VALUE(log,'$.fb_install_referrer_campaign_group_id'), ')')) ELSE JSON_VALUE(log,'$.campaign_name') END) AS `campaign`\\n    ,LOWER(CASE WHEN (JSON_VALUE(log,'$.network_name') = 'Unattributed') THEN (concat(JSON_VALUE(log,'$.fb_install_referrer_campaign_name'), '(', JSON_VALUE(log,'$.fb_install_referrer_campaign_id'), ')')) ELSE JSON_VALUE(log,'$.adgroup_name') END) AS `adgroup`\\n    ,LOWER(CASE WHEN (JSON_VALUE(log,'$.network_name') = 'Unattributed') THEN (concat(JSON_VALUE(log,'$.fb_install_referrer_adgroup_name'), '(', JSON_VALUE(log,'$.fb_install_referrer_adgroup_id'), ')')) ELSE JSON_VALUE(log,'$.creative_name') END) AS `creative`\\n    ,JSON_VALUE(log,'$.os_name') as `os_name`\\n    ,JSON_VALUE(log,'$.country_code') as `country_code`\\n    ,JSON_VALUE(JSON_VALUE(log,'$.publisher_parameters'), '$.ta_distinct_id') as `distinct_id`\\n    ,JSON_VALUE(JSON_VALUE(log,'$.publisher_parameters'), '$.open_id') as `open_id`\\n    ,JSON_VALUE(JSON_VALUE(log,'$.publisher_parameters'), '$.ta_account_id') as `account_id`\\n    ,CURRENT_TIMESTAMP as `flink_ts`\\n    ,JSON_OBJECT(\\n       'ip' VALUE JSON_VALUE(log,'$.ip')\\n      ,'ua' VALUE JSON_VALUE(log,'$.ua')\\n      ,'idfa' VALUE JSON_VALUE(log,'$.idfa')\\n      ,'idfv' VALUE JSON_VALUE(log,'$.idfv')\\n      ,'gps_adid' VALUE JSON_VALUE(log,'$.gps_adid')\\n      ,'android_id' VALUE JSON_VALUE(log,'$.android_id')\\n      ,'mac_md5' VALUE JSON_VALUE(log,'$.mac_md5')\\n      ,'oaid' VALUE JSON_VALUE(log,'$.oaid')\\n      ,'gclid' VALUE JSON_VALUE(log,'$.gclid')\\n      ,'gbraid' VALUE JSON_VALUE(log,'$.gbraid')\\n      ,'dcp_wbraid' VALUE JSON_VALUE(log,'$.dcp_wbraid')\\n    ) as `device_properties`\\n    ,`log`\\n  FROM ods_adjust_in \\n  WHERE COALESCE(JSON_VALUE(log,'$.activity_kind'), JSON_VALUE(log,'$.event_name')) not in ('impression', 'click')\\n)\\nUNION ALL \\nSELECT\\n  to_date(CONVERT_TZ(JSON_VALUE(log,'$.event_time'), 'UTC', 'Asia/Shanghai')) as `date`\\n  ,'AppsFlyer' as `mmp_type`\\n  ,JSON_VALUE(log,'$.app_id') as `app_id`\\n  ,LOWER(REPLACE( TRIM(JSON_VALUE(log,'$.event_name') ), ' ', '-')) as `event_name`\\n  ,TO_TIMESTAMP(CONVERT_TZ(JSON_VALUE(log,'$.event_time'), 'UTC', 'Asia/Shanghai')) as `event_time`\\n  ,JSON_VALUE(log,'$.appsflyer_id') as `mmp_id`\\n  ,JSON_VALUE(JSON_VALUE(log,'$.custom_data'), '$.ta_distinct_id') as `distinct_id`\\n  ,JSON_VALUE(JSON_VALUE(log,'$.custom_data'), '$.open_id') as `open_id`\\n  ,JSON_VALUE(JSON_VALUE(log,'$.custom_data'), '$.ta_account_id') as `account_id`\\n  ,LOWER(JSON_VALUE(log,'$.platform')) AS `os_name`\\n  ,LOWER(JSON_VALUE(log,'$.country_code')) as `country_code`\\n  ,TO_TIMESTAMP(CONVERT_TZ(JSON_VALUE(log,'$.install_time'), 'UTC', 'Asia/Shanghai')) as `install_time`\\n  ,LOWER(JSON_VALUE(log,'$.bundle_id')) AS `bundle_id`\\n  ,LOWER(JSON_VALUE(log,'$.media_source')) AS `media`\\n  ,LOWER(JSON_VALUE(log,'$.af_channel')) AS `channel`\\n  ,CONCAT(LOWER(JSON_VALUE(log,'$.campaign')), ' (', LOWER(JSON_VALUE(log,'$.af_c_id')), ')') AS `campaign`\\n  ,LOWER(JSON_VALUE(log,'$.af_c_id')) AS `campaign_id`\\n  ,CONCAT(LOWER(JSON_VALUE(log,'$.af_adset')), ' (', LOWER(JSON_VALUE(log,'$.af_adset_id')), ')') AS `adgroup`\\n  ,LOWER(JSON_VALUE(log,'$.af_adset_id')) AS `adgroup_id`\\n  ,CONCAT(LOWER(JSON_VALUE(log,'$.af_ad')), ' (', LOWER(JSON_VALUE(log,'$.af_ad_id')), ')') AS `ad`\\n  ,LOWER(JSON_VALUE(log,'$.af_ad_id')) AS `ad_id`\\n  ,CURRENT_TIMESTAMP as `flink_ts`\\n  ,JSON_OBJECT(\\n     'ip' VALUE JSON_VALUE(log,'$.ip')\\n    ,'ua' VALUE JSON_VALUE(log,'$.user_agent')\\n    ,'idfa' VALUE JSON_VALUE(log,'$.idfa')\\n    ,'idfv' VALUE JSON_VALUE(log,'$.idfv')\\n    ,'gps_adid' VALUE JSON_VALUE(log,'$.advertising_id')\\n    ,'android_id' VALUE JSON_VALUE(log,'$.android_id')\\n    ,'oaid' VALUE JSON_VALUE(log,'$.oaid')\\n  ) as `device_properties`\\n  ,`log`\\nFROM ods_af_in;\\n```\\n\\n**Data reporting**: similar to Shenzhe\\n\\n![](/blog/joymaker/data_reporting.png)\\n\\n## **6.  Practical - Tencent Cloud Environment**\\n\\nThe Flink task in this article is running in Tencent's kubernetes environment, which includes some related knowledge points that will not be detailed here, for example:\\n\\n- **Mirror synchronisation**: harbor configures automatic synchronisation of mirrors to the Tencent cloud mirror service TCR.\\n\\n- **Mirror configuration**: such as secret-free pull, cache acceleration.\\n\\n- **Flink webui extranet access**: need to buy from the extranet lb in order to access Flink UI.\\n\\n- **Environmental access**: because the kubernetes cluster is mainly a closed intranet computing environment, with the system network, security groups, permissions, routing, etc. need to be well configured and tested, in order to let the task really run in the production environment better.\\n\\n- **Security Group Configuration**: In order to pull real-time data from overseas games and write it to the domestic Kafka cluster to ensure a smooth data flow, we use a dedicated line and open a dedicated extranet access to the kubernetes Flink super node to achieve this.\\n\\nRelevant screenshots:\\n\\n![](/blog/joymaker/cloud_environment.png)\\n\\n![](/blog/joymaker/instance_management.png)\\n\\n## **7.  Benefits and expectations**\\n\\n**After using Apache StreamPark, the biggest impression we got is that StreamPark is very easy to use. With simple configuration, we can quickly deploy Flink tasks to any cluster and monitor and manage thousands of jobs in real time, which is very nice!**\\n\\n- The benefits are as follows: A large number of real-time tasks were quickly migrated to Kubernetes via StreamPark. After YARN was dropped, the Apache Doris cluster, which had been a part of the early mix, became very stable after there was no resource contention.\\n\\n- When replenishing data, we can quickly borrow the capacity of super nodes in TKE, and through the StreamPark platform, it is easy to populate and shrink thousands of pods in a minute, which makes replenishment very efficient.\\n\\n- The technology stack has become more advanced and unified. The pile of components in the era of big data has become more and more efficient and unified. At present, Joy Big Data has already completed the Kubernetes cloud nativeisation of a series of components such as Doris, Flink, Spark, Hive, Grafana, Prometheus, etc., which makes operation and maintenance simpler and more efficient, and StreamPark has played a great role in the Flink containerisation process.\\n\\n- StreamPark platform is easy to operate, complete, stable and responsive to the community, which really makes stream processing job development more and more simple, and brings us a lot of convenience, allowing us to focus more on business, kudos~.\\n\\nHere, we also expect StreamPark can do better and better, here are some optimisation suggestions:\\n\\n- **Support Operator Deployment**: Expect Flink on kubernetes to support operator deployment method because of its benefits of customising resource types, specifying invocation order, monitoring metrics and so on.\\n\\n- **Support autoscaler**: Because real-time streaming tasks have business peaks and valleys, special weekly events, doubled data volume, and artificial periodic adjustment of parallelism is not very suitable. Therefore, in combination with Kubernetes, we expect StreamPark to complete the operator way to create a Flink cluster as soon as possible. In Flink 1.17 or above, you can set a better threshold, use the automatic pop-up shrinkage to achieve the peak period to pop up more resources to process the data, and in the trough period to shrink the resources to save the cost, and we are very much looking forward to it.\\n\\n- **programme error logic**: the current Flink task set a fixed number of times or ratio retry, but the current StreamPark mail alarms are particularly frequent, not the expected task fails once for an alarm, so I hope that it will be further upgraded.\\n\\nFinally, we would like to thank the **Apache StreamPark** community for their selfless help in using StreamPark. Their professionalism and user-centred attitude has enabled us to use this powerful framework more efficiently and smoothly. We look forward to StreamPark getting better and better, and becoming a model for up-and-coming Apache projects!\"},{\"id\":\"streampark-usercase-chinaunion\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-chinaunion\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/2-streampark-usercase-chinaunion.md\",\"source\":\"@site/blog/2-streampark-usercase-chinaunion.md\",\"title\":\"China Union's Flink Real-Time Computing Platform Ops Practice\",\"description\":\"Abstract\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"},{\"label\":\"FlinkSQL\",\"permalink\":\"/blog/tags/flink-sql\"}],\"readingTime\":16.34,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-chinaunion\",\"title\":\"China Union's Flink Real-Time Computing Platform Ops Practice\",\"tags\":[\"StreamPark\",\"Production Practice\",\"FlinkSQL\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Apache StreamPark\u2122 Cloud Native Platform Practice at Joymaker\",\"permalink\":\"/blog/streampark-usercase-joymaker\"},\"nextItem\":{\"title\":\"Based on Apache Paimon + Apache StreamPark\u2122's Streaming Data Warehouse Practice by Bondex\",\"permalink\":\"/blog/streampark-usercase-bondex-with-paimon\"}},\"content\":\"![](/blog/chinaunion/overall_architecture_en.png)\\n\\n**Abstract:** This article is compiled from the sharing of Mu Chunjin, the head of China Union Data Science's real-time computing team and Apache StreamPark Committer, at the Flink Forward Asia 2022 platform construction session. The content of this article is mainly divided into four parts:\\n\\n- Introduction to the Real-Time Computing Platform Background\\n- Operational Challenges of Flink Real-Time Jobs\\n- Integrated Management Based on StreamPark\\n- Future Planning and Evolution\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## **Introduction to the Real-Time Computing Platform Background**\\n\\nThe image above depicts the overall architecture of the real-time computing platform. At the bottom layer, we have the data sources. Due to some sensitive information, the detailed information of the data sources is not listed. It mainly includes three parts: business databases, user behavior logs, and user location. China Union has a vast number of data sources, with just the business databases comprising tens of thousands of tables. The data is primarily processed through Flink SQL and the DataStream API. The data processing workflow includes real-time parsing of data sources by Flink, real-time computation of rules, and real-time products. Users perform real-time data subscriptions on the visualization subscription platform. They can draw an electronic fence on the map and set some rules, such as where the data comes from, how long it stays inside the fence, etc. They can also filter some features. User information that meets these rules will be pushed in real-time. Next is the real-time security part. If a user connects to a high-risk base station or exhibits abnormal operational behavior, we may suspect fraudulent activity and take actions such as shutting down the phone number, among other things. Additionally, there are some real-time features of users and a real-time big screen display.\\n\\n\\n\\n![](/blog/chinaunion/data_processing_processes_en.png)\\n\\nThe image above provides a detailed workflow of data processing.\\n\\nThe first part is collection and parsing. Our data sources come from business databases, including OGG and DTS format messages, log messages, user behavior, and user location data, totaling over 50 different data sources. This number is expected to gradually increase. All data sources are processed in real-time using Flink, and Metrics have been added to monitor the latency of data sources.\\n\\nThe second part is real-time computing. This stage deals with a massive amount of data, in the trillions, supporting over 10,000 real-time data subscriptions. There are more than 200 Flink tasks. We encapsulate a certain type of business into a scenario, and a single Flink job can support multiple subscriptions in the same scenario. Currently, the number of Flink jobs is continuously increasing, and in the future, it might increase to over 500. One of the major challenges faced here is the real-time association of trillion-level data with electronic fences and user features on a daily basis. There are tens of thousands of electronic fences, and user features involve hundreds of millions of users. Initially, we stored electronic fence information and user features in HBase, but this led to significant pressure on HBase, frequent performance issues, and data latency. Furthermore, once data backlog occurred, it took a long time to clear. Thanks to the powerful Flink State, we have now stored the electronic fence information and user features in the state, which has adequately supported high-concurrency scenarios. We have also added performance monitoring for data processing. Finally, there are some applications for real-time products and marketing touchpoints on the front end.\\n\\n![](/blog/chinaunion/platform_evolution_en.png)\\n\\nIn 2018, we adopted a third-party black-box computing engine, which did not support flexible customization of personalized functions, and depended heavily on external systems, resulting in high loads on these external systems and complex operations and maintenance. In 2019, we utilized Spark Streaming's micro-batch processing. From 2020, we began to use Flink for stream computing. Starting from 2021, almost all Spark Streaming micro-batch processing tasks have been replaced by Flink. At the same time, Apache StreamPark was launched to manage our Flink jobs.\\n\\n![](/blog/chinaunion/platform_background_en.png)\\n\\nTo summarize the platform background, it mainly includes the following parts:\\n\\n- Large data volume: processing an average of trillions of data per day.\\n- Numerous data sources: integrated with more than 50 types of real-time data sources.\\n- Numerous subscriptions: supported more than 10,000 data service subscriptions.\\n- Numerous users: supported the usage of more than 30 internal and external users.\\n\\n![](/blog/chinaunion/operational_background_en.png)\\n\\nThe operational maintenance background can also be divided into the following parts:\\n\\n- High support demand: More than 50 types of data sources, and over 10,000 data service subscriptions.\\n- Numerous real-time jobs: Currently, there are 200+ Flink production jobs, and the number is continuously and rapidly increasing, potentially reaching 500+ in the future.\\n- High frequency of launches: There are new or enhanced Flink jobs going live every day.\\n- Numerous developers: Over 50 R&D personnel are involved in developing Flink real-time computing tasks.\\n- Numerous users: Over 30 internal and external organizations' users are utilizing the platform.\\n- Low monitoring latency: Once an issue is identified, we must address it immediately to avoid user complaints.\\n\\n## **Flink Real-Time Job Operation and Maintenance Challenges**\\n\\n![](/blog/chinaunion/difficulties_en.png)\\n\\nGiven the platform and operational maintenance background, particularly with the increasing number of Flink jobs, we have encountered significant challenges in two main areas: job operation and maintenance dilemmas, and business support difficulties.\\n\\n\\n\\nIn terms of job operation and maintenance dilemmas, firstly, the job deployment process is lengthy and inefficient. Under China Union's principle that security is the top priority, deploying programs on servers involves connecting to a VPN, logging in through the 4A system, packaging and compiling, deploying, and then starting the program. This entire process is quite long. Initially, when developing Flink, we started jobs using scripts, leading to uncontrollable code branches. After deployment, it was also difficult to trace back. Moreover, it's challenging to synchronize scripts with code on git because developers tend to prefer directly modifying scripts on the server, easily forgetting to upload changes to git.\\n\\n\\n\\nDue to various factors in the job operation and maintenance difficulties, business support challenges arise, such as a high rate of failures during launch, impact on data quality, lengthy launch times, high data latency, and issues with missed alarm handling, leading to complaints. In addition, the impact on our business is unclear, and once a problem arises, addressing the issue becomes the top priority.\\n\\n## **Integrated Management based on Apache StreamPark\u2122**\\n\\n![](/blog/chinaunion/job_management_en.png)\\n\\nIn response to the two dilemmas mentioned above, we have resolved many issues through StreamPark's integrated management. First, let's take a look at the dual evolution of StreamPark, which includes Flink Job Management and Flink Job DevOps Platform. In terms of job management, StreamPark supports deploying Flink real-time jobs to different clusters, such as Flink's native Standalone mode, and the Session, Application, and PerJob modes of Flink on Yarn. In the latest version, it will support Kubernetes Native Session mode. The middle layer includes project management, job management, cluster management, team management, variable management, and alarm management.\\n\\n\\n\\n- Project Management: When deploying a Flink program, you can fill in the git address in project management and select the branch you want to deploy.\\n- Job Management: You can specify the execution mode of the Flink job, such as which type of cluster you want to submit to. You can also configure some resources, such as the number of TaskManagers, the memory size of TaskManager/JobManager, parallelism, etc. Additionally, you can set up some fault tolerance measures; for instance, if a Flink job fails, StreamPark can support automatic restarts, and it also supports the input of some dynamic parameters.\\n- Cluster Management: You can add and manage big data clusters through the interface.\\n- Team Management: In the actual production process of an enterprise, there are multiple teams, and these teams are isolated from each other.\\n- Variable Management: You can maintain some variables in one place. For example, you can define Kafka's Broker address as a variable. When configuring Flink jobs or SQL, you can replace the Broker's IP with a variable. Moreover, if this Kafka needs to be decommissioned later, you can also use this variable to check which jobs are using this cluster, facilitating some subsequent processes.\\n- Alarm Management: Supports multiple alarm modes, such as WeChat, DingTalk, SMS, and email.\\n\\n\\nStreamPark supports the submission of Flink SQL and Flink Jar, allows for resource configuration, and supports state tracking, indicating whether the state is running, failed, etc. Additionally, it provides a metrics dashboard and supports the viewing of various logs.\\n\\n![](/blog/chinaunion/devops_platform_en.png)\\n\\nThe Flink Job DevOps platform primarily consists of the following parts:\\n- Teams: StreamPark supports multiple teams, each with its team administrator who has all permissions. There are also team developers who only have a limited set of permissions.\\n- Compilation and Packaging: When creating a Flink project, you can configure the git address, branch, and packaging commands in the project, and then compile and package with a single click of the build button.\\n- Release and Deployment: During release and deployment, a Flink job is created. Within the Flink job, you can choose the execution mode, deployment cluster, resource settings, fault tolerance settings, and fill in variables. Finally, the Flink job can be started or stopped with a single click.\\n- State Monitoring: After the Flink job is started, real-time tracking of its state begins, including Flink's running status, runtime duration, Checkpoint information, etc. There is also support for one-click redirection to Flink's Web UI.\\n- Logs and Alerts: This includes logs from the build and start-up processes and supports alerting methods such as DingTalk, WeChat, email, and SMS.\\n\\n![](/blog/chinaunion/multi_team_support_en.png)\\n\\nCompanies generally have multiple teams working on real-time jobs simultaneously. In our company, this includes a real-time data collection team, a data processing team, and a real-time marketing team. StreamPark supports resource isolation for multiple teams.\\n\\n![](/blog/chinaunion/platformized_management_en.png)\\n\\nManagement of the Flink job platform faces the following challenges:\\n- Numerous scripts: There are several hundred scripts on the platform, scattered across multiple servers.\\n- Various types of scripts: When starting Flink jobs, there are start scripts, stop scripts, and daemon scripts, and it is very difficult to control operation permissions.\\n- Inconsistent scripts: The scripts on the server are inconsistent with the scripts on git.\\n- Difficult to ascertain script ownership: It is unclear who is responsible for the Flink jobs and their purpose.\\n- Uncontrollable branches: When starting a job, you need to specify the git branch in the script, resulting in untraceable branches.\\n\\n\\n\\nBased on the challenges mentioned above, StreamPark has addressed the issues of unclear ownership and untraceable branches through project management. This is because when creating a project, you need to manually specify certain branches. Once the packaging is successful, these branches are recorded. Job management centralizes configurations, preventing scripts from being too dispersed. Additionally, there is strict control over the permissions for starting and stopping jobs, preventing an uncontrollable state due to script permissions. StreamPark interacts with clusters through interfaces to obtain job information, allowing for more precise job control.\\n\\n\\n\\nReferring to the image above, you can see at the bottom of the diagram that packaging is conducted through project management, configuration is done via job management, and then it is released. This process allows for one-click start and stop operations, and jobs can be submitted through the API.\\n\\n![\u56fe\u7247](/blog/chinaunion/development_efficiency_en.png)\\n\\nIn the early stages, we needed to go through seven steps for deployment, including connecting to a VPN, logging in through 4A, executing compile scripts, executing start scripts, opening Yarn, searching for the job name, and entering the Flink UI. StreamPark supports one-click deployment for four of these steps, including one-click packaging, one-click release, one-click start, and one-click access to the Flink UI.\\n\\n![\u56fe\u7247](/blog/chinaunion/submission_process_en.png)\\n\\nThe image above illustrates the job submission process of our StreamPark platform. Firstly, StreamPark proceeds to release the job, during which some resources are uploaded. Following that, the job is submitted, accompanied by various configured parameters, and it is published to the cluster using the Flink Submit method via an API call. At this point, there are multiple Flink Submit instances corresponding to different execution modes, such as Yarn Session, Yarn Application, Kubernetes Session, Kubernetes Application, and so on; all of these are controlled here. After submitting the job, if it is a Flink on Yarn job, the platform will acquire the Application ID or Job ID of the Flink job. This ID is then stored in our database. Similarly, if the job is executed based on Kubernetes, a Job ID will be obtained. Subsequently, when tracking the job status, we primarily use these stored IDs to monitor the state of the job.\\n\\n![\u56fe\u7247](/blog/chinaunion/status_acquisition_bottleneck_en.png)\\n\\nAs mentioned above, in the case of Flink on Yarn jobs, two IDs are acquired upon job submission: the Application ID and the Job ID. These IDs are used to retrieve the job status. However, when there is a large number of Flink jobs, certain issues may arise. StreamPark utilizes a status retriever that periodically sends requests to the ResourceManager every five seconds, using the Application ID or Job ID stored in our database. If there are a considerable number of jobs, during each polling cycle, the ResourceManager is responsible for calling the Job Manager's address to access its status. This can lead to significant pressure on the number of connections to the ResourceManager and an overall increase in the number of connections.\\n\\n\\n\\nIn the diagram mentioned earlier, the connection count to the ResourceManager shows periodic and sustained increases, indicating that the ResourceManager is in a relatively critical state. This is evidenced by monitoring data from the server, which indeed shows a higher number of connections to the ResourceManager.\\n\\n![\u56fe\u7247](/blog/chinaunion/state_optimization_en.png)\\n\\nTo address the issues mentioned above, we have made some optimizations in StreamPark. Firstly, after submitting a job, StreamPark saves the Application ID or Job ID, and it also retrieves and stores the direct access address of the Job Manager in the database. Therefore, instead of polling the ResourceManager for job status, it can directly call the addresses of individual Job Managers to obtain the real-time status. This significantly reduces the number of connections to the ResourceManager. As can be seen from the latter part of the diagram above, there are basically no significant spikes in connection counts, which substantially alleviates the pressure on the ResourceManager. Moreover, this ensures that as the number of Flink jobs continues to grow, the system will not encounter bottlenecks in status retrieval.\\n\\n![\u56fe\u7247](/blog/chinaunion/state_recovery_en.png)\\n\\nAnother issue that StreamPark resolves is safeguarding Flink's state recovery. In the past, when we used scripts for operations and maintenance, especially during business upgrades, it was necessary to recover from the latest checkpoint when starting Flink. However, developers often forgot to recover from the previous checkpoint, leading to significant data quality issues and complaints. StreamPark's process is designed to mitigate this issue. Upon the initial start of a Flink job, it polls every five seconds to retrieve checkpoint records, saving them in a database. When manually stopping a Flink job through StreamPark, users have the option to perform a savepoint. If this option is selected, the path of the savepoint is saved in the database. In addition, records of each checkpoint are also stored in the database. When restarting a Flink job, the system defaults to using the latest checkpoint or savepoint record. This effectively prevents issues associated with failing to recover from the previous checkpoint. It also avoids the resource wastage caused by having to rerun jobs with offset rollbacks to address problems, while ensuring consistency in data processing.\\n\\n![\u56fe\u7247](/blog/chinaunion/multiple_environments_and_components_en.png)\\n\\nStreamPark also addresses the challenges associated with referencing multiple components across various environments. In a corporate setting, there are typically multiple environments, such as development, testing, and production. Each environment generally includes multiple components, such as Kafka, HBase, Redis, etc. Additionally, within a single environment, there may be multiple instances of the same component. For example, in a real-time computing platform at China Union, when consuming data from an upstream Kafka cluster and writing the relevant data to a downstream Kafka cluster, two sets of Kafka are involved within the same environment. It can be challenging to determine the specific environment and component based solely on IP addresses. To address this, we define the IP addresses of all components as variables. For instance, the Kafka cluster variable, Kafka.cluster, exists in development, testing, and production environments, but it points to different Broker addresses in each. Thus, regardless of the environment in which a Flink job is configured, referencing this variable is sufficient. This approach significantly reduces the incidence of operational failures in production environments.\\n\\n![\u56fe\u7247](/blog/chinaunion/multiple_execution_modes_en.png)\\n\\nStreamPark supports multiple execution modes for Flink, including three deployment modes based on Yarn: Application, Perjob, and Session. Additionally, it supports two deployment modes for Kubernetes: Application and Session, as well as some Remote modes.\\n\\n![\u56fe\u7247](/blog/chinaunion/versioning_en.png)\\n\\nStreamPark also supports multiple versions of Flink. For example, while our current version is 1.14.x, we would like to experiment with the new 1.16.x release. However, it\u2019s not feasible to upgrade all existing jobs to 1.16.x. Instead, we can opt to upgrade only the new jobs to 1.16.x, allowing us to leverage the benefits of the new version while maintaining compatibility with the older version.\\n\\n## **Future Planning and Evolution**\\n\\n![\u56fe\u7247](/blog/chinaunion/contribution_and_enhancement_en.png)\\n\\nIn the future, we will increase our involvement in the development of StreamPark, and we have planned the following directions for enhancement:\\n- High Availability: StreamPark currently does not support high availability, and this aspect needs further strengthening.\\n- State Management: In enterprise practices, each operator in a Flink job has a UID. If the Flink UID is not set, it could lead to situations where state recovery is not possible when upgrading the Flink job. This issue cannot be solved through the platform at the moment. Therefore, we plan to add this functionality to the platform. We will introduce a feature that checks whether the operator has a UID set when submitting a Flink Jar. If not, a reminder will be issued to avoid state recovery issues every time a Flink job is deployed. Previously, when facing such situations, we had to use the state processing API to deserialize from the original state, and then create a new state using the state processing API for the upgraded Flink to load.\\n- More Detailed Monitoring: Currently, StreamPark supports sending alerts when a Flink job fails. We hope to also send alerts when a Task fails, and need to know the reason for the failure. In addition, enhancements are needed in job backpressure monitoring alerts, Checkpoint timeout alerts, failure alerts, and performance metric collection.\\n- Stream-Batch Integration: Explore a platform that integrates both streaming and batch processing, combining the Flink stream-batch unified engine with data lake storage that supports stream-batch unification.\\n\\n![](/blog/chinaunion/road_map_en.png)\\n\\nThe above diagram represents the Roadmap for StreamPark.\\n- Data Source: StreamPark will support rapid integration with more data sources, achieving one-click data onboarding.\\n- Operation Center: Acquire more Flink Metrics to further enhance the capabilities in monitoring and operation.\\n- K8S-operator: The existing Flink on K8S is somewhat cumbersome, having gone through the processes of packaging Jars, building images, and pushing images. There is a need for future improvements and optimization, and we are actively embracing the upstream K8S-operator integration.\\n- Streaming Data Warehouse: Enhance support for Flink SQL job capabilities, simplify the submission of Flink SQL jobs, and plan to integrate with Flink SQL Gateway. Enhance capabilities in the SQL data warehouse domain, including metadata storage, unified table creation syntax validation, runtime testing, and interactive queries, while actively embracing Flink upstream to explore real-time data warehouses and streaming data warehouses.\"},{\"id\":\"streampark-usercase-bondex-with-paimon\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-bondex-with-paimon\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/3-streampark-usercase-bondex-paimon.md\",\"source\":\"@site/blog/3-streampark-usercase-bondex-paimon.md\",\"title\":\"Based on Apache Paimon + Apache StreamPark\u2122's Streaming Data Warehouse Practice by Bondex\",\"description\":\"Foreword: This article mainly introduces the implementation of a streaming data warehouse by Bondex, a supply chain logistics service provider, in the process of digital transformation using the Paimon + StreamPark platform. We provide an easy-to-follow operational manual with the Apache StreamPark integrated stream-batch platform to help users submit Flink tasks and quickly master the use of Paimon.\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"},{\"label\":\"paimon\",\"permalink\":\"/blog/tags/paimon\"},{\"label\":\"streaming-warehouse\",\"permalink\":\"/blog/tags/streaming-warehouse\"}],\"readingTime\":27.255,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-bondex-with-paimon\",\"title\":\"Based on Apache Paimon + Apache StreamPark\u2122's Streaming Data Warehouse Practice by Bondex\",\"tags\":[\"StreamPark\",\"Production Practice\",\"paimon\",\"streaming-warehouse\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"China Union's Flink Real-Time Computing Platform Ops Practice\",\"permalink\":\"/blog/streampark-usercase-chinaunion\"},\"nextItem\":{\"title\":\"Apache StreamPark\u2122 in the Large-Scale Production Practice at Shunwang Technology\",\"permalink\":\"/blog/streampark-usercase-shunwang\"}},\"content\":\"![](/blog/bondex/Bondex.png)\\n\\n**Foreword: **This article mainly introduces the implementation of a streaming data warehouse by Bondex, a supply chain logistics service provider, in the process of digital transformation using the Paimon + StreamPark platform. We provide an easy-to-follow operational manual with the Apache StreamPark integrated stream-batch platform to help users submit Flink tasks and quickly master the use of Paimon.\\n\\n- Introduction to Company Business\\n- Pain Points and Selection in Big Data Technology\\n- Production Practice\\n- Troubleshooting Analysis\\n- Future Planning\\n\\n\x3c!-- truncate --\x3e\\n\\n## 01 Introduction to Company Business\\n\\nBondex Group has always focused on the field of supply chain logistics. By creating an excellent international logistics platform, it provides customers with end-to-end one-stop intelligent supply chain logistics services. The group currently has over 2,000 employees, an annual turnover of more than 12 billion RMB, a network covering over 200 ports globally, and more than 80 branches and subsidiaries at home and abroad, aiding Chinese enterprises to connect with the world.\\n\\n### **Business Background**\\n\\nAs the company continues to expand and the complexity of its business increases, in order to better achieve resource optimization and process improvement, the Operations and Process Management Department needs to monitor the company's business operations in real time to ensure the stability and efficiency of business processes.\\n\\nThe Operations and Process Management Department is responsible for overseeing the execution of various business processes within the company, including the volume of orders for sea, air, and rail transport across different regions and business divisions, large customer order volumes, route order volumes, the amount of business entrusted to each operation site for customs, warehousing, and land transportation, as well as the actual revenue and expenses of each region and business division on the day. Through monitoring and analysis of these processes, the company can identify potential issues and bottlenecks, propose measures for improvement and suggestions, in order to optimize operational efficiency.\\n\\n**Real-Time Data Warehouse Architecture:**\\n\\n![](/blog/bondex/realtime_warehouse.png)\\n\\nThe current system requires direct collection of real-time data from the production system, but there are multiple data sources that need to be associated for queries. The Fanruan report is not user-friendly when dealing with multiple data sources and cannot re-aggregate multiple data sources. Scheduled queries to the production system database can put pressure on it, affecting the stable operation of the production system. Therefore, we need to introduce a warehouse that can handle real-time data through [Flink CDC](https://github.com/ververica/flink-cdc-connectors) technology for stream processing. This data warehouse needs to be able to collect real-time data from multiple data sources and on this basis, perform complex associated SQL queries, machine learning, etc., and avoid unscheduled queries to the production system, thereby reducing the load on the production system and ensuring its stable operation.\\n\\n## 02 Big Data Technology Pain Points and Selection\\n\\nSince its establishment, the Bondex big data team has always focused on using efficient operational tools or platforms to achieve effective staffing arrangements, optimize repetitive labor, and reduce manual operations.\\n\\nWhile offline batch data processing has been able to support the group's basic cockpit and management reporting, the Transportation Management Department of the group has proposed the need for real-time statistics on order quantities and operational volumes. The finance department has expressed the need for a real-time display of cash flow. In this context, a big data-based integrated stream-batch solution became imperative.\\n\\nAlthough the big data department has already utilized [Apache Doris](https://github.com/apache/doris) for integrated storage and computing of lakehouse architecture and has published articles on lakehouse construction in the Doris community, there are some issues that need to be addressed, such as the inability to reuse streaming data storage, the inaccessibility of intermediate layer data, and the inability to perform real-time aggregation calculations.\\n\\nSorted by the evolution of architecture over recent years, the common architectural solutions are as follows:\\n\\n### **Hadoop Architecture**\\n\\nThe demarcation point between traditional data warehouses and internet data warehouses dates back to the early days of the internet when the requirements for data analysis were not high, mainly focusing on reports with low real-time needs to support decision-making. This gave rise to offline data analysis solutions.\\n\\n**Advantages: **Supports a rich variety of data types, capable of massive computations, low requirements for machine configurations, low timeliness, fault-tolerant.\\n\\n**Disadvantages: **Does not support real-time; complex to maintain; the query optimizer is not as good as MPP, slow response.\\n\\nSelection Basis: Does not support real-time; maintenance is complex, which does not conform to the principle of streamlined staffing; poor performance.\\n\\n### **Lambda Architecture**\\n\\nLambda Architecture is a real-time big data processing framework proposed by Nathan Marz, the author of Storm. Marz developed the famous real-time big data processing framework [Apache Storm](https://github.com/apache/storm) while working at Twitter, and the Lambda Architecture is the culmination of his years of experience in distributed big data systems.\\n\\n![](/blog/bondex/lambda.png)\\n\\nData stream processing is divided into three layers: Serving Layer, Speed Layer, and Batch Layer:\\n\\n- Batch Layer: Processes offline data and eventually provides a view service to the business;\\n- Speed Layer: Processes real-time incremental data and eventually provides a view service to the business;\\n- Serving Layer: Responds to user requests, performs aggregation calculations on offline and incremental data, and ultimately provides the service;\\n\\nThe advantage is that offline and real-time computations are separated, using two sets of frameworks, which makes the architecture stable.\\n\\nThe disadvantage is that it is difficult to maintain consistency between offline and real-time data, and operational staff need to maintain two sets of frameworks and three layers of architecture. Developers need to write three sets of code.\\n\\nSelection Basis: Data consistency is uncontrollable; operations and development require significant workload, which does not conform to the principle of streamlined staffing.\\n\\n### **Kappa Architecture**\\n\\nThe Kappa Architecture uses a single stream-processing framework to address both offline and real-time data, solving all problems with a real-time stream, with the goal of providing fast and reliable query access to results. It is highly suitable for various data processing workloads, including continuous data pipelines, real-time data processing, machine learning models and real-time analytics, IoT systems, and many other use cases with a single technology stack.\\n\\nIt is typically implemented using a streaming processing engine such as Apache Flink, Apache Storm, Apache Kinesis, Apache Kafka, designed to handle large data streams and provide fast and reliable query results.\\n\\n![](/blog/bondex/kappa.png)\\n\\n**Advantages: **Single data stream processing framework.\\n\\n**Disadvantages: **Although its architecture is simpler compared to Lambda Architecture, the setup and maintenance of the streaming processing framework are relatively complex, and it lacks true offline data processing capabilities; storing large amounts of data in streaming platforms can be costly.\\n\\nSelection Basis: The capability for offline data processing needs to be retained, and costs controlled.\\n\\n### **Iceberg**\\n\\nTherefore, we also researched [Apache Iceberg](https://github.com/apache/iceberg), whose snapshot feature can to some extent achieve streaming-batch integration. However, the issue with it is that the real-time table layer based on Kafka is either not queryable or cannot reuse existing tables, with a strong dependency on Kafka. It requires the use of Kafka to write intermediate results to Iceberg tables, increasing system complexity and maintainability.\\n\\nSelection Basis: A Kafka-free real-time architecture has been implemented, intermediate data cannot be made queryable or reusable.\\n\\n### **Streaming Data Warehouse (Continuation of the Kappa Architecture)**\\n\\nSince the FTS0.3.0 version, the BonDex big data team has participated in the construction of a streaming data warehouse, aiming to further reduce the complexity of the data processing framework and streamline personnel configuration. The initial principle was to get involved with the trend, to continuously learn and improve, and to move closer to cutting-edge technology. The team unanimously believes that it is essential to embrace challenges and \\\"cross the river by feeling the stones.\\\" Fortunately, after several iterations, the problems that initially arose have been gradually resolved with the efficient cooperation of the community.\\n\\n**The architecture of the streaming data warehouse is as follows:**\\n\\n![](/blog/bondex/streaming_warehouse.png)\\n\\nContinuing the characteristics of the Kappa architecture with a single stream processing framework, the advantage lies in the fact that the underlying Paimon technology support makes the data traceable throughout the entire chain. The data warehouse layer architecture can be reused, while also considering the processing capabilities of both offline and real-time data, reducing the waste of storage and computing resources.\\n\\n## 03 Production Practices\\n\\nThis solution adopts Flink Application on K8s clusters, with Flink CDC for real-time ingestion of relational database data from business systems. Tasks for Flink + Paimon Streaming Data Warehouse are submitted through the StreamPark task platform, with the Trino engine ultimately used to access Finereport for service provision and developer queries. Paimon's underlying storage supports the S3 protocol, and as the company's big data services rely on Alibaba Cloud, Object Storage Service (OSS) is used as the data filesystem.\\n\\n[StreamPark](https://github.com/apache/incubator-streampark) is a real-time computing platform that leverages the powerful capabilities of [Paimon](https://github.com/apache/incubator-paimon) to process real-time data streams. This platform offers the following key features:\\n\\n**Real-time Data Processing: **StreamPark supports the submission of real-time data stream tasks, capable of real-time acquisition, transformation, filtering, and analysis of data. This is extremely important for applications that require rapid response to real-time data, such as real-time monitoring, real-time recommendations, and real-time risk control.\\n\\n**Scalability: **Capable of efficiently processing large-scale real-time data with good scalability. It can operate in a distributed computing environment, automatically handling parallelization, fault recovery, and load balancing to ensure efficient and reliable data processing.\\n\\n**Flink Integration: **Built on [Apache Flink](https://github.com/apache/flink), it leverages Flink\u2019s powerful stream processing engine for high performance and robustness. Users can fully utilize the features and ecosystem of Flink, such as its extensive connectors, state management, and event-time processing.\\n\\n**Ease of Use: **Provides a straightforward graphical interface and simplified API, enabling easy construction and deployment of data processing tasks without needing to delve into underlying technical details.\\n\\nBy submitting Paimon tasks on the StreamPark platform, we can establish a full-link real-time flowing, queryable, and layered reusable Pipline.\\n\\n![](/blog/bondex/pipline.png)\\n\\nThe main components versions used are as follows:\\n\\n- flink-1.16.0-scala-2.12\\n- paimon-flink-1.16-0.4-20230424.001927-40.jar\\n- apache-streampark_2.12-2.0.0\\n- kubernetes v1.18.3\\n\\n### **Environment Setup**\\n\\nDownload flink-1.16.0-scala-2.12.tar.gz which can be obtained from the official Flink website. Download the corresponding version of the package to the StreamPark deployment server.\\n\\n```shell\\n# Unzip\\ntar zxvf flink-1.16.0-scala-2.12.tar.gz\\n\\n# Modify the flink-conf configuration file and start the cluster\\n\\njobmanager.rpc.address: localhost\\njobmanager.rpc.port: 6123\\njobmanager.bind-host: localhost\\njobmanager.memory.process.size: 4096m\\ntaskmanager.bind-host: localhost\\ntaskmanager.host: localhost\\ntaskmanager.memory.process.size: 4096m\\ntaskmanager.numberOfTaskSlots: 4\\nparallelism.default: 4\\nakka.ask.timeout: 100s\\nweb.timeout: 1000000\\n\\n#checkpoints&&savepoints\\nstate.checkpoints.dir: file:///opt/flink/checkpoints\\nstate.savepoints.dir: file:///opt/flink/savepoints\\nexecution.checkpointing.interval: 2min\\n\\n# When the job is manually canceled/paused, the Checkpoint state information of the job will be retained\\nexecution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION\\nstate.backend: rocksdb\\n\\n# Number of completed Checkpoints to retain\\nstate.checkpoints.num-retained: 2000\\nstate.backend.incremental: true\\nexecution.checkpointing.checkpoints-after-tasks-finish.enabled: true\\n\\n#OSS\\nfs.oss.endpoint: oss-cn-zhangjiakou-internal.aliyuncs.com\\nfs.oss.accessKeyId: xxxxxxxxxxxxxxxxxxxxxxx\\nfs.oss.accessKeySecret: xxxxxxxxxxxxxxxxxxxxxxx\\nfs.oss.impl: org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem\\njobmanager.execution.failover-strategy: region\\nrest.port: 8081\\nrest.address: localhost\\n```\\n\\nIt is suggested to add FLINK_HOME locally for convenient troubleshooting before deploying on k8s.\\n\\nvim /etc/profile\\n\\n```shell\\n#FLINK\\nexport FLINK_HOME=/data/src/flink-1.16.0-scala-2.12\\nexport PATH=$PATH:$FLINK_HOME/bin\\nsource /etc/profile\\n```\\n\\nIn StreamPark, add Flink conf:\\n\\n![](/blog/bondex/flink_conf.jpg)\\n\\nTo build the Flink 1.16.0 base image, pull the corresponding version of the image from Docker Hub.\\n\\n```dockerfile\\n# Pull the image\\ndocker pull flink:1.16.0-scala_2.12-java8\\n\\n# Tag the image\\ndocker tagflink:1.16.0-scala_2.12-java8  registry-vpc.cn-zhangjiakou.aliyuncs.com/xxxxx/flink:1.16.0-scala_2.12-java8\\n\\n# Push to the company repository\\ndocker pushregistry-vpc.cn-zhangjiakou.aliyuncs.com/xxxxx/flink:1.16.0-scala_2.12-java8\\n```\\n\\nCreate a Dockerfile & target directory to place the flink-oss-fs-hadoop JAR package in that directory. Download the shaded Hadoop OSS file system jar package from:\\n\\nhttps://repository.apache.org/snapshots/org/apache/paimon/paimon-oss/\\n\\n.\\n\\n\u251c\u2500\u2500 Dockerfile\\n\\n\u2514\u2500\u2500 target\\n\\n \u2514\u2500\u2500 flink-oss-fs-hadoop-1.16.0.jar\\n\\n```shell\\ntouch Dockerfile\\n\\nmkdir target\\n\\n#vim Dockerfile\\nFROM registry-vpc.cn-zhangjiakou.aliyuncs.com/xxxxx/flink:1.16.0-scala_2.12-java8\\n\\nRUN mkdir /opt/flink/plugins/oss-fs-hadoop\\n\\nCOPY target/flink-oss-fs-hadoop-1.16.0.jar  /opt/flink/plugins/oss-fs-hadoop\\n```\\n\\n### **Build base image**\\n\\n```shell\\ndocker build -t flink-table-store:v1.16.0 .\\n\\ndocker tag flink-table-store:v1.16.0 registry-vpc.cn-zhangjiakou.aliyuncs.com/xxxxx/flink-table-store:v1.16.0\\n\\ndocker push registry-vpc.cn-zhangjiakou.aliyuncs.com/xxxxx/flink-table-store:v1.16.0\\n```\\n\\nNext, prepare the Paimon jar package. You can download the corresponding version from the Apache [Repository](https://repository.apache.org/content/groups/snapshots/org/apache/paimon). It's important to note that it should be consistent with the major version of Flink.\\n\\n### **Managing Jobs with Apache StreamPark\u2122**\\n\\n**Prerequisites:**\\n\\n- Kubernetes client connection configuration\\n- Kubernetes RBAC configuration\\n- Container image repository configuration (the free version of Alibaba Cloud image is used in this case)\\n- Create a PVC resource to mount checkpoints/savepoints\\n\\n**Kubernetes Client Connection Configuration:**\\n\\nCopy the k8s master node's `~/.kube/config` configuration directly to the directory on the StreamPark server, then execute the following command on the StreamPark server to display the k8s cluster as running, which indicates successful permission and network verification.\\n\\n```shell\\nkubectl cluster-info\\n```\\n\\nKubernetes RBAC Configuration, create the streampark namespace:\\n\\n```shell\\nkubectl create ns streampark\\n```\\n\\nUse the default account to create the clusterrolebinding resource:\\n\\n```shell\\nkubectl create secret docker-registry streamparksecret \\n--docker-server=registry-vpc.cn-zhangjiakou.aliyuncs.com \\n--docker-username=xxxxxx \\n--docker-password=xxxxxx -n streamx```\\n```\\n\\n**Container Image Registry Configuration:**\\n\\nIn this case, Alibaba Cloud's Container Registry Service (ACR) is used, but you can also substitute it with a self-hosted image service such as Harbor.\\n\\nCreate a namespace named StreamPark (set the security setting to private).\\n\\n![](/blog/bondex/aliyun.png)\\n\\nConfigure the image repository in StreamPark; task build images will be pushed to the repository.\\n\\n![](/blog/bondex/dockersystem_setting.png)\\n\\nCreate a k8s secret key to pull images from ACR; streamparksecret is the name of the secret, customizable.\\n\\n```shell\\nkubectl create secret docker-registry streamparksecret \\n--docker-server=registry-vpc.cn-zhangjiakou.aliyuncs.com \\n--docker-username=xxxxxx \\n--docker-password=xxxxxx -n streamx\\n```\\n\\nCreation of PVC Resources for Checkpoints/Savepoints, Utilizing Alibaba Cloud's OSS for K8S Persistence\\n\\n**OSS CSI Plugin:**\\n\\nThe OSS CSI plugin can be used to help simplify storage management. You can use the CSI configuration to create a PV, and define PVCs and pods as usual. For the YAML file reference, visit:\\nhttps://bondextest.oss-cn-zhangjiakou.aliyuncs.com/ossyaml.zip\\n\\n**Configuration Requirements:**\\n\\n\\\\- Create a service account with the necessary RBAC permissions, please refer as below:\\n\\nhttps://github.com/kubernetes-sigs/alibaba-cloud-csi-driver/blob/master/docs/oss.md\\n\\n```shell\\nkubectl -f rbac.yaml\\n```\\n\\n\\\\- Deploy the OSS CSI Plugin:\\n\\n```shell\\nkubectl -f oss-plugin.yaml\\n```\\n\\n\\\\- Create PV for CP (Checkpoints) & SP (Savepoints):\\n\\n```shell\\nkubectl -f checkpoints_pv.yaml kubectl -f savepoints_pv.yaml\\n```\\n\\n\\\\- Create PVC for CP & SP:\\n\\n```shell\\nkubectl -f checkpoints_pvc.yaml kubectl -f savepoints_pvc.yaml\\n```\\n\\nOnce the dependent environment is configured, we can start using Paimon for layered development of the streaming data warehouse.\\n\\n### **Case Study**\\n\\nReal-time calculation of sea and air freight consignment volumes.\\n\\nJob Submission: Initialize Paimon catalog configuration.\\n\\n![](/blog/bondex/paimon_catalog_setting.png)\\n\\n```sql\\nSET 'execution.runtime-mode' = 'streaming';\\nset 'table.exec.sink.upsert-materialize' = 'none';\\nSET 'sql-client.execution.result-mode' = 'tableau';\\n-- Create and use FTS Catalog with underlying storage solution using Alibaba Cloud OSS\\nCREATE CATALOG `table_store` WITH (\\n'type' = 'paimon',\\n'warehouse' = 'oss://xxxxx/xxxxx' # customize oss storage path\\n);\\nUSE CATALOG `table_store`;\\n```\\n\\nA single job extracts table data from postgres, mysql, and sqlserver databases and writes it into Paimon.\\n\\n![](/blog/bondex/application_setting.png)\\n\\n![](/blog/bondex/pom.jpg)\\n\\n![](/blog/bondex/pod_template.png)\\n\\n**Details are as follows:**\\n\\n```\\nDevelopment Mode\uff1aFlink SQL\\n\\nExecution Mode \uff1akubernetes application\\n\\nFlink Version \uff1aflink-1.16.0-scala-2.12\\n\\nKubernetes Namespace \uff1astreamx\\n\\nKubernetes ClusterId \uff1a(Task name can be customized)\\n\\n# Base image uploaded to Alibaba Cloud Image Repository\\nFlink Base Docker Image \uff1aregistry-vpc.cn-zhangjiakou.aliyuncs.com/xxxxx/flink-table-store:v1.16.0    \\n\\nRest-Service Exposed Type\uff1aNodePort\\n\\n# Paimon base dependency package:\\n\\npaimon-flink-1.16-0.4-20230424.001927-40.jar\\n\\nflink-shaded-hadoop-2-uber-2.8.3-10.0.jar\\n\\n# FlinkCDC dependency package download address:\\n\\nhttps://github.com/ververica/flink-cdc-connectors/releases/tag/release-2.2.0\\n```\\n\\n**pod template:**\\n\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\nname: pod-template\\nspec:\\n\\ncontainers:\\n  - name: flink-main-container\\n    volumeMounts:\\n      - name: flink-checkpoints-csi-pvc\\n        mountPath: /opt/flink/checkpoints\\n      - name: flink-savepoints-csi-pvc\\n        mountPath: /opt/flink/savepoints\\n\\nvolumes:\\n  - name: flink-checkpoints-csi-pvc\\n    persistentVolumeClaim:\\n      claimName: flink-checkpoints-csi-pvc\\n  - name: flink-savepoints-csi-pvc\\n    persistentVolumeClaim:\\n      claimName: flink-savepoints-csi-pvc\\n\\nimagePullSecrets:\\n- name: streamparksecret\\n```\\n\\n**Flink sql\uff1a**\\n\\n1. Establish the relationship between the source table and the ODS table in Paimon, here it is a one-to-one mapping between the source table and the target table.\\n\\n```sql\\n-- PostgreSQL database example\\nCREATE TEMPORARY TABLE `shy_doc_hdworkdochd` (\\n`doccode` varchar(50) not null COMMENT 'Primary Key',\\n`businessmodel` varchar(450) COMMENT 'Business Model',\\n`businesstype` varchar(450)  COMMENT 'Business Type',\\n`transporttype` varchar(50) COMMENT 'Transportation Type',\\n......\\n`bookingguid` varchar(50) COMMENT 'Operation Number',\\nPRIMARY KEY (`doccode`) NOT ENFORCED\\n) WITH (\\n'connector' = 'postgres-cdc',\\n'hostname' = 'Database server IP address',\\n'port' = 'Port number',\\n'username' = 'Username',\\n'password' = 'Password',\\n'database-name' = 'Database name',\\n'schema-name' = 'dev',\\n'decoding.plugin.name' = 'wal2json',,\\n'table-name' = 'doc_hdworkdochd',\\n'debezium.slot.name' = 'hdworkdochdslotname03'\\n);\\n\\nCREATE TEMPORARY TABLE `shy_base_enterprise` (\\n`entguid` varchar(50) not null COMMENT 'Primary Key',\\n`entorgcode` varchar(450) COMMENT 'Customer Code',\\n`entnature` varchar(450)  COMMENT 'Customer Type',\\n`entfullname` varchar(50) COMMENT 'Customer Full Name',\\nPRIMARY KEY (`entguid`,`entorgcode`) NOT ENFORCED\\n) WITH (\\n'connector' = 'postgres-cdc',\\n'hostname' = 'Database server IP address',\\n'port' = 'Port number',\\n'username' = 'Username',\\n'password' = 'Password',\\n'database-name' = 'Database name',\\n'schema-name' = 'dev',\\n'decoding.plugin.name' = 'wal2json',\\n'table-name' = 'base_enterprise',\\n'debezium.snapshot.mode'='never', -- Incremental synchronization (ignore this property for full + incremental)\\n'debezium.slot.name' = 'base_enterprise_slotname03'\\n);\\n\\n-- Create the corresponding target table in the ods layer on Paimon according to the source table structure\\nCREATE TABLE IF NOT EXISTS ods.`ods_shy_jh_doc_hdworkdochd` (\\n`o_year` BIGINT NOT NULL COMMENT 'Partition Field',\\n`create_date` timestamp NOT NULL COMMENT 'Creation Time',\\nPRIMARY KEY (`o_year`, `doccode`) NOT ENFORCED\\n) PARTITIONED BY (`o_year`)\\nWITH (\\n'changelog-producer.compaction-interval' = '2m'\\n) LIKE `shy_doc_hdworkdochd` (EXCLUDING CONSTRAINTS EXCLUDING OPTIONS);\\nCREATE TABLE IF NOT EXISTS ods.`ods_shy_base_enterprise` (\\n`create_date` timestamp NOT NULL COMMENT 'Creation Time',\\nPRIMARY KEY (`entguid`,`entorgcode`) NOT ENFORCED\\n)\\nWITH (\\n'changelog-producer.compaction-interval' = '2m'\\n) LIKE `shy_base_enterprise` (EXCLUDING CONSTRAINTS EXCLUDING OPTIONS);\\n\\n-- Set the job name, execute the job task to write the source table data into the corresponding Paimon table in real time\\nSET 'pipeline.name' = 'ods_doc_hdworkdochd';\\nINSERT INTO\\nods.`ods_shy_jh_doc_hdworkdochd`\\nSELECT\\n*\\n,YEAR(`docdate`) AS `o_year`\\n,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS `create_date`\\nFROM\\n`shy_doc_hdworkdochd` where `docdate` is not null and `docdate` > '2023-01-01';\\nSET 'pipeline.name' = 'ods_shy_base_enterprise';\\nINSERT INTO\\nods.`ods_shy_base_enterprise`\\nSELECT\\n*\\n,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS `create_date`\\nFROM\\n`shy_base_enterprise` where entorgcode is not null and entorgcode <> '';\\n\\n-- MySQL database example\\nCREATE TEMPORARY TABLE `doc_order` (\\n`id` BIGINT NOT NULL COMMENT 'Primary Key',\\n`order_no` varchar(50) NOT NULL COMMENT 'Order Number',\\n`business_no` varchar(50) COMMENT 'OMS Service Number',\\n......\\n`is_deleted` int COMMENT 'Is Deleted',\\nPRIMARY KEY (`id`) NOT ENFORCED\\n) WITH (\\n'connector' = 'mysql-cdc',\\n'hostname' = 'Database server address',\\n'port' = 'Port number',\\n'username' = 'Username',\\n'password' = 'Password',\\n'database-name' = 'Database name',\\n'table-name' = 'doc_order'\\n);\\n\\n-- Create the corresponding target table in the ods layer on Paimon according to the source table structure\\nCREATE TABLE IF NOT EXISTS ods.`ods_bondexsea_doc_order` (\\n`o_year` BIGINT NOT NULL COMMENT 'Partition Field',\\n`create_date` timestamp NOT NULL COMMENT 'Creation Time',\\nPRIMARY KEY (`o_year`, `id`) NOT ENFORCED\\n) PARTITIONED BY (`o_year`)\\nWITH (\\n'changelog-producer.compaction-interval' = '2m'\\n) LIKE `doc_order` (EXCLUDING CONSTRAINTS EXCLUDING OPTIONS);\\n\\n-- Set the job name, execute the job task to write the source table data into the corresponding Paimon table in real time\\nSET 'pipeline.name' = 'ods_bondexsea_doc_order';\\nINSERT INTO\\nods.`ods_bondexsea_doc_order`\\nSELECT\\n*\\n,YEAR(`gmt_create`) AS `o_year`\\n,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS `create_date`\\nFROM `doc_order` where gmt_create > '2023-01-01';\\n\\n-- SQL Server database example\\nCREATE TEMPORARY TABLE `OrderHAWB` (\\n`HBLIndex` varchar(50) NOT NULL COMMENT 'Primary Key',\\n`CustomerNo` varchar(50) COMMENT 'Customer Number',\\n......\\n`CreateOPDate` timestamp COMMENT 'Billing Date',\\nPRIMARY KEY (`HBLIndex`) NOT ENFORCED\\n) WITH (\\n'connector' = 'sqlserver-cdc',\\n'hostname' = 'Database server address',\\n'port' = 'Port number',\\n'username' = 'Username',\\n'password' = 'Password',\\n'database-name' = 'Database name',\\n'schema-name' = 'dbo',\\n-- 'debezium.snapshot.mode' = 'initial' -- Extract both full and incremental\\n'scan.startup.mode' = 'latest-offset',-- Extract only incremental data\\n'table-name' = 'OrderHAWB'\\n);\\n\\n-- Create the corresponding target table in the ods layer on Paimon according to the source table structure\\nCREATE TABLE IF NOT EXISTS ods.`ods_airsea_airfreight_orderhawb` (\\n`o_year` BIGINT NOT NULL COMMENT 'Partition Field',\\n`create_date` timestamp NOT NULL COMMENT 'Creation Time',\\nPRIMARY KEY (`o_year`, `HBLIndex`) NOT ENFORCED\\n) PARTITIONED BY (`o_year`)\\nWITH (\\n'changelog-producer.compaction-interval' = '2m'\\n) LIKE `OrderHAWB` (EXCLUDING CONSTRAINTS EXCLUDING OPTIONS);\\n\\n-- Set the job name, execute the job task to write the source table data into the corresponding Paimon table in real time\\nSET 'pipeline.name' = 'ods_airsea_airfreight_orderhawb';\\nINSERT INTO\\nods.`ods_airsea_airfreight_orderhawb`\\nSELECT\\nRTRIM(`HBLIndex`) as `HBLIndex`\\n......\\n,`CreateOPDate`\\n,YEAR(`CreateOPDate`) AS `o_year`\\n,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS `create_date`\\nFROM `OrderHAWB` where CreateOPDate > '2023-01-01';\\n```\\n\\nThe real-time data writing effect of the business table into the Paimon ods table is as follows:\\n\\n![](/blog/bondex/ods.png)\\n\\n2. Flatten the data from the ods layer tables and write it into the dwd layer. This process is essentially about merging the related business tables from the ods layer into the dwd layer. The main focus here is on processing the value of the count_order field. Since the data in the source table may be logically or physically deleted, using the count function for statistics may lead to issues. Therefore, we use the sum aggregate to calculate the order quantity. Each reference_no corresponds to a count_order of 1. If it is logically voided, it will be processed as 0 through SQL, and Paimon will automatically handle physical deletions.\\n\\nFor the dim dimension table, we directly use the dimension table processed in Doris, as the update frequency of the dimension table is low, so there was no need for secondary development within Paimon.\\n\\n```sql\\n-- Create a wide table at the paimon-dwd layer\\nCREATE TABLE IF NOT EXISTS dwd.`dwd_business_order` (\\n`reference_no` varchar(50) NOT NULL COMMENT 'Primary key for the consignment order number',\\n`bondex_shy_flag` varchar(8) NOT NULL COMMENT 'Differentiator',\\n`is_server_item` int NOT NULL COMMENT 'Whether it has been linked to an order',\\n`order_type_name` varchar(50) NOT NULL COMMENT 'Business category',\\n`consignor_date` DATE COMMENT 'Statistic date',\\n`consignor_code` varchar(50) COMMENT 'Customer code',\\n`consignor_name` varchar(160) COMMENT 'Customer name',\\n`sales_code` varchar(32) NOT NULL COMMENT 'Sales code',\\n`sales_name` varchar(200) NOT NULL COMMENT 'Sales name',\\n`delivery_center_op_id` varchar(32) NOT NULL COMMENT 'Delivery number',\\n`delivery_center_op_name` varchar(200) NOT NULL COMMENT 'Delivery name',\\n`pol_code` varchar(100) NOT NULL COMMENT 'POL code',\\n`pot_code` varchar(100) NOT NULL COMMENT 'POT code',\\n`port_of_dest_code` varchar(100) NOT NULL COMMENT 'POD code',\\n`is_delete` int not NULL COMMENT 'Whether it is void',\\n`order_status` varchar(8) NOT NULL COMMENT 'Order status',\\n`count_order` BIGINT not NULL COMMENT 'Order count',\\n`o_year` BIGINT NOT NULL COMMENT 'Partition field',\\n`create_date` timestamp NOT NULL COMMENT 'Creation time',\\nPRIMARY KEY (`o_year`,`reference_no`,`bondex_shy_flag`) NOT ENFORCED\\n) PARTITIONED BY (`o_year`)\\nWITH (\\n-- Set 2 buckets under each partition\\n'bucket' = '2',\\n'changelog-producer' = 'full-compaction',\\n'snapshot.time-retained' = '2h',\\n'changelog-producer.compaction-interval' = '2m'\\n);\\n\\n-- Set the job name to merge the related business tables from the ods layer into the dwd layer\\nSET 'pipeline.name' = 'dwd_business_order';\\nINSERT INTO\\ndwd.`dwd_business_order`\\nSELECT\\no.doccode,\\n......,\\nYEAR (o.docdate) AS o_year\\n,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS `create_date`\\nFROM\\nods.ods_shy_jh_doc_hdworkdochd o\\nINNER JOIN ods.ods_shy_base_enterprise en ON o.businessguid = en.entguid\\nLEFT JOIN dim.dim_hhl_user_code sales ON o.salesguid = sales.USER_GUID\\nLEFT JOIN dim.dim_hhl_user_code op ON o.bookingguid = op.USER_GUID\\nUNION ALL\\nSELECT\\nbusiness_no,\\n......,\\nYEAR ( gmt_create ) AS o_year\\n,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS `create_date`\\nFROM\\nods.ods_bondexsea_doc_order\\nUNION ALL\\nSELECT\\n  HBLIndex,\\n ......,\\n  YEAR ( CreateOPDate ) AS o_year\\n,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS `create_date`\\nFROM\\n  ods.`ods_airsea_airfreight_orderhawb`\\n;\\n```\\n\\nIn the Flink UI, you can see the ods data is joined and cleansed in real-time through Paimon to the table dwd_business_order.\\n\\n![](/blog/bondex/dwd_business_order.png)\\n\\n2. The data from the dwd layer is lightly aggregated to the dwm layer, and the related data is written into the dwm.dwm_business_order_count table. The data in this table will perform a sum on the aggregated fields based on the primary key, with the sum_orderCount field being the result of the aggregation. Paimon will automatically handle the data that is physically deleted during the sum.\\n\\n```sql\\n-- Create a lightweight summary table on the dwm layer, summarizing the order volume by date, sales, operation, business category, customer, port of loading, and destination port\\nCREATE TABLE IF NOT EXISTS dwm.`dwm_business_order_count` (\\n`l_year` BIGINT NOT NULL COMMENT 'Statistic year',\\n`l_month` BIGINT NOT NULL COMMENT 'Statistic month',\\n`l_date` DATE NOT NULL  COMMENT 'Statistic date',\\n`bondex_shy_flag` varchar(8) NOT NULL COMMENT 'Identifier',\\n`order_type_name` varchar(50) NOT NULL COMMENT 'Business category',\\n`is_server_item` int NOT NULL COMMENT 'Whether an order has been associated',\\n`customer_code` varchar(50) NOT NULL COMMENT 'Customer code',\\n`sales_code` varchar(50) NOT NULL COMMENT 'Sales code',\\n`delivery_center_op_id` varchar(50) NOT NULL COMMENT 'Delivery ID',\\n`pol_code` varchar(100) NOT NULL COMMENT 'Port of loading code',\\n`pot_code` varchar(100) NOT NULL COMMENT 'Transshipment port code',\\n`port_of_dest_code` varchar(100) NOT NULL COMMENT 'Destination port code',\\n`customer_name` varchar(200) NOT NULL COMMENT 'Customer name',\\n`sales_name` varchar(200) NOT NULL COMMENT 'Sales name',\\n`delivery_center_op_name` varchar(200) NOT NULL COMMENT 'Delivery name',\\n`sum_orderCount` BIGINT NOT NULL COMMENT 'Order count',\\n`create_date` timestamp NOT NULL COMMENT 'Creation time',\\nPRIMARY KEY (`l_year`, \\n             `l_month`,\\n             `l_date`,\\n             `order_type_name`,\\n             `bondex_shy_flag`,\\n             `is_server_item`,\\n             `customer_code`,\\n             `sales_code`,\\n             `delivery_center_op_id`,\\n             `pol_code`,\\n             `pot_code`,\\n             `port_of_dest_code`) NOT ENFORCED\\n) WITH (\\n'changelog-producer' = 'full-compaction',\\n  'changelog-producer.compaction-interval' = '2m',\\n  'merge-engine' = 'aggregation', -- Use aggregation to calculate sum\\n  'fields.sum_orderCount.aggregate-function' = 'sum',\\n  'fields.create_date.ignore-retract'='true',\\n  'fields.sales_name.ignore-retract'='true',\\n  'fields.customer_name.ignore-retract'='true',\\n  'snapshot.time-retained' = '2h',\\n'fields.delivery_center_op_name.ignore-retract'='true'\\n);\\n-- Set the job name\\nSET 'pipeline.name' = 'dwm_business_order_count';\\nINSERT INTO\\ndwm.`dwm_business_order_count`\\nSELECT\\nYEAR(o.`consignor_date`) AS `l_year`\\n,MONTH(o.`consignor_date`) AS `l_month`\\n......,\\n,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS create_date\\nFROM\\ndwd.`dwd_business_order` o\\n;\\n```\\n\\nThe Flink UI effect is as follows: the data from dwd_business_orders is aggregated into dwm_business_order_count:\\n\\n![](/blog/bondex/dwm_business_order_count.png)\\n\\n4. Aggregate the data from the dwm layer to the dws layer, where the dws layer performs summaries at even finer dimensions.\\n\\n```sql\\n-- Create a table to aggregate daily order counts by operator and business type\\nCREATE TABLE IF NOT EXISTS dws.`dws_business_order_count_op` (\\n  `l_year` BIGINT NOT NULL COMMENT 'Statistic Year',\\n  `l_month` BIGINT NOT NULL COMMENT 'Statistic Month',\\n  `l_date` DATE NOT NULL  COMMENT 'Statistic Date',\\n  `order_type_name` varchar(50) NOT NULL COMMENT 'Business Category',\\n  `delivery_center_op_id` varchar(50) NOT NULL COMMENT 'Delivery ID',\\n  `delivery_center_op_name` varchar(200) NOT NULL COMMENT 'Delivery Name',\\n  `sum_orderCount` BIGINT NOT NULL COMMENT 'Order Count',\\n  `create_date` timestamp NOT NULL COMMENT 'Creation Time',\\n  PRIMARY KEY (`l_year`, `l_month`,`l_date`,`order_type_name`,`delivery_center_op_id`) NOT ENFORCED\\n) WITH (\\n  'merge-engine' = 'aggregation', -- Use aggregation to compute sum\\n  'fields.sum_orderCount.aggregate-function' = 'sum',\\n  'fields.create_date.ignore-retract'='true',\\n  'snapshot.time-retained' = '2h',\\n  'fields.delivery_center_op_name.ignore-retract'='true'\\n);\\n-- Set the job name\\nSET 'pipeline.name' = 'dws_business_order_count_op';\\nINSERT INTO\\n  dws.`dws_business_order_count_op`\\nSELECT\\n  o.`l_year`\\n  ,o.`l_month`\\n  ,o.`l_date`\\n  ,o.`order_type_name`\\n  ,o.`delivery_center_op_id`\\n  ,o.`delivery_center_op_name`\\n  ,o.`sum_orderCount`\\n  ,TO_TIMESTAMP(CONVERT_TZ(cast(CURRENT_TIMESTAMP as varchar), 'UTC', 'Asia/Shanghai')) AS create_date\\nFROM\\n  dwm.`dwm_business_order_count` o\\n;\\n```\\n\\nThe Flink UI reflects the following: Data from `dws_business_order_count_op` is written to `dws_business_order_count_op`:\\n\\n![](/blog/bondex/dws_business_order_count_op.png)\\n\\nOverall Data Flow Example\\n\\n![](/blog/bondex/all_datastream.jpg)\\n\\nSource Table:\\n\\n![](/blog/bondex/source.png)\\n\\npaimon-ods:\\n\\n![](/blog/bondex/paimon-ods.png)\\n\\npaimon-dwd:\\n\\n![](/blog/bondex/paimon-dwd.png)\\n\\npaimon-dwm:\\n\\n![](/blog/bondex/paimon-dwm.png)\\n\\npaimon-dws:\\n\\n![](/blog/bondex/paimon-dws.png)\\n\\nA special reminder: When extracting from SQL Server databases, if the source table is too large, a full extraction will lock the table. It is recommended to use incremental extraction if the business allows. For a full extraction, SQL Server can use an interim approach where the full data is imported into MySQL, then from MySQL to Paimon-ODS, and later incremental extraction is done through SQL Server.\\n\\n## 04 Troubleshooting and Analysis\\n\\n**1. Inaccurate Aggregated Data Calculation**\\n\\nSQL Server CDC collects data into the Paimon table, explanation:\\n\\n**DWD table:**\\n\\n'changelog-producer' = 'input'\\n\\n**ADS table:**\\n\\n'merge-engine' = 'aggregation', -- Using aggregation to compute sum\\n\\n'fields.sum_amount.aggregate-function' = 'sum'\\n\\nThe ADS layer's aggregated table uses agg sum, which can result in the DWD data stream not generating update_before, creating an incorrect data stream with update_after. For example, if the upstream source table updates from 10 to 30, the DWD layer's data will change to 30, and the ADS aggregation layer's data will also change to 30. But now it turns into an append data, resulting in incorrect data of 10+30=40.\\n\\nSolution:\\n\\nBy specifying 'changelog-producer' = 'full-compaction',\\nTable Store will compare the results between full compactions and produce the differences as changelog.\\nThe latency of changelog is affected by the frequency of full compactions.\\n\\nBy specifying changelog-producer.compaction-interval table property (default value 30min),\\nusers can define the maximum interval between two full compactions to ensure latency.\\nThis table property does not affect normal compactions and they may still be performed once in a while by writers to reduce reader costs.\\n\\nThis approach can solve the above mentioned issue. However, it has led to a new problem. The default changelog-producer.compaction-interval is 30 minutes, meaning that it takes 30 minutes for changes upstream to be reflected in the ads query. During production, it has been found that changing the compaction interval to 1 minute or 2 minutes can cause inaccuracies in the ADS layer aggregation data again.\\n\\n```sql\\n'changelog-producer.compaction-interval' = '2m'\\n```\\n\\nWhen writing into the Flink Table Store, it is necessary to configure table.exec.sink.upsert-materialize= none to avoid generating an upsert stream, ensuring that the Flink Table Store can retain a complete changelog for subsequent stream read operations.\\n\\n```sql\\nset 'table.exec.sink.upsert-materialize' = 'none'\\n```\\n\\n**2. The same sequence.field causes the dwd detailed wide table to not receive data updates**\\n\\n**mysql cdc collects data into the paimon table**\\n\\nExplanation:\\n\\nAfter executing an update on the MySQL source, the data modifications are successfully synchronized to the dwd_orders table.\\n\\n![](/blog/bondex/dwd_orders.png)\\n\\nHowever, upon examining the dwd_enriched_orders table data, it is found to be unsynchronized. When starting the stream mode to observe the data, it is discovered that there is no data flow.\\n\\n![](/blog/bondex/log.png)\\n\\n**Solution:**\\n\\nUpon investigation, it was discovered that the issue was caused by the configuration of the parameter 'sequence.field' = 'o_orderdate' (using o_orderdate to generate a sequence ID, and when merging records with the same primary key, the record with the larger sequence ID is chosen). Since the o_orderdate field does not change when modifying the price, the 'sequence.field' remains the same, leading to an uncertain order. Therefore, for ROW1 and ROW2, since their o_orderdate are the same, the update will randomly select between them. By removing this parameter, the system will normally generate a sequence number based on the input order, which will not affect the synchronization result.\\n\\n**3. Aggregate function 'last_non_null_value' does not support retraction**\\n\\nError:\\n\\n```\\nCaused by: java.lang.UnsupportedOperationException: Aggregate function 'last_non_null_value' does not support retraction, If you allow this function to ignore retraction messages, you can configure 'fields.${field_name}.ignore-retract'='true'.\\n```\\n\\nAn explanation can be found in the official documentation:\\n\\n```\\nOnly sum supports retraction (UPDATE_BEFORE and DELETE), other aggregate functions do not support retraction.\\n```\\n\\nThis can be understood as: except for the SUM function, other Agg functions do not support Retraction. To avoid errors when receiving DELETE and UPDATEBEFORE messages, it is necessary to configure `'fields.${field_name}.ignore-retract'='true'` for the specified field to ignore retraction and solve this error.\\n\\n```sql\\nWITH (\\n\\n'merge-engine' = 'aggregation', -- Use aggregation to compute sum\\n\\n'fields.sum_orderCount.aggregate-function' = 'sum',\\n\\n'fields.create_date.ignore-retract'='true' #field create_date\\n\\n);\\n```\\n\\n**4. Paimon Task Interruption Failure**\\n\\nTask abnormal interruption leads to pod crash, viewing loki logs shows akka.pattern.AskTimeoutException: Ask timed out on\\n\\n![](/blog/bondex/loki.png)\\n\\njava.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(JobMasterGateway.updateTaskExecutionState(TaskExecutionState))] at recipient [akka.tcp://flink@fts-business-order-count.streampark:6123/user/rpc/jobmanager_2] timed out. This is usually caused by: 1) Akka failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase akka.ask.timeout.\\\\n\\\"\\n\\nThe preliminary judgment is that the triggering of akka's timeout mechanism is likely due to the above two reasons. Therefore, adjust the cluster's akka timeout settings and carry out individual task segmentation or increase resource configuration.\\n\\n\\n\\nTo proceed, let's first see how to modify the parameters:\\n\\n\\n\\n| key              | default | describe                                                     |\\n| ---------------- | ------- | ------------------------------------------------------------ |\\n| akka.ask.timeout | 10s     | Timeout used for all futures and blocking Akka calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d). |\\n| web.timeout      | 600000  | Timeout for asynchronous operations by the web monitor in milliseconds. |\\n\\nIn `conf/flink-conf.yaml`, add the following parameters at the end:\\n\\n**akka.ask.timeout: 100s**\\n\\n**web.timeout:1000000**\\n\\nThen manually refresh the `flink-conf.yaml` in Streampark to verify if the parameters have synchronized successfully.\\n\\n**5. snapshot no such file or directory**\\n\\nIt was discovered that `cp` (checkpoint) is failing.\\n\\n[Image showing failure of cp]\\n\\nCorresponding logs at the time show Snapshot missing, with the task status shown as running, but data from the MySQL source table cannot be written into the paimon ods table.\\n\\n[Image showing status log]\\n\\nThe reason for cp failure is identified as: due to high computation and CPU intensity, threads within the TaskManager are constantly processing elements, without having time to perform the checkpoint.\\n\\nThe reason for being unable to read the Snapshot is: insufficient resources in the Flink cluster, leading to competition between Writer and Committer. During Full-Compaction, an incomplete Snapshot of an expired part was read. The official response to this issue has been fixed:\\n\\nhttps://github.com/apache/incubator-paimon/pull/1308\\n\\nAnd the solution to the checkpoint failure is to increase parallelism, by adding more deployment taskmanager slots and enhancing the jobmanager CPU resources.\\n\\n```\\n-D kubernetes.jobmanager.cpu=0.8\\n\\n-D kubernetes.jobmanager.cpu.limit-factor=1\\n\\n-D taskmanager.numberOfTaskSlots=8\\n\\n-D jobmanager.adaptive-batch-scheduler.default-source-parallelism=2\\n```\\n\\n![](/blog/bondex/dynamic_properties.jpg)\\n\\n![](/blog/bondex/flink_dashboard.png)\\n\\nIn complex real-time tasks, resources can be increased by modifying dynamic parameters.\\n\\n## 05 Future Planning\\n\\n- Our self-built data platform, Bondata, is integrating Paimon's metadata information, data metric system, lineage, and one-click pipeline features. This integration aims to form HCBondata's data assets and will serve as the foundation for a one-stop data governance initiative.\\n- Subsequently, we will integrate with Trino Catalog to access Doris, realizing a one-service solution for both offline and real-time data.\\n- We will continue to advance the pace of building an integrated streaming and batch data warehouse within the group, adopting the architecture of Doris + Paimon.\\n\\nHere, I would like to thank Teacher Zhixin and the StreamPark community for their strong support during the use of StreamPark + Paimon. The problems encountered in the learning process are promptly clarified and resolved. We will also actively participate in community exchanges and contributions in the future, enabling Paimon to provide more developers and enterprises with integrated stream and batch data lake solutions.\"},{\"id\":\"streampark-usercase-shunwang\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-shunwang\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/4-streampark-usercase-shunwang.md\",\"source\":\"@site/blog/4-streampark-usercase-shunwang.md\",\"title\":\"Apache StreamPark\u2122 in the Large-Scale Production Practice at Shunwang Technology\",\"description\":\"Preface: This article primarily discusses the challenges encountered by Shunwang Technology in using the Flink computation engine, and how StreamPark is leveraged as a real-time data platform to address these challenges, thus supporting the company's business on a large scale.\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"},{\"label\":\"FlinkSQL\",\"permalink\":\"/blog/tags/flink-sql\"}],\"readingTime\":11.575,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-shunwang\",\"title\":\"Apache StreamPark\u2122 in the Large-Scale Production Practice at Shunwang Technology\",\"tags\":[\"StreamPark\",\"Production Practice\",\"FlinkSQL\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Based on Apache Paimon + Apache StreamPark\u2122's Streaming Data Warehouse Practice by Bondex\",\"permalink\":\"/blog/streampark-usercase-bondex-with-paimon\"},\"nextItem\":{\"title\":\"Apache StreamPark\u2122's Best Practices at Dustess, Simplifying Complexity for the Ultimate Experience\",\"permalink\":\"/blog/streampark-usercase-dustess\"}},\"content\":\"![](/blog/shunwang/autor.png)\\n\\n**Preface:** This article primarily discusses the challenges encountered by Shunwang Technology in using the Flink computation engine, and how StreamPark is leveraged as a real-time data platform to address these challenges, thus supporting the company's business on a large scale.\\n\\n\\n- Introduction to the company's business\\n- Challenges encountered\\n- Why choose StreamPark\\n- Implementation in practice\\n- Benefits Brought\\n- Future planning\\n\\n\x3c!-- truncate --\x3e\\n\\n## **Introduction to the Company's Business**\\n\\nHangzhou Shunwang Technology Co., Ltd. was established in 2005. Upholding the corporate mission of connecting happiness through technology, it is one of the influential pan-entertainment technology service platforms in China. Over the years, the company has always been driven by products and technology, dedicated to creating immersive entertainment experiences across all scenes through digital platform services.\\n\\n\\n\\nSince its inception, Shunwang Technology has experienced rapid business growth, serving 80,000 offline physical stores, owning more than 50 million internet users, reaching over 140 million netizens annually, with 7 out of every 10 public internet service venues using Shunwang Technology's products.\\n\\nWith a vast user base, Shunwang Technology has been committed to providing a superior product experience and achieving digital transformation of the enterprise. Since 2015, it has vigorously developed big data capabilities, with Flink playing a crucial role in Shunwang Technology\u2019s real-time computing. At Shunwang Technology, real-time computing is roughly divided into four application scenarios:\\n\\n- Real-time update of user profiles: This includes internet cafe profiles and netizen profiles.\\n- Real-time risk control: This includes activity anti-brushing, monitoring of logins from different locations, etc.\\n- Data synchronization: This includes data synchronization from Kafka to Hive / Iceberg / ClickHouse, etc.\\n- Real-time data analysis: This includes real-time big screens for games, voice, advertising, live broadcasts, and other businesses.\\n\\nTo date, Shunwang Technology has to process TB-level data daily, with a total of more than 700 real-time tasks, of which FlinkSQL tasks account for over 95%. With the rapid development of the company's business and the increased demand for data timeliness, it is expected that the number of Flink tasks will reach 900+ by the end of this year.\\n\\n## **Challenges Encountered**\\n\\nFlink, as one of the most popular real-time computing frameworks currently, boasts powerful features such as high throughput, low latency, and stateful computations. However, in our exploration, we found that while Flink has strong computational capabilities, the community has not provided effective solutions for job development management and operational issues. We have roughly summarized some of the pain points we encountered in Flink job development management into four aspects, as follows:\\n\\n![Image](/blog/shunwang/pain.png)\\n\\nFacing a series of pain points in the management and operation of Flink jobs, we have been looking for suitable solutions to lower the barrier to entry for our developers using Flink and improve work efficiency.\\n\\nBefore we encountered StreamPark, we researched some companies' Flink management solutions and found that they all developed and managed Flink jobs through self-developed real-time job platforms. Consequently, we decided to develop our own real-time computing management platform to meet the basic needs of our developers for Flink job management and operation. Our platform is called Streaming-Launcher, with the following main functions:\\n\\n![Image](/blog/shunwang/launcher.png)\\n\\nHowever, as our developers continued to use Streaming-Launcher, they discovered quite a few deficiencies: the development cost for Flink remained too high, work efficiency was poor, and troubleshooting was difficult. We summarized the defects of Streaming-Launcher as follows:\\n\\n### **Cumbersome SQL Development Process**\\n\\nBusiness developers need multiple tools to complete the development of a single SQL job, increasing the barrier to entry for our developers.\\n\\n![cc0b1414ed43942e0ef5e9129c2bf817](/blog/shunwang/sql_develop.png)\\n\\n### **Drawbacks of SQL-Client**\\n\\nThe SQL-Client provided by Flink currently has certain drawbacks regarding the support for job execution modes.\\n\\n![Image](/blog/shunwang/sql_client.png)\\n\\n### **Lack of Unified Job Management**\\n\\nWithin Streaming-Launcher, there is no provision of a unified job management interface. Developers cannot intuitively see the job running status and can only judge the job situation through alarm information, which is very unfriendly to developers. If a large number of tasks fail at once due to Yarn cluster stability problems or network fluctuations and other uncertain factors, it is easy for developers to miss restoring a certain task while manually recovering jobs, which can lead to production accidents.\\n\\n### **Cumbersome Problem Diagnosis Process**\\n\\nTo view logs for a job, developers must go through multiple steps, which to some extent reduces their work efficiency.\\n\\n![Image](/blog/shunwang/step.png)\\n\\n## **Why Use Apache StreamPark\u2122**\\n\\nFaced with the defects of our self-developed platform Streaming-Launcher, we have been considering how to further lower the barriers to using Flink and improve work efficiency. Considering the cost of human resources and time, we decided to seek help from the open-source community and look for an appropriate open-source project to manage and maintain our Flink tasks.\\n\\n\\n\\n### 01  **Apache StreamPark\u2122: A Powerful Tool for Solving Flink Issues**\\n\\nFortunately, in early June 2022, we stumbled upon StreamPark on GitHub and embarked on a preliminary exploration full of hope. We found that StreamPark's capabilities can be broadly divided into three areas: user permission management, job operation and maintenance management, and development scaffolding.\\n\\n**User Permission Management**\\n\\nIn the StreamPark platform, to avoid the risk of users having too much authority and making unnecessary misoperations that affect job stability and the accuracy of environmental configurations, some user permission management functions are provided. This is very necessary for enterprise-level users.\\n\\n\\n\\n![Image](/blog/shunwang/permission.png)\\n\\n\\n\\n**Job Operation and Maintenance Management**\\n\\n**Our main focus during the research on StreamPark was its capability to manage the entire lifecycle of jobs:** from development and deployment to management and problem diagnosis. **Fortunately, StreamPark excels in this aspect, relieving developers from the pain points associated with Flink job management and operation.** Within StreamPark\u2019s job operation and maintenance management, there are three main modules: basic job management functions, Jar job management, and FlinkSQL job management as shown below:\\n\\n![Image](/blog/shunwang/homework_manager.png)\\n\\n**Development Scaffolding**\\n\\nFurther research revealed that StreamPark is not just a platform but also includes a development scaffold for Flink jobs. It offers a better solution for code-written Flink jobs, standardizing program configuration, providing a simplified programming model, and a suite of Connectors that lower the barrier to entry for DataStream development.\\n\\n\\n\\n![Image](/blog/shunwang/connectors.png)\\n\\n\\n\\n\\n\\n### 02  **How Apache StreamPark\u2122 Addresses Issues of the Self-Developed Platform**\\n\\nWe briefly introduced the core capabilities of StreamPark above. During the technology selection process at Shunwang Technology, we found that StreamPark not only includes the basic functions of our existing Streaming-Launcher but also offers a more complete set of solutions to address its many shortcomings. Here, we focus on the solutions provided by StreamPark for the deficiencies of our self-developed platform, Streaming-Launcher.\\n\\n\\n\\n![Image](/blog/shunwang/function.png)\\n\\n\\n\\n**One-Stop Flink Job Development Capability**\\n\\nTo lower the barriers to Flink job development and improve developers' work efficiency, **StreamPark provides features like FlinkSQL IDE, parameter management, task management, code management, one-click compilation, and one-click job deployment and undeployment**. Our research found that these integrated features of StreamPark could further enhance developers\u2019 work efficiency. To some extent, developers no longer need to concern themselves with the difficulties of Flink job management and operation and can focus on developing the business logic. These features also solve the pain point of cumbersome SQL development processes in Streaming-Launcher.\\n\\n![Image](/blog/shunwang/application.png)\\n\\n**Support for Multiple Deployment Modes**\\n\\nThe Streaming-Launcher was not flexible for developers since it only supported the Yarn Session mode. StreamPark provides a comprehensive solution for this aspect. **StreamPark fully supports all of Flink's deployment modes: Remote, Yarn Per-Job, Yarn Application, Yarn Session, K8s Session, and K8s Application,** allowing developers to freely choose the appropriate running mode for different business scenarios.\\n\\n**Unified Job Management Center**\\n\\nFor developers, job running status is one of their primary concerns. In Streaming-Launcher, due to the lack of a unified job management interface, developers had to rely on alarm information and Yarn application status to judge the state of tasks, which was very unfriendly. StreamPark addresses this issue by providing a unified job management interface, allowing for a clear view of the running status of each task.\\n\\n![Image](/blog/shunwang/management.png)\\n\\nIn the Streaming-Launcher, developers had to go through multiple steps to diagnose job issues and locate job runtime logs. StreamPark offers a one-click jump feature that allows quick access to job runtime logs.\\n\\n![Image](/blog/shunwang/logs.png)\\n\\n\\n\\n## Practical Implementation\\n\\nWhen introducing StreamPark to Shunwang Technology, due to the company's business characteristics and some customized requirements from the developers, we made some additions and optimizations to the functionalities of StreamPark. We have also summarized some problems encountered during the use and corresponding solutions.\\n\\n### 01  **Leveraging the Capabilities of Flink-SQL-Gateway**\\n\\nAt Shunwang Technology, we have developed the ODPS platform based on the Flink-SQL-Gateway to facilitate business developers in managing the metadata of Flink tables. Business developers perform DDL operations on Flink tables in ODPS, and then carry out analysis and query operations on the created Flink tables in StreamPark. Throughout the entire business development process, we have decoupled the creation and analysis of Flink tables, making the development process appear more straightforward.\\n\\nIf developers wish to query real-time data in ODPS, we need to provide a Flink SQL runtime environment. We have used StreamPark to run a Yarn Session Flink environment to support ODPS in conducting real-time queries.\\n\\n![Image](/blog/shunwang/homework.png)\\n\\nCurrently, the StreamPark community is intergrating with Flink-SQL-Gateway to further lower the barriers to developing real-time jobs.\\n\\nhttps://github.com/apache/streampark/issues/2274\\n\\n\\n\\n### 02  **Enhancing Flink Cluster Management Capabilities**\\n\\nAt Shunwang Technology, there are numerous jobs synchronizing data from Kafka to Iceberg / PG / Clickhouse / Hive. These jobs do not have high resource requirements and timeliness demands on Yarn. However, if Yarn Application and per-job modes are used for all tasks, where each task starts a JobManager, this would result in a waste of Yarn resources. For this reason, we decided to run these numerous data synchronization jobs in Yarn Session mode.\\n\\nIn practice, we found it difficult for business developers to intuitively know how many jobs are running in each Yarn Session, including the total number of jobs and the number of jobs that are running. Based on this, to facilitate developers to intuitively observe the number of jobs in a Yarn Session, we added All Jobs and Running Jobs in the Flink Cluster interface to indicate the total number of jobs and the number of running jobs in a Yarn Session.\\n\\n![Image](/blog/shunwang/cluster.png)\\n\\n\\n\\n### 03  **Enhancing Alert Capabilities**\\n\\nSince each company's SMS alert platform is implemented differently, the StreamPark community has not abstracted a unified SMS alert function. Here, we have implemented the SMS alert function through the Webhook method.\\n\\n![Image](/blog/shunwang/alarm.png)\\n\\n\\n\\n### 04  **Adding a Blocking Queue to Solve Throttling Issues**\\n\\nIn production practice, we found that when a large number of tasks fail at the same time, such as when a Yarn Session cluster goes down, platforms like Feishu/Lark and WeChat have throttling issues when multiple threads call the alert interface simultaneously. Consequently, only a portion of the alert messages will be sent by StreamPark due to the throttling issues of platforms like Feishu/Lark and WeChat, which can easily mislead developers in troubleshooting. At Shunwang Technology, we added a blocking queue and an alert thread to solve the throttling issue.\\n\\n![Image](/blog/shunwang/queue.png)\\n\\nWhen the job monitoring scheduler detects an abnormality in a job, it generates a job exception message and sends it to the blocking queue. The alert thread will continuously consume messages from the blocking queue. Upon receiving a job exception message, it will send the alert to different platforms sequentially according to the user-configured alert information. Although this method might delay the alert delivery to users, in practice, we found that even with 100+ Flink jobs failing simultaneously, the delay in receiving alerts is less than 3 seconds. Such a delay is completely acceptable to our business development colleagues. This improvement has been recorded as an ISSUE and is currently under consideration for contribution to the community.\\n\\nhttps://github.com/apache/streampark/issues/2142\\n\\n\\n\\n## Benefits Brought\\n\\nWe started exploring and using StreamX 1.2.3 (the predecessor of StreamPark) and after more than a year of running in, we found that StreamPark truly resolves many pain points in the development management and operation and maintenance of Flink jobs.\\n\\nThe greatest benefit that StreamPark has brought to Shunwang Technology is the lowered threshold for using Flink and improved development efficiency. Previously, our business development colleagues had to use multiple tools such as vscode, GitLab, and a scheduling platform in the original Streaming-Launcher to complete a FlinkSQL job development. The process was tedious, going through multiple tools from development to compilation to release. StreamPark provides one-stop service, allowing job development, compilation, and release to be completed on StreamPark, simplifying the entire development process.\\n\\n**At present, StreamPark has been massively deployed in the production environment at Shunwang Technology, with the number of FlinkSQL jobs managed by StreamPark increasing from the initial 500+ to nearly 700, while also managing more than 10 Yarn Session Clusters.**\\n\\n![Image](/blog/shunwang/achievements1.png)\\n\\n![Image](/blog/shunwang/achievements2.png)\\n\\n## Future Plans\\n\\nAs one of the early users of StreamPark, Shunwang Technology has been communicating with the community for a year and participating in the polishing of StreamPark's stability. We have submitted the Bugs encountered in production operations and new Features to the community. In the future, we hope to manage the metadata information of Flink tables on StreamPark, and implement cross-data-source query analysis functions based on the Flink engine through multiple Catalogs. Currently, StreamPark is integrating with Flink-SQL-Gateway capabilities, which will greatly help in the management of table metadata and cross-data-source query functions in the future.\\n\\nSince Shunwang Technology primarily runs jobs in Yarn Session mode, we hope that StreamPark can provide more support for Remote clusters, Yarn Session clusters, and K8s Session clusters, such as monitoring and alerts, and optimizing operational processes.\\n\\nConsidering the future, as the business develops, we may use StreamPark to manage more Flink real-time jobs, and StreamPark in single-node mode may not be safe. Therefore, we are also looking forward to the High Availability (HA) of StreamPark.\\n\\nWe will also participate in the construction of the capabilities of StreamPark is integrating with Flink-SQL-Gateway, enriching Flink Cluster functionality, and StreamPark HA.\"},{\"id\":\"streampark-usercase-dustess\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-dustess\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/5-streampark-usercase-dustess.md\",\"source\":\"@site/blog/5-streampark-usercase-dustess.md\",\"title\":\"Apache StreamPark\u2122's Best Practices at Dustess, Simplifying Complexity for the Ultimate Experience\",\"description\":\"Abstract\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"},{\"label\":\"FlinkSQL\",\"permalink\":\"/blog/tags/flink-sql\"}],\"readingTime\":14.915,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-dustess\",\"title\":\"Apache StreamPark\u2122's Best Practices at Dustess, Simplifying Complexity for the Ultimate Experience\",\"tags\":[\"StreamPark\",\"Production Practice\",\"FlinkSQL\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Apache StreamPark\u2122 in the Large-Scale Production Practice at Shunwang Technology\",\"permalink\":\"/blog/streampark-usercase-shunwang\"},\"nextItem\":{\"title\":\"Apache StreamPark\u2122's Production Practice in Joyme\",\"permalink\":\"/blog/streampark-usercase-joyme\"}},\"content\":\"**Abstract:** This article originates from the production practices of StreamPark at Dustess Information, written by the senior data development engineer, Gump. The main content includes:\\n\\n1. Technology selection\\n2. Practical implementation\\n3. Business support & capability opening\\n4. Future planning\\n5. Closing remarks\\n\\nDustess Information is a one-stop private domain operation management solution provider based on the WeChat Work ecosystem. It is committed to becoming the leading expert in private domain operation and management across all industries, helping enterprises build a new model of private domain operation management in the digital age, and promoting high-quality development for businesses.\\n\\nCurrently, Dustess has established 13 city centers nationwide, covering five major regions: North China, Central China, East China, South China, and Southwest China, providing digital marketing services to over 10,000 enterprises across more than 30 industries.\\n\\n\x3c!-- truncate --\x3e\\n\\n## **01 Technology Selection**\\n\\nDustess Information entered a rapid development phase in 2021. With the increase in service industries and corporate clients, the demand for real-time solutions became more pressing, necessitating the immediate implementation of a real-time computing platform.\\n\\nAs the company is in a phase of rapid growth, with urgent and rapidly changing needs, the team's technology selection followed these principles:\\n\\n- **Speed:** Due to urgent business needs, we required a quick implementation of the planned technology selection into production.\\n- **Stability:** On the basis of speed, the chosen technology must provide stable service for the business.\\n- **Innovation:** On the basis of the above, the selected technology should also be as modern as possible.\\n- **Comprehensiveness:** The selected technology should meet the company's rapidly developing and changing business needs, be in line with the team's long-term development goals, and support quick and efficient secondary development.\\n\\nFirstly, in terms of the computing engine: We chose Flink for the following reasons:\\n\\n- Team members have an in-depth understanding of Flink and are well-versed in its source code.\\n- Flink supports both batch and stream processing. Although the company's current batch processing architecture is based on Hive, Spark, etc., using Flink for stream computing facilitates the subsequent construction of unified batch and stream processing and lake-house architecture.\\n- The domestic ecosystem of Flink has become increasingly mature, and Flink is starting to break boundaries towards the development of stream-based data warehousing.\\n\\nAt the platform level, we comprehensively compared StreamPark, Apache Zeppelin, and flink-streaming-platform-web, also thoroughly read their source code and conducted an analysis of their advantages and disadvantages. We won\u2019t elaborate on the latter two projects in this article, but those interested can search for them on GitHub. We ultimately chose StreamPark for the following reasons:\\n\\n### **High Completion**\\n\\n#### **1. Supports Multiple Flink Versions**\\n\\n//Video link (Flink Multi-Version Support Demo)\\n\\n\\n\\nWhen creating a task, you can **freely choose the Flink version**. The Flink binary package will be automatically uploaded to HDFS (if using Yarn for submission), and only one copy of a version's binary package will be saved on HDFS. When the task is initiated, the Flink binary package in HDFS will be automatically loaded according to the context, which is very elegant. This can meet the needs for coexistence of multiple versions and for testing new versions of Flink during upgrades.\\n\\n![](/blog/dustess/flink_home.png)\\n\\n#### **2. Supports Multiple Deployment Modes**\\n\\nStreamPark supports **all the mainstream submission modes** for Flink, such as standalone, yarn-session, yarn application, yarn-perjob, kubernetes-session, kubernetes-application. Moreover, StreamPark does not simply piece together Flink run commands to submit tasks. Instead, it introduces the Flink Client source package and directly calls the Flink Client API for task submission. The advantages of this approach include modular code, readability, ease of extension, stability, and the ability to quickly adapt to upgrades of the Flink version.\\n\\n![](/blog/dustess/execution_mode.png)\\n\\nFlink SQL can greatly improve development efficiency and the popularity of Flink. StreamPark\u2019s support for **Flink SQL is very comprehensive**, with an excellent SQL editor, dependency management, multi-version task management, etc. The StreamPark official website states that it will introduce metadata management integration for Flink SQL in the future. Stay tuned.\\n\\n![](/blog/dustess/flink_sql.png)\\n\\n![](/blog/dustess/flink_sql_version.png)\\n\\n#### **4. Online Building of JAVA Tasks**\\n\\n//Video link (JAVA Task Building Demo)\\n\\n\\n\\nAlthough Flink SQL is now powerful enough, using JVM languages like Java and Scala to develop Flink tasks can be more flexible, more customizable, and better for tuning and improving resource utilization. The biggest problem with submitting tasks via Jar packages, compared to SQL, is the management of the Jar uploads. Without excellent tooling products, this can significantly reduce development efficiency and increase maintenance costs.\\n\\nBesides supporting Jar uploads, StreamPark also provides an **online update build** feature, which elegantly solves the above problems:\\n\\n1. Create Project: Fill in the GitHub/Gitlab (supports enterprise private servers) address and username/password, and StreamPark can Pull and Build the project.\\n\\n2. When creating a StreamPark Custom-Code task, refer to the Project, specify the main class, and optionally automate Pull, Build, and bind the generated Jar when starting the task, which is very elegant!\\n\\nAt the same time, the StreamPark community is also perfecting the entire task compilation and launch process. The future StreamPark will be even more refined and professional on this foundation.\\n\\n![](/blog/dustess/system_list.png)\\n\\n#### **5. Comprehensive Task Parameter Configuration**\\n\\nFor data development using Flink, the parameters submitted with Flink run are almost impossible to maintain. StreamPark has also **elegantly solved** this kind of problem, mainly because, as mentioned above, StreamPark directly calls the Flink Client API and has connected the entire process through the StreamPark product frontend.\\n\\n![](/blog/dustess/parameter_configuration.png)\\n\\nAs you can see, StreamPark's task parameter settings cover all the mainstream parameters, and every parameter has been thoughtfully provided with an introduction and an optimal recommendation based on best practices. This is also very beneficial for newcomers to Flink, helping them to avoid common pitfalls!\\n\\n#### **6. Excellent Configuration File Design**\\n\\nIn addition to the native parameters for Flink tasks, which are covered by the task parameters above, StreamPark also provides a powerful **Yaml configuration file** mode and **programming model**.\\n\\n![](/blog/dustess/extended_parameters.jpg)\\n\\n1. For Flink SQL tasks, you can configure the parameters that StreamPark has already built-in, such as **CheckPoint, retry mechanism, State Backend, table planner, mode**, etc., directly using the task's Yaml configuration file.\\n\\n2. For Jar tasks, StreamPark offers a generic programming model that encapsulates the native Flink API. Combined with the wrapper package provided by StreamPark, it can very elegantly retrieve custom parameters from the configuration file. For more details, see the documentation:\\n\\nProgramming model:\\n\\n```\\nhttps://streampark.apache.org/docs/development/dev-model\\n```\\n\\nBuilt-in Configuration File Parameters:\\n\\n```\\nhttps://streampark.apache.org/docs/development/config\\n```\\n\\nIn addition:\\n\\nStreamPark also **supports Apache Flink\xae native tasks**. The parameter configuration can be statically maintained within the Java task internal code, covering a wide range of scenarios, such as seamless migration of existing Flink tasks, etc.\\n\\n#### **7. Checkpoint Management**\\n\\nRegarding Flink's Checkpoint (Savepoint) mechanism, the greatest difficulty is maintenance. StreamPark has also elegantly solved this problem:\\n\\n- StreamPark will **automatically maintain** the task Checkpoint directory and versions in the system for easy retrieval.\\n- When users need to update and restart an application, they can choose whether to save a Savepoint.\\n- When restarting a task, it is possible to choose to recover from a specified version of Checkpoint/Savepoint.\\n\\nAs shown below, developers can very intuitively and conveniently upgrade or deal with exceptional tasks, which is very powerful.\\n\\n![](/blog/dustess/checkpoint.png)\\n\\n![](/blog/dustess/recover.jpg)\\n\\n#### **8. Comprehensive Alerting Features**\\n\\nFor streaming computations, which are 7*24H resident tasks, monitoring and alerting are very important. StreamPark also has a **comprehensive solution** for these issues:\\n\\n- It comes with an email-based alerting method, which has zero development cost and can be used once configured.\\n- Thanks to the excellent modularity of the StreamPark source code, it's possible to enhance the code at the Task Track point and introduce the company's internal SDK for telephone, group, and other alerting methods, all with a very low development cost.\\n\\n![](/blog/dustess/alarm_email.png)\\n\\n### **Excellent Source Code**\\n\\nFollowing the principle of technology selection, a new technology must be sufficiently understood in terms of underlying principles and architectural ideas before it is considered for production use. Before choosing StreamPark, its architecture and source code were subjected to in-depth study and reading. It was found that the underlying technologies used by StreamPark are very familiar to Chinese developers: MySQL, Spring Boot, Mybatis Plus, Vue, etc. The code style is unified and elegantly implemented with complete annotations. The modules are independently abstracted and reasonable, employing numerous design patterns, and the code quality is very high, making it highly suitable for troubleshooting and further development in the later stages.\\n\\n![](/blog/dustess/code_notebook.png)\\n\\nIn November 2021, StreamPark was successfully selected by Open Source China as a GVP - Gitee \\\"Most Valuable Open Source Project,\\\" which speaks volumes about its quality and potential.\\n\\n![](/blog/dustess/certificate.png)\\n\\n### **03 Active Community**\\n\\nThe community is currently very active. Since the end of November 2021, when StreamPark (based on 1.2.0-release) was implemented, StreamPark had just started to be recognized by everyone, and there were some minor bugs in the user experience (not affecting core functionality). At that time, in order to go live quickly, some features were disabled and some minor bugs were fixed. Just as we were preparing to contribute back to the community, we found that these had already been fixed, indicating that the community's iteration cycle is very fast. In the future, our company's team will also strive to stay in sync with the community, quickly implement new features, and improve data development efficiency while reducing maintenance costs.\\n\\n## **02 Implementation Practice**\\n\\nStreamPark's environment setup is very straightforward, following the official website's building tutorial you can complete the setup within a few hours. It now supports a front-end and back-end separation packaging deployment model, which can meet the needs of more companies, and there has already been a Docker Build related PR, suggesting that StreamPark's compilation and deployment will become even more convenient and quick in the future. Related documentation is as follows:\\n\\n```\\nhttps://streampark.apache.org/docs/user-guide/deployment\\n```\\n\\nFor rapid implementation and production use, we chose the reliable On Yarn resource management mode (even though StreamPark already supports K8S quite well), and there are already many companies that have deployed using StreamPark on K8S, which you can refer to:\\n\\n```\\nhttps://streampark.apache.org/blog/flink-development-framework-streampark\\n```\\n\\nIntegrating StreamPark with the Hadoop ecosystem can be said to be zero-cost (provided that Flink is integrated with the Hadoop ecosystem according to the Flink official website, and tasks can be launched via Flink scripts).\\n\\nCurrently, we are also conducting K8S testing and solution design, and will be migrating to K8S in the future.\\n\\n### **01 Implementing FlinkSQL Tasks**\\n\\nAt present, our company's tasks based on Flink SQL are mainly for simple real-time ETL and computing scenarios, with about 10 tasks, all of which have been very stable since they went live.\\n\\n![](/blog/dustess/online_flinksql.png)\\n\\nStreamPark has thoughtfully prepared a demo SQL task that can be run directly on a newly set up platform. This attention to detail demonstrates the community's commitment to user experience. Initially, our simple tasks were written and executed using Flink SQL, and StreamPark's support for Flink SQL is excellent, with a superior SQL editor and innovative POM and Jar package dependency management that can meet many SQL scenario needs.\\n\\nCurrently, we are researching and designing solutions related to metadata, permissions, UDFs, etc.\\n\\n### **02 Implementing Jar Tasks**\\n\\nSince most of the data development team members have a background in Java and Scala, we've implemented Jar-based builds for more flexible development, transparent tuning of Flink tasks, and to cover more scenarios. Our implementation was in two phases:\\n\\n**First Phase:** StreamPark provides support for native Apache Flink\xae projects. We configured our existing tasks' Git addresses in StreamPark, used Maven to package them as Jar files, and created StreamPark Apache Flink\xae tasks for seamless migration. In this process, StreamPark was merely used as a platform tool for task submission and state maintenance, without leveraging the other features mentioned above.\\n\\n**Second Phase:** After migrating tasks to StreamPark in the first phase and having them run on the platform, the tasks' configurations, such as checkpoint, fault tolerance, and adjustments to business parameters within Flink tasks, required source code modifications, pushes, and builds. This was very inefficient and opaque.\\n\\nTherefore, following StreamPark's QuickStart, we quickly integrated StreamPark's programming model, which is an encapsulation for StreamPark Flink tasks (for Apache Flink).\\n\\nExample\uff1a\\n\\n```\\nStreamingContext = ParameterTool + StreamExecutionEnvironment\\n```\\n\\n- StreamingContext is the encapsulation object for StreamPark\\n- ParameterTool is the parameter object after parsing the configuration file\\n\\n```\\n String value = ParameterTool.get(\\\"${user.custom.key}\\\")\\n```\\n\\n- StreamExecutionEnvironment is the native task context for Apache Flink\\n\\n## **03 Business Support & Capability Opening**\\n\\nCurrently, Dustess Info's real-time computing platform based on StreamPark has been online since the end of November last year and has launched 50+ Flink tasks, including 10+ Flink SQL tasks and 40+ Jar tasks. At present, it is mainly used internally by the data team, and the real-time computing platform will be opened up for use by business teams across the company shortly, which will significantly increase the number of tasks.\\n\\n![](/blog/dustess/online_jar.png)\\n\\n### **01 Real-Time Data Warehouse**\\n\\nThe real-time data warehouse mainly uses Jar tasks because the model is more generic. Using Jar tasks can generically handle a large number of data table synchronization and calculations, and even achieve configuration-based synchronization. Our real-time data warehouse mainly uses Apache Doris for storage, with Flink handling the cleaning and calculations (the goal being storage-computation separation).\\n\\nUsing StreamPark to integrate other components is also very straightforward, and we have also abstracted the configuration related to Apache Doris and Kafka into the configuration file, which greatly enhances our development efficiency and flexibility.\\n\\n### **02 Capability Opening**\\n\\nOther business teams outside the data team also have many stream processing scenarios. Hence, after secondary development of the real-time computing platform based on StreamPark, we opened up the following capabilities to all business teams in the company:\\n\\n- Business capability opening: The upstream real-time data warehouse collects all business tables through log collection and writes them into Kafka. Business teams can base their business-related development on Kafka, or they can perform OLAP analysis through the real-time data warehouse (Apache Doris).\\n- Computing capability opening: The server resources of the big data platform are made available for use by business teams.\\n- Solution opening: The mature Connectors of the Flink ecosystem and support for Exactly Once semantics can reduce the development and maintenance costs related to stream processing for business teams.\\n\\nCurrently, StreamPark does not support multi-business group functions. The multi-business group function will be abstracted and contributed to the community.\\n\\n![](/blog/dustess/manager.png)\\n\\n![](/blog/dustess/task_retrieval.png)\\n\\n## **04 Future Planning**\\n\\n### **01 Flink on K8S**\\n\\nCurrently, all our company's Flink tasks run on Yarn, which meets current needs, but Flink on Kubernetes has the following advantages:\\n\\n- **Unified Operations**. The company has unified operations with a dedicated department managing K8S.\\n- **CPU Isolation**. There is CPU isolation between K8S Pods, so real-time tasks do not affect each other, leading to more stability.\\n- **Separation of Storage and Computation**. Flink's computational resources and state storage are separated; computational resources can be mixed with other component resources, improving machine utilization.\\n- **Elastic Scaling**. It is capable of elastic scaling, better saving manpower and material costs.\\n\\nI am also currently organizing and implementing related technical architectures and solutions and have completed the technical verification of Flink on Kubernetes using StreamPark in an experimental environment. With the support of the StreamPark platform and the enthusiastic help of the community, I believe that production implementation is not far off.\\n\\n### **02 Stream-Batch Unification Construction**\\n\\nPersonally, I think the biggest difference between batch and stream processing lies in the scheduling strategy of operator tasks and the data transfer strategy between operators:\\n\\n- **Batch processing**: Upstream and downstream operator tasks have sequential scheduling (upstream tasks end and release resources), and data has a Shuffle strategy (landing on disk). The downside is lower timeliness and no intermediate state in computation, but the upside is high throughput, suitable for offline computation of super-large data volumes.\\n- **Stream processing**: Upstream and downstream operator tasks start at the same time (occupying resources simultaneously), and data is streamed between nodes through the network. The downside is insufficient throughput, but the advantage is high timeliness and intermediate state in computation, suitable for real-time and incremental computation scenarios.\\n\\nAs mentioned above, I believe that choosing **batch or stream processing** **is a tuning method for data development according to different data volumes and business scenarios**. However, currently, because the computing engine and platform distinguish offline and real-time, it causes development and maintenance fragmentation, with prohibitively high costs. To achieve stream-batch unification, the following aspects must be realized:\\n\\n- Unified storage (unification of metadata): Supports batch and stream writing/reading.\\n- Unified computing engine: Able to use a set of APIs or SQL to develop offline and real-time tasks.\\n- Unified data platform: Able to support the persistent real-time tasks, as well as offline scheduling strategies.\\n\\nRegarding the unification of stream and batch, I am also currently researching and organizing, and I welcome interested friends to discuss and study the project together.\\n\\n## **05 Closing Words**\\n\\nThat's all for the sharing of StreamPark in the production practice at Dustess Info. Thank you all for reading this far. The original intention of writing this article was to bring a bit of StreamPark's production practice experience and reference to everyone, and together with the buddies in the StreamPark community, to jointly build StreamPark. In the future, I plan to participate and contribute more. A big thank you to the developers of StreamPark for providing such an excellent product; in many details, we can feel everyone's dedication. Although the current production version used by the company (1.2.0-release) still has some room for improvement in task group search, edit return jump page, and other interactive experiences, the merits outweigh the minor issues. I believe that StreamPark will get better and better, **and I also believe that StreamPark will promote the popularity of Apache Flink**. Finally, let's end with a phrase from the Apache Flink\xae community: The future is real-time!\\n\\n\\n\\n![](/blog/dustess/author.png)\"},{\"id\":\"streampark-usercase-joyme\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-joyme\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/6-streampark-usercase-joyme.md\",\"source\":\"@site/blog/6-streampark-usercase-joyme.md\",\"title\":\"Apache StreamPark\u2122's Production Practice in Joyme\",\"description\":\"Abstract\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"},{\"label\":\"FlinkSQL\",\"permalink\":\"/blog/tags/flink-sql\"}],\"readingTime\":10.14,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-joyme\",\"title\":\"Apache StreamPark\u2122's Production Practice in Joyme\",\"tags\":[\"StreamPark\",\"Production Practice\",\"FlinkSQL\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Apache StreamPark\u2122's Best Practices at Dustess, Simplifying Complexity for the Ultimate Experience\",\"permalink\":\"/blog/streampark-usercase-dustess\"},\"nextItem\":{\"title\":\"An All-in-One Computation Tool in Haibo Tech's Production Practice and facilitation in Smart City Construction\",\"permalink\":\"/blog/streampark-usercase-haibo\"}},\"content\":\"**Abstract:** This article presents the production practices of StreamPark at Joyme, written by Qin Jiyong, a big data engineer at Joyme. The main contents include:\\n\\n- Encountering StreamPark\\n- Flink SQL job development\\n- Custom code job development\\n- Monitoring and alerting\\n- Common issues\\n- Community impressions\\n- Summary\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1 Encountering Apache StreamPark\u2122\\n\\nEncountering StreamPark was inevitable. Based on our existing real-time job development mode, we had to find an open-source platform to support our company's real-time business. Our current situation was as follows:\\n\\n- We wrote jobs and packaged them to servers, then executed the Flink run command to submit them, which was a cumbersome and inefficient process.\\n- Flink SQL was submitted through a self-developed old platform. The developers of the old platform had left, and no one maintained the subsequent code, even if someone did, they would have to face the problem of high maintenance costs.\\n- Some of the authors were SparkStreaming jobs, with two sets of streaming engines and frameworks not unified, resulting in high development costs.\\n- Real-time jobs were developed in Scala and Java, with languages and technology stacks not unified.\\n\\nFor all these reasons, we needed an open-source platform to manage our real-time jobs, and we also needed to refactor to unify the development mode and language and centralize project management.\\n\\nThe first encounter with StreamPark basically confirmed our choice. We quickly deployed and installed according to the official documentation, performed some operations after setting up, and were greeted with a user-friendly interface. StreamPark's support for multiple versions of Flink, permission management, job monitoring, and other series of functions already met our needs well. Further understanding revealed that its community is also very active. We have witnessed the process of StreamPark's feature completion since version 1.1.0. The development team is very ambitious, and we believe they will continue to improve.\\n\\n## 2 Development of Flink SQL Jobs\\n\\nThe Flink SQL development mode has brought great convenience. For some simple metric developments, it is possible to complete them with just a few SQL statements, without the need to write a single line of code. Flink SQL has facilitated the development work for many colleagues, especially since writing code can be somewhat difficult for those who work on warehouses.\\n\\nTo add a new task, you open the task addition interface of StreamPark, where the default Development Mode is Flink SQL mode. You can write the SQL logic directly in the Flink SQL section.\\n\\nFor the Flink SQL part, you can progressively write the logic SQL following the Flink official website's documentation. Generally, for our company, it consists of three parts: the Source connection, intermediate logic processing, and finally the Sink. Essentially, the Source is consuming data from Kafka, the logic layer will involve MySQL for dimension table queries, and the Sink part is mostly Elasticsearch, Redis, or MySQL.\\n\\n### **1. Writing SQL**\\n\\n```sql\\n-- Connect kafka\\nCREATE TABLE source_table (\\n `Data` ROW<uid STRING>\\n) WITH (\\n'connector.type' = 'kafka',\\n'connector.version' = 'universal',\\n'connector.topic' = '\u4e3b\u9898',\\n'connector.properties.bootstrap.servers'='broker\u5730\u5740',\\n'connector.startup-mode' = 'latest-offset',\\n'update-mode' = 'append',\\n'format.type' = 'json',\\n'connector.properties.group.id' = '\u6d88\u8d39\u7ec4id',\\n'format.derive-schema' = 'true'\\n);\\n\\n-- Landing table sink\\nCREATE TABLE sink_table (\\n`uid` STRING\\n) WITH (\\n'connector.type' = 'jdbc',\\n'connector.url' = 'jdbc:mysql://xxx/xxx?useSSL=false',\\n'connector.username' = 'username',\\n'connector.password' = 'password',\\n'connector.table' = 'tablename',\\n'connector.write.flush.max-rows' = '50',\\n'connector.write.flush.interval' = '2s',\\n'connector.write.max-retries' = '3'\\n);\\n\\n-- Code logic pass\\nINSERT INTO sink_table\\nSELECT  Data.uid  FROM source_table;\\n```\\n\\n### **2. Add Dependency**\\n\\nIn terms of dependencies, it's an unique feature to Streampark. A complete Flink SQL job is innovatively split into two components within StreamPark: the SQL and the dependencies. The SQL part is easy to understand and requires no further explanation, but the dependencies are the various Connector JARs needed within the SQL, such as Kafka and MySQL Connectors. If these are used within the SQL, then these Connector dependencies must be introduced. In StreamPark, there are two ways to add dependencies: one is based on the standard Maven pom coordinates, and the other is by uploading the required Jars from a local source. These two methods can also be mixed and used as needed; simply apply, and these dependencies will be automatically loaded when the job is submitted.\\n\\n![](/blog/joyme/add_dependency.png)\\n\\n### **3. Parameter Configuration**\\n\\nThe task addition and modification page has listed some common parameter settings, and for more extensive configuration options, a yaml configuration file is provided. Here, we've only set up checkpoint and savepoint configurations. The first is the location of the checkpoint, and the second is the frequency of checkpoint execution. We haven't changed other configurations much; users can customize these settings as per their needs.\\n\\nThe rest of the parameter settings should be configured according to the specific circumstances of the job. If the volume of data processed is large or the logic is complex, it might require more memory and higher parallelism. Sometimes, adjustments need to be made multiple times based on the job's operational performance.\\n\\n![](/blog/joyme/checkpoint_configuration.png)\\n\\n### **4. Dynamic Parameter Settings**\\n\\nSince our deployment mode is on Yarn, we configured the name of the Yarn queue in the dynamic options. Some configurations have also been set to enable incremental checkpoints and state TTL (time-to-live), all of which can be found on Flink's official website. Before, some jobs frequently encountered out-of-memory issues. After adding the incremental parameter and TTL, the job operation improved significantly. Also, in cases where the Flink SQL job involves larger states and complex logic, I personally feel that it's better to implement them through streaming code for more control.\\n\\n- -Dyarn.application.queue=Yarn queue name\\n- -Dstate.backend.incremental=true\\n- -Dtable.exec.state.ttl=expiration time\\n\\nAfter completing the configuration, submit & deploy it from the application interface.\\n\\n![](/blog/joyme/application_job.png)\\n\\n## 3 Custom Code Job Development\\n\\nFor streaming jobs, we use Flink Java for development, having refactored previous Spark Scala, Flink Scala, and Flink Java jobs and then integrated these projects together. The purpose of this integration is to facilitate maintenance. Custom code jobs require the submission of code to Git, followed by project configuration:\\n\\n![](/blog/joyme/project_configuration.png)\\n\\nOnce the configuration is completed, compile the corresponding project to finish the packaging phase. Thus, the Custom code jobs can also reference it. Compilation is required every time the code needs to go live; otherwise, only the last compiled code is available. Here's an issue: for security reasons, our company\u2019s GitLab account passwords are regularly updated. This leads to a situation where the StreamPark projects have the old passwords configured, resulting in a failure when pulling projects from Git during compilation. To address this problem, we contacted the community and learned that the capability to modify projects has been added in the subsequent version 1.2.1.\\n\\n![](/blog/joyme/flink_system.png)\\n\\nTo create a new task, select Custom code, choose the Flink version, select the project and the module Jar package, and choose the development application mode as Apache Flink\xae (standard Flink program), program main function entry class, and the task's name.\\n\\n![](/blog/joyme/add_projectconfiguration.png)\\n\\nAs well as the task\u2019s parallelism, monitoring method, etc., memory size should be configured based on the needs of the task. Program Args, the program parameters, are defined according to the program's needs. For example: If our unified startup class is StartJobApp, to start a job, it's necessary to pass the full name of the job, informing the startup class which class to find to launch the task\u2014essentially, which is a reflection mechanism. After the job configuration is complete, it is also submitted with Submit and then deployed from the application interface.\\n\\n![](/blog/joyme/application_interface.png)\\n\\n## 4 Monitoring and Alerts\\n\\nThe monitoring in StreamPark requires configuration in the setting module to set up the basic information for sending emails.\\n\\n![](/blog/joyme/system_setting.png)\\n\\nThen, within the tasks, configure the restart strategy: monitor how many times an exception occurs within a certain time, and then decide whether the strategy is to alert or restart, as well as which email address should receive the alert notifications. The version currently used by our company is 1.2.1, which supports only email sending.\\n\\n![](/blog/joyme/email_setting.png)\\n\\nWhen our jobs fail, we can receive alerts through email. These alerts are quite clear, showing which job is in what state. You can also click on the specific address below to view details.\\n\\n![](/blog/joyme/alarm_eamil.png)\\n\\nRegarding alerts, we have developed a scheduled task based on StreamPark's t_flink_app table. Why do this? Because most people might not check their emails promptly when it comes to email notifications. Therefore, we opted to monitor the status of each task and send corresponding monitoring information to our Lark (Feishu) alert group, enabling us to promptly identify and address issues with the tasks. It's a simple Python script, then configured with crontab to execute at scheduled times.\\n\\n## 5 Common Issues\\n\\nWhen it comes to the abnormal issues of jobs, we have summarized them into the following categories:\\n\\n### **1. Job Launch Failure**\\n\\nThe issue of job launch failure refers to the situation where the job does not start upon initiation. In this case, one needs to check the detailed information logs on the interface. There is an eye-shaped button in our task list; by clicking on it, one can find the submitted job log information in the start logs. If there is clear prompt information, the issue can be directly addressed. If not, one has to check the streamx.out file under the logs/ directory of the backend deployment task to find the log information regarding the launch failure.\\n\\n![](/blog/joyme/start_log.png)\\n\\n### **2. Job Running Failure**\\n\\nIf the task has started but fails during the running phase, this situation might seem similar to the first, but is actually entirely different. This means that the task has been submitted to the cluster, but the task itself has problems running. One can still apply the troubleshooting method from the first scenario to open the specific logs of the job, find the task information on yarn, and then go to yarn's logs with the yarn's tackurl recorded in the logs to look for the specific reasons. Whether it is the absence of the Sql's Connector or a null pointer in a line of code, one can find the detailed stack information. With specific information, one can find the right remedy.\\n\\n![](/blog/joyme/yarn_log.png)\\n\\n## 6 Community Impression\\n\\nOften when we discuss issues in the StreamPark user group, we get immediate responses from community members. Issues that cannot be resolved at the moment are generally fixed in the next version or the latest code branch. In the group, we also see many non-community members actively helping each other out. There are many big names from other communities as well, and many members actively join the community development work. The whole community feels very active to me!\\n\\n## 7 Conclusion\\n\\nCurrently, our company runs 60 real-time jobs online, with Flink SQL and custom code each making up about half. More real-time tasks will be put online subsequently. Many colleagues worry about the stability of StreamPark, but based on several months of production practice in our company, StreamPark is just a platform to help you develop, deploy, monitor, and manage jobs. Whether it is stable or not depends on the stability of our own Hadoop Yarn cluster (we use the onyan mode) and has little to do with StreamPark itself. It also depends on the robustness of the Flink SQL or code you write. These two aspects should be the primary concerns. Only when these two aspects are problem-free can the flexibility of StreamPark be fully utilized to improve job performance. To discuss the stability of StreamPark in isolation is somewhat extreme.\\n\\nThat is all the content shared by StreamPark at Joyme. Thank you for reading this article. We are very grateful for such an excellent product provided by StreamPark, which is a true act of benefiting others. From version 1.0 to 1.2.1, the bugs encountered are promptly fixed, and every issue is taken seriously. We are still using the on yarn deployment mode. Restarting yarn will still cause jobs to be lost, but restarting yarn is not something we do every day. The community will also look to fix this problem as soon as possible. I believe that StreamPark will get better and better, with a promising future ahead.\"},{\"id\":\"streampark-usercase-haibo\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-haibo\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/7-streampark-usercase-haibo.md\",\"source\":\"@site/blog/7-streampark-usercase-haibo.md\",\"title\":\"An All-in-One Computation Tool in Haibo Tech's Production Practice and facilitation in Smart City Construction\",\"description\":\"Summary An All-in-One Computation Tool in Haibo Tech's Production Practice and facilitation in Smart City Construction,\\\" is the Big Data Architect at Haibo Tech. The main topics covered include:\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"},{\"label\":\"FlinkSQL\",\"permalink\":\"/blog/tags/flink-sql\"}],\"readingTime\":4.435,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-haibo\",\"title\":\"An All-in-One Computation Tool in Haibo Tech's Production Practice and facilitation in Smart City Construction\",\"tags\":[\"StreamPark\",\"Production Practice\",\"FlinkSQL\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Apache StreamPark\u2122's Production Practice in Joyme\",\"permalink\":\"/blog/streampark-usercase-joyme\"},\"nextItem\":{\"title\":\"Ziroom's Real-Time Computing Platform Practice Based on Apache StreamPark\u2122\",\"permalink\":\"/blog/streampark-usercase-ziru\"}},\"content\":\"**Summary:** The author of this article, \\\"StreamPark: An All-in-One Computation Tool in Haibo Tech's Production Practice and facilitation in Smart City Construction,\\\" is the Big Data Architect at Haibo Tech. The main topics covered include:\\n\\n1. Choosing StreamPark\\n2. Getting Started Quickly\\n3. Application Scenarios\\n4. Feature Extensions\\n5. Future Expectations\\n\\nHaibo Tech is an industry-leading company offering AI IoT products and solutions. Currently, they provide full-stack solutions, including algorithms, software, and hardware products, to clients nationwide in public safety, smart cities, and smart manufacturing domains.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## **01. Choosing Apache StreamPark\u2122**\\n\\nHaibo Tech started using Flink SQL to aggregate and process various real-time IoT data since 2020. With the accelerated pace of smart city construction in various cities, the types and volume of IoT data to be aggregated are also increasing. This has resulted in an increasing number of Flink SQL tasks being maintained online, making a dedicated platform for managing numerous Flink SQL tasks an urgent need.\\n\\nAfter evaluating Apache Zeppelin and StreamPark, we chose StreamPark as our real-time computing platform. Compared to Apache Zeppelin, StreamPark may not be as well-known. However, after experiencing the initial release of StreamPark and reading its design documentation, we recognized that its all-in-one design philosophy covers the entire lifecycle of Flink task development. This means that configuration, development, deployment, and operations can all be accomplished on a single platform. Our developers, operators, and testers can collaboratively work on StreamPark. The **low-code** + **all-in-one** design principles solidified our confidence in using StreamPark.\\n\\n// Video link (streampark official video)\\n\\n\\n\\n## **02. Practical Implementation**\\n\\n### **1. Quick Start**\\n\\nUsing StreamPark to accomplish a real-time aggregation task is as simple as putting an elephant into a fridge, and it can be done in just three steps:\\n\\n- Edit SQL\\n\\n![](/blog/haibo/flink_sql.png)\\n\\n- Upload dependency packages\\n\\n![](/blog/haibo/dependency.png)\\n\\n- Deploy and run\\n\\n![](/blog/haibo/deploy.png)\\n\\nWith just the above three steps, you can complete the aggregation task from Mysql to Elasticsearch, significantly improving data access efficiency.\\n\\n### **2. Production Practice**\\n\\nStreamPark is primarily used at Haibo for running real-time Flink SQL tasks: reading data from Kafka, processing it, and outputting to Clickhouse or Elasticsearch.\\n\\nStarting from October 2021, the company gradually migrated Flink SQL tasks to the StreamPark platform for centralized management. It supports the aggregation, computation, and alerting of our real-time IoT data.\\n\\nAs of now, StreamPark has been deployed in various government and public security production environments, aggregating and processing real-time IoT data, as well as capturing data on people and vehicles. Below is a screenshot of the StreamPark platform deployed on a city's dedicated network:\\n\\n![](/blog/haibo/application.png)\\n\\n## **03. Application Scenarios**\\n\\n#### **1. Real-time IoT Sensing Data Aggregation**\\n\\nFor aggregating real-time IoT sensing data, we directly use StreamPark to develop Flink SQL tasks. For methods not provided by Flink SQL, StreamPark also supports UDF-related functionalities. Users can upload UDF packages through StreamPark, and then call the relevant UDF in SQL to achieve more complex logical operations.\\n\\nThe \\\"SQL+UDF\\\" approach meets most of our data aggregation scenarios. If business changes in the future, we only need to modify the SQL statement in StreamPark to complete business changes and deployment.\\n\\n![](/blog/haibo/data_aggregation.png)\\n\\n#### **2. Flink CDC Database Synchronization**\\n\\nTo achieve synchronization between various databases and data warehouses, we use StreamPark to develop Flink CDC SQL tasks. With the capabilities of Flink CDC, we've implemented data synchronization between Oracle and Oracle, as well as synchronization between Mysql/Postgresql and Clickhouse.\\n\\n![](/blog/haibo/flink_cdc.png)\\n\\n**3. Data Analysis Model Management**\\n\\nFor tasks that can't use Flink SQL and need Flink code development, such as real-time control models and offline data analysis models, StreamPark offers a Custom code approach, allowing users to upload executable Flink Jar packages and run them.\\n\\nCurrently, we have uploaded over 20 analysis models, such as personnel and vehicles, to StreamPark, which manages and operates them.\\n\\n![](/blog/haibo/data_aggregation.png)\\n\\n**In Summary:** Whether it's Flink SQL tasks or Custom code tasks, StreamPark provides excellent support to meet various business scenarios. However, StreamPark lacks task scheduling capabilities. If you need to schedule tasks regularly, StreamPark currently cannot meet this need. Community members are actively developing scheduling-related modules, and the soon-to-be-released version 1.2.3 will support task scheduling capabilities, so stay tuned.\\n\\n## **04. Feature Extension**\\n\\nDatahub is a metadata management platform developed by Linkedin, offering data source management, data lineage, data quality checks, and more. Haibo Tech has developed an extension based on StreamPark and Datahub, implementing table-level/field-level lineage features. With the data lineage feature, users can check the field lineage relationship of Flink SQL and save the lineage relationship to the Linkedin/Datahub metadata management platform.\\n\\n// Two video links (Data lineage feature developed based on streampark)\\n\\n\\n\\n## **05. Future Expectations**\\n\\nCurrently, the StreamPark community's Roadmap indicates that StreamPark 1.3.0 will usher in a brand new Workbench experience, a unified resource management center (unified management of JAR/UDF/Connectors), batch task scheduling, and more. These are also some of the brand-new features we are eagerly anticipating.\\n\\nThe Workbench will use a new workbench-style SQL development style. By selecting a data source, SQL can be generated automatically, further enhancing Flink task development efficiency. The unified UDF resource center will solve the current problem where each task has to upload its dependency package. The batch task scheduling feature will address StreamPark's current inability to schedule tasks.\\n\\nBelow is a prototype designed by StreamPark developers, so please stay tuned.\\n\\n![](/blog/haibo/data_source.png)\"},{\"id\":\"streampark-usercase-ziru\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-ziru\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/8-streampark-usercase-ziru.md\",\"source\":\"@site/blog/8-streampark-usercase-ziru.md\",\"title\":\"Ziroom's Real-Time Computing Platform Practice Based on Apache StreamPark\u2122\",\"description\":\"Introduction: Ziroom, an O2O internet company focusing on providing rental housing products and services, has built an online, data-driven, and intelligent platform that covers the entire chain of urban living. Real-time computing has always played an important role in Ziroom. To date, Ziroom processes TB-level data daily. This article, brought by the real-time computing team from Ziroom, introduces the in-depth practice of Ziroom's real-time computing platform based on StreamPark.\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"}],\"readingTime\":22.39,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-ziru\",\"title\":\"Ziroom's Real-Time Computing Platform Practice Based on Apache StreamPark\u2122\",\"tags\":[\"StreamPark\",\"Production Practice\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"An All-in-One Computation Tool in Haibo Tech's Production Practice and facilitation in Smart City Construction\",\"permalink\":\"/blog/streampark-usercase-haibo\"},\"nextItem\":{\"title\":\"Changan Automobile\u2019s upgrade practice from self-developed platform to Apache StreamPark\u2122\",\"permalink\":\"/blog/streampark-usercase-changan\"}},\"content\":\"![](/blog/ziru/cover.png)\\n\\n**Introduction:** Ziroom, an O2O internet company focusing on providing rental housing products and services, has built an online, data-driven, and intelligent platform that covers the entire chain of urban living. Real-time computing has always played an important role in Ziroom. To date, Ziroom processes TB-level data daily. This article, brought by the real-time computing team from Ziroom, introduces the in-depth practice of Ziroom's real-time computing platform based on StreamPark.\\n\\n- Challenges in real-time computing\\n- The journey to the solution\\n- In-depth practice based on StreamPark\\n- Summary of practical experience and examples\\n- Benefits brought by the implementation\\n- Future plans\\n\\n\x3c!-- truncate --\x3e\\n\\nAs an O2O internet brand offering rental housing products and services, Ziroom was established in October 2011. To date, Ziroom has served nearly 500,000 landlords and 5 million customers, managing over 1 million housing units. As of March 2021, Ziroom has expanded to 10 major cities including Beijing, Shanghai, Shenzhen, Hangzhou, Nanjing, Guangzhou, Chengdu, Tianjin, Wuhan, and Suzhou. Ziroom has created an online, data-driven, and intelligent platform for quality residential products for both To C and To B markets, covering the entire chain of urban living. The Ziroom app has accumulated 140 million downloads, with an average of 400 million online service calls per day, and owns tens of thousands of smart housing units. Ziroom has now established an O2O closed loop for renting, services, and community on PC, app, and WeChat platforms, eliminating all redundant steps in the traditional renting model, restructuring the residential market through the O2O model, and building China's largest O2O youth living community.\\n\\nWith a vast user base, Ziroom has been committed to providing superior product experiences and achieving digital transformation of the enterprise. Since 2021, real-time computing, particularly Flink, has played an important role in Ziroom. To date, Ziroom processes TB-level data daily, with over 500 real-time jobs supporting more than 10 million data calls per day.\\n\\n## **Challenges in Real-Time Computing**\\n\\nAt Ziroom, real-time computing is mainly divided into two application scenarios:\\n\\n- Data synchronization: Includes Kafka, MySQL, and MongoDB data synchronization to Hive / Paimon / ClickHouse, etc.\\n\\n- Real-time data warehouse: Includes real-time indicators for businesses like rentals, acquisitions, and home services.\\n\\nIn the process of implementing real-time computing, we faced several challenges, roughly as follows:\\n\\n### **01 Low Efficiency in Job Deployment**\\n\\nThe process of developing and deploying real-time jobs at Ziroom is as follows: data warehouse developers embed Flink SQL code in the program, debug locally, compile into FatJar, and then submit the job as a work order and JAR package to the operation team. The operation team member responsible for job deployment then deploys the job to the online Kubernetes session environment through the command line. This process involves many steps, each requiring manual intervention, resulting in extremely low efficiency and being prone to errors, affecting work efficiency and stability. Therefore, there is an urgent need to build an efficient and automated real-time computing platform to meet the growing demands of real-time computing.\\n\\n![](/blog/ziru/job_goes_online.png)\\n\\n### **02 Unclear Job Ownership Information**\\n\\nDue to the lack of unified management of the real-time computing platform, business code is managed by GitLab. Although this solved some problems, we found deficiencies between the repository code and the management of online Flink jobs: lack of clear ownership, lack of grouping and effective permission control, leading to chaotic job management and difficult responsibility tracing. To ensure the consistency and controllability of code and online jobs, there is an urgent need to establish a strict and clear job management system, including strict code version control, clear job ownership and responsible persons, and effective permission control.\\n\\n### **03 Difficulty in Job Maintenance**\\n\\nAt Ziroom, multiple versions of Flink jobs are running. Due to frequent API changes and lack of backward compatibility in major version upgrades of Apache Flink, the cost of upgrading project code becomes very high. Therefore, managing these different versions of jobs has become a headache.\\n\\nWithout a\\n\\nunified job platform, these jobs are submitted using scripts. Jobs vary in importance and data volume, requiring different resources and runtime parameters, necessitating corresponding modifications. Modifications can be made by editing the submission script or directly setting parameters in the code, but this makes configuration information difficult to access, especially when jobs restart or fail and FlinkUI is unavailable, turning configuration information into a black box. Therefore, there is an urgent need for a more efficient platform that supports configuration of real-time computing jobs.\\n\\n### **04 Difficulty in Job Development and Debugging**\\n\\nIn our previous development process, we typically embedded SQL code within program code in the local IDEA environment for job development and debugging, verifying the correctness of the program. However, this approach has several disadvantages:\\n\\n1. Difficulty in multi-data source debugging. Often, a requirement involves multiple different data sources. For local environment debugging, developers need to apply for white-list access to data, which is both time-consuming and cumbersome.\\n\\n2. SQL code is hard to read and modify. As SQL code is embedded in program code, it's difficult to read and inconvenient to modify. More challenging is debugging through SQL segments, as the lack of SQL version control and syntax verification makes it hard for developers to locate specific SQL lines in client logs to identify the cause of execution failure.\\n\\nTherefore, there is a need to improve the efficiency of development and debugging.\\n\\n## **The Journey to the Solution**\\n\\nIn the early stages of platform construction, we comprehensively surveyed almost all relevant projects in the industry, covering both commercial paid versions and open-source versions, starting from early 2022. After investigation and comparison, we found that these projects have their limitations to varying extents, and their usability and stability could not be effectively guaranteed.\\n\\nOverall, StreamPark performed best in our evaluation. It was the only project without major flaws and with strong extensibility: supporting both SQL and JAR jobs, with the most complete and stable deployment mode for Flink jobs. Its unique architectural design not only avoids locking in specific Flink versions but also supports convenient version switching and parallel processing, effectively solving job dependency isolation and conflict issues. The job management & operations capabilities we focused on were also very complete, including monitoring, alerts, SQL validation, SQL version comparison, CI, etc. StreamPark's support for Flink on K8s was the most comprehensive among all the open-source projects we surveyed. However, StreamPark's K8s mode submission required local image building, leading to storage resource consumption.\\n\\nIn the latest 2.2 version, the community has already restructured this part.\\n\\nAfter analyzing the pros and cons of many open-source projects, we decided to participate in projects with excellent architecture, development potential, and an actively dedicated core team. Based on this understanding, we made the following decisions:\\n\\n1. In terms of job deployment mode, we decided to adopt the On Kubernetes mode. Real-time jobs have dynamic resource consumption, creating a strong need for Kubernetes' elastic scaling, which helps us better cope with data output fluctuations and ensure job stability.\\n\\n2. In the selection of open-source components, after comprehensive comparison and evaluation of various indicators, we finally chose what was then StreamX. Subsequent close communication with the community allowed us to deeply appreciate the serious and responsible attitude of the founders and the united and friendly atmosphere of the community. We also witnessed the project's inclusion in the Apache Incubator in September 2022, making us hopeful for its future.\\n\\n3. On the basis of StreamPark, we aim to promote integration with the existing ecosystem of the company to better meet our business needs.\\n\\n## **In-depth Practice Based on Apache StreamPark\u2122**\\n\\nBased on the above decisions, we initiated the evolution of the real-time computing platform, oriented by \\\"pain point needs,\\\" and built a stable, efficient, and easy-to-maintain real-time computing platform based on StreamPark. Since the beginning of 2022, we have participated in the construction of the community while officially scheduling our internal platform construction.\\n\\nFirst, we further improved related functionalities on the basis of StreamPark:\\n\\n![](/blog/ziru/platform_construction.png)\\n\\n### **01 LDAP Login Support**\\n\\nOn the basis of StreamPark, we further improved related functionalities, including support for LDAP, so that in the future we can fully open up real-time capabilities, allowing analysts from the company's four business lines to use the platform, expected to reach about 170 people. With the increase in numbers, account management becomes increasingly important, especially in the case of personnel changes, account cancellation, and application become frequent and time-consuming operations. Therefore, integrating LDAP becomes particularly important. We communicated with the community in a timely manner and initiated a discussion, eventually contributing this Feature. Now, starting LDAP in StreamPark has become very simple, requiring just two steps:\\n\\n#### step1: Fill in the corresponding LDAP\\n\\nconfiguration:\\n\\nEdit the application.yml file, setting the LDAP basic information as follows:\\n\\n```yaml\\nldap:\\n  # Is ldap enabled?\\n  enable: false\\n  ## AD server IP, default port 389\\n  urls: ldap://99.99.99.99:389\\n  ## Login Account\\n  base-dn: dc=streampark,dc=com\\n  username: cn=Manager,dc=streampark,dc=com\\n  password: streampark\\n  user:\\n    identity-attribute: uid\\n    email-attribute: mail\\n```\\n\\n#### step2: LDAP Login\\n\\nOn the login interface, click LDAP login method, then enter the corresponding account and password, and click to log in.\\n\\n![](/blog/ziru/ldap.png)\\n\\n### **02 Automatic Ingress Generation for Job Submission**\\n\\nDue to the company's network security policy, only port 80 is opened on the Kubernetes host machines by the operation team, making it impossible to directly access the job WebUI on Kubernetes via \\\"domain + random port.\\\" To solve this problem, we needed to use Ingress to add a proxy layer to the access path, achieving the effect of access routing. In StreamPark version 2.0, we contributed the functionality related to Ingress [3]. We adopted a strategy pattern implementation, initially obtaining Kubernetes metadata information to identify its version and accordingly constructing respective objects, ensuring smooth use of the Ingress function across various Kubernetes environments.\\n\\nThe specific configuration steps are as follows:\\n\\n#### step1: Click to enter Setting-> Choose Ingress Setting, fill in the domain name\\n\\n![](/blog/ziru/ingress_setting.png)\\n\\n#### step2: Submit a job\\n\\n![](/blog/ziru/k8s_job.png)\\n\\nUpon entering the K8s management platform, you can observe that submitting a Flink job also corresponds to submitting an Ingress with the same name.\\n\\n#### step3: Click to enter Flink's WebUI\\n\\nYou will notice that the generated address consists of three parts: domain + job submission namespace + job name.\\n\\n![](/blog/ziru/flink_webui.png)\\n\\n### **03 Support for Viewing Job Deployment Logs**\\n\\nIn the process of continuous job deployment, we gradually realized that without logs, we cannot perform effective operations. Log retention, archiving, and viewing became an important part in our later problem-solving process. Therefore, in StreamPark version 2.0, we contributed the ability to archive startup logs in On Kubernetes mode and view them on the page [4]. Now, by clicking the log viewing button in the job list, it is very convenient to view the real-time logs of the job.\\n\\n![](/blog/ziru/k8s_log.png)\\n\\n### **04 Integration of Grafana Monitoring Chart Links**\\n\\nIn actual use, as the number of jobs increased, the number of users rose, and more departments were involved, we faced the problem of difficulty in self-troubleshooting. Our team's operational capabilities are actually very limited. Due to the difference in professional fields, when we tell users to view charts and logs on Grafana and ELK, users often feel at a loss and do not know how to find information related to their jobs.\\n\\nTo solve this problem, we proposed a demand in the community: we hoped that each job could directly jump to the corresponding monitoring chart and log archive page through a hyperlink, so that users could directly view the monitoring information and logs related to their jobs. This avoids tedious searches in complex system interfaces, thus improving the efficiency of troubleshooting.\\n\\nWe had a discussion in the community, and it was quickly responded to, as everyone thought it was a common need. Soon, a developer contributed a design and related PR, and the issue was quickly resolved. Now, to enable this feature in StreamPark has become very simple:\\n\\n#### step1: Create a badge label\\n\\n![](/blog/ziru/create_badge_label.png)\\n\\n#### step2: Associate the badge label with a redirect link\\n\\n![](/blog/ziru/relevancy.png)\\n\\n#### step3: Click the badge label for link redirection\\n\\n![](/blog/ziru/link_jump.png)\\n\\n### **05 Integration of Flink SQL Security for Permission Control**\\n\\nIn our system, lineage management is based on Apache Atlas, and permission management is based on the open-source project Flink-sql-security, which supports user-level data desensitization and row-level data access control, allowing specific users to only access desensitized data or authorized rows.\\n\\nThis design is to handle some complex inheritance logic. For example, when joining encrypted field age in Table A with Table B to obtain Table C, the age field in Table C should inherit the encryption logic of Table A to ensure the encryption status of data is not lost during processing. This way, we can better protect data security and ensure that data complies with security standards throughout the processing process.\\n\\nFor permission control, we developed a Flink-sql-security-streampark plugin based on Flink-s\\n\\nql-security. The basic implementation is as follows:\\n\\n1. During submission check, the system parses the submitted SQL, obtaining InputTable and OutputTable datasets.\\n\\n2. The system queries the remote permission service to obtain the user's bound RBAC (Role-Based Access Control) permissions.\\n\\n3. Based on the RBAC permissions, the system gets the encryption rules for the corresponding tables.\\n\\n4. The system rewrites the SQL, wrapping the original SQL query fields with a preset encryption algorithm, thereby reorganizing the logic.\\n\\n5. Finally, the system submits according to the reorganized logic.\\n\\nThrough this integration and plugin development, we implemented permission control for user query requests, thereby ensuring data security.\\n\\n**01 Row-level Permission Conditions**\\n\\n![](/blog/ziru/row_level_permissions.png)\\n\\n![](/blog/ziru/row_level_permissions_table.png)\\n\\nInput SQL\\n\\n```shell\\nSELECT * FROM orders;\\n```\\nUser A's actual execution SQL:\\n```shell\\nSELECT * FROM orders WHERE region = 'beijing';\\n```\\nUser B's actual execution SQL:\\n```shell\\nSELECT * FROM orders WHERE region = 'hangzhou';\\n```\\n\\n**02 Field Desensitization Conditions**\\n\\n![](/blog/ziru/field_desensitization.png)\\n\\n![](/blog/ziru/field_desensitization_table.png)\\n\\nInput SQL\\n```shell\\nSELECT name, age, price, phone FROM user;\\n```\\nExecution SQL:\\n\\nUser A's actual execution SQL:\\n```shell\\nSELECT Encryption_function(name), age, price, Sensitive_field_functions(phone) FROM user;\\n```\\nUser B's actual execution SQL:\\n```shell\\nSELECT name, Encryption_function(age), price, Sensitive_field_functions(phone) FROM user;\\n```\\n\\n### **06 Data Synchronization Platform Based on Apache StreamPark\u2122**\\n\\nWith the successful implementation of StreamPark's technical solutions in the company, we achieved deep support for Flink jobs, bringing a qualitative leap in data processing. This prompted us to completely revamp our past data synchronization logic, aiming to reduce operational costs through technical optimization and integration. Therefore, we gradually replaced historical Sqoop jobs, Canal jobs, and Hive JDBC Handler jobs with Flink CDC jobs, Flink stream, and batch jobs. In this process, we continued to optimize and strengthen StreamPark's interface capabilities, adding a status callback mechanism and achieving perfect integration with the DolphinScheduler [7] scheduling system, further enhancing our data processing capabilities.\\n\\nExternal system integration with StreamPark is simple, requiring only a few steps:\\n\\n1. First, create a token for API access:\\n\\n![](/blog/ziru/token.png)\\n\\n2. View the external call link of the Application:\\n\\n![](/blog/ziru/call_link.png)\\n\\n```shell\\ncurl -X POST '/flink/app/start' \\\\\\n-H 'Authorization: $token' \\\\\\n-H 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \\\\\\n--data-urlencode 'savePoint=' \\\\\\n--data-urlencode 'allowNonRestored=false' \\\\\\n--data-urlencode 'savePointed=false' \\\\\\n--data-urlencode 'id=100501'\\n```\\n\\n3. Configure Http scheduling in DolphinScheduler\\n\\n![](/blog/ziru/http_scheduling.png)\\n\\n## **Summary of Practical Experience**\\n\\nDuring our in-depth use of StreamPark, we summarized some common issues and explored solutions in the practice process, which we have compiled into examples for reference.\\n\\n### **01 Building Base Images**\\n\\nTo deploy a Flink job on Kubernetes using StreamPark, you first need to prepare a Base image built on Flink. Then, on the Kubernetes platform, the user-provided image is used to start the Flink job. If we continue to use the official \\\"bare image,\\\" it is far from sufficient for actual development. Business logic developed by users often involves multiple upstream and downstream data sources, requiring related data source Connectors and dependencies like Hadoop. Therefore, these dependencies need to be included in the image. Below, I will introduce the specific operation steps.\\n\\n#### step1: First, create a folder containing two folders and a Dockerfile file\\n\\n![](/blog/ziru/dockerfile.png)\\n\\nconf folder: Contains HDFS configuration files, mainly for configuring Flink's Checkpoint writing and using Hive metadata in FlinkSQL\\n\\n![](/blog/ziru/hdfs_conf.png)\\n\\nlib folder: Contains the related Jar dependency packages, as follows:\\n\\n![](/blog/ziru/lib.png)\\n\\nDockerfile file for defining image construction\\n\\n```dockerfile\\nFROM apache/flink:1.14.5-scala_2.11-java8\\nENV TIME_ZONE=Asia/Shanghai\\nCOPY ./conf /opt/hadoop/conf\\nCOPY lib $FLINK_HOME/lib/\\n```\\n\\n#### step2: Image build command using multi-architecture build mode, as follows:\\n\\n```dockerfile\\ndocker buildx build --push --platform linux/amd64 -t ${private image repository address}\\n```\\n\\n### **02 Base Image Integration with Arthas Example**\\n\\nAs more jobs are released and go live within our company, we often encounter performance degradation in long-running jobs, such as reduced Kafka consumption capacity, increased memory usage, and extended GC time. We recommend using Arthas, an open-source Java diagnostic tool by Alibaba. It allows real-time global viewing of Java application load, memory, GC, thread status, and without modifying application code, it enables viewing method call parameters, exceptions, monitoring method execution time, class loading information, etc., greatly enhancing our efficiency in troubleshooting online issues.\\n\\n![](/blog/ziru/arthas.png)\\n\\n![](/blog/ziru/advanced.png)\\n\\n![](/blog/ziru/arthas_log.png)\\n\\nTherefore, we integrated Arthas into the base image to facilitate runtime problem troubleshooting.\\n\\n```dockerfile\\nFROM apache/flink:1.14.5-scala_2.11-java8\\nENV TIME_ZONE=Asia/Shanghai\\nCOPY ./conf /opt/hadoop/conf\\nCOPY lib $FLINK_HOME/lib/\\nRUN apt-get update --fix-missing && apt-get install -y fontconfig --fix-missing && \\\\\\n    apt-get install -y openjdk-8-jdk && \\\\\\n    apt-get install -y ant && \\\\\\n    apt-get clean;\\n\\nRUN apt-get install sudo -y\\n\\n# Fix certificate issues\\nRUN apt-get update && \\\\\\n    apt-get install ca-certificates-java && \\\\\\n    apt-get clean && \\\\\\n    update-ca-certificates -f;\\n\\n# Setup JAVA_HOME -- useful for docker commandline\\nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\\nRUN export JAVA_HOME\\nRUN apt-get install -y unzip\\nRUN curl -Lo arthas-packaging-latest-bin.zip  'https://arthas.aliyun.com/download/latest_version?mirror=aliyun'\\nRUN unzip -d arthas-latest-bin arthas-packaging-latest-bin.zip\\n```\\n\\n### **03 Resolution of Dependency Conflicts in Images**\\n\\nIn the process of using StreamPark, we often encounter dependency conflict exceptions like NoClassDefFoundError, ClassNotFoundException, and NoSuchMethodError in Flink jobs running on base images. The troubleshooting approach is to find the package path of the conflicting class indicated in the error. For example, if the error class is in org.apache.orc:orc-core, go to the corresponding module directory, run `mvn dependency::tree`, search for orc-core, see who brought in the dependency, and remove it using exclusion. Below, I will introduce in detail a method of custom packaging to resolve dependency conflicts, illustrated by a dependency conflict caused by the flink-shaded-hadoop-3-uber JAR package in a base image.\\n\\n#### step1: Clone the flink-shaded project locally\ud83d\udc47\\n\\n```shell\\ngit clone https://github.com/apache/flink-shaded.git\\n```\\n\\n![](/blog/ziru/flink_shaded.png)\\n\\n#### step2: Load the project into IDEA\\n\\n![](/blog/ziru/idea.png)\\n\\n#### step3: Exclude the conflicting parts and then package them.\\n\\n### **04 Centralized Job Configuration Example**\\n\\nOne of the great conveniences of using StreamPark is centralized configuration management. You can configure all settings in the conf file in the Flink directory bound to the platform.\\n\\n```shell\\ncd /flink-1.14.5/conf\\nvim flink-conf.yaml\\n```\\n\\n![](/blog/ziru/conf.png)\\n\\nAfter completing the configuration, save it. Then go to the platform's Setting, and click on the Flink Conf icon.\\n\\n![](/blog/ziru/flink_conf.png)\\n\\nClicking Sync Conf will synchronize the global configuration file, and new jobs will be submitted with the new configuration.\\n\\n![](/blog/ziru/sync_conf.png)\\n\\n### **05 Apache StreamPark\u2122 DNS Resolution Configuration**\\n\\nA correct and reasonable DNS resolution configuration is very important when submitting FlinkSQL on the StreamPark platform. It mainly involves the following points:\\n\\n1. Flink jobs' Checkpoint writing to HDFS requires a snapshot write to an HDFS node obtained through ResourceManager. If there are expansions in the Hadoop cluster in the enterprise, and these new nodes are not covered by the DNS resolution service, this will directly lead to Checkpoint failure, affecting online stability.\\n\\n2. Flink jobs typically need to configure connection strings for different internal data sources. Configuring the database's real IP address often leads to job exits due to IP changes during database migration. Therefore, in production, connection strings are often composed of domain names and attribute parameters, with DNS services resolving them to real IP addresses for access.\\n\\nInitially, we maintained DNS configuration through Pod Template.\\n\\n```shell\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-template\\nspec:\\n  hostAliases:\\n    - ip: 10.216.xxx.79\\n      hostnames:\\n        - handoop1\\n    - host\\n\\nnames:\\n        - handoop2\\n      ip: 10.16.xx.48\\n    - hostnames:\\n        - handoop3\\n      ip: 10.16.xx.49\\n    - hostnames:\\n        - handoop4\\n      ip: 10.16.xx.50\\n   .......\\n```\\n\\nAlthough theoretically feasible, we encountered a series of problems in practice. When expanding HDFS, we found failures in Flink's Checkpoint function, and database migration also faced connection failures, causing sudden online service outages. After in-depth investigation, we found that the root cause was in DNS resolution.\\n\\nPreviously, we used hostAliases to maintain the mapping between domain names and IP addresses. However, this method was costly in practice, as every update of hostAliases required stopping all Flink jobs, undoubtedly increasing our operational costs. To seek a more flexible and reliable method to manage DNS resolution configuration and ensure the normal operation of Flink jobs, we decided to build dnsmasq for bidirectional DNS resolution.\\n\\nAfter configuring and installing dnsmasq, we first needed to override the resolv.conf configuration file in the /etc directory of the Flink image. However, since resolv.conf is a read-only file, if we want to override it, we need to use mounting. Therefore, we first configured resolv.conf as a ConfigMap for use during the override. This way, we can more flexibly and reliably manage DNS resolution configuration, ensuring stable operation of Flink jobs.\\n\\n```yaml\\napiVersion: v1\\ndata:\\n  resolv.conf: \\\"nameserver  10.216.138.226\\\" //DNS service\\nkind: ConfigMap\\nmetadata:\\n  creationTimestamp: \\\"2022-07-13T10:16:18Z\\\"\\n  managedFields:\\n  name: dns-configmap\\n  namespace: native-flink\\n```\\n\\nMounting it through Pod Template.\\n\\n![](/blog/ziru/pod_template.png)\\n\\nThis way, DNS related to the big data platform can be maintained on dnsmasq, while the host machine running Flink jobs can follow the DNS resolution process.\\n\\n1. First, check the local hosts file to see if there is a corresponding relationship, read the record for resolution, and proceed to the next step if not.\\n\\n2. The operating system checks the local DNS cache, and if not found, moves to the next step.\\n\\n3. The operating system searches the DNS server address defined in our network configuration.\\n\\nThis achieves dynamic recognition of DNS changes.\\n\\n### **06 Multi-Instance Deployment Practice**\\n\\nIn actual production environments, we often need to operate multiple clusters, including a set for testing and a set for official online use. Tasks are first verified and performance tested in the test cluster, then released to the official online cluster after ensuring accuracy.\\n\\n#### step1: Modify the port number to avoid conflicts between multiple service ports\\n\\n![](/blog/ziru/port_number.png)\\n\\n#### step2: Modify workspace\\n\\nDifferent instance services need to configure different workspaces to avoid resource interference leading to strange bugs.\\n\\n#### step3: Launch multi-instance services\\n\\nTo achieve isolation between production and testing environments, we introduced a key step at the beginning of the startup process. We input the command (for the Hadoop B cluster):\\n\\n```shell\\nexport HADOOP_CONF_DIR=/home/streamx/conf\\n```\\n\\nThis effectively cut off the default logic of Flink on K8s loading HDFS configuration. This operation ensures that A StreamPark only connects to A Hadoop environment, while B StreamPark connects to B Hadoop environment, thus achieving complete isolation between testing and production environments.\\n\\nSpecifically, after this command takes effect, we can ensure that Flink jobs submitted on port 10002 connect to the B Hadoop environment. Thus, the B Hadoop environment is isolated from the Hadoop environment used by Flink jobs submitted on port 10000 in the past, effectively preventing interference between different environments and ensuring system stability and reliability.\\n\\nThe following content is an analysis of Flink's logic for loading the Hadoop environment:\\n\\n```yaml\\n// Process of finding Hadoop configuration files\\n//1. First, check if the parameter kubernetes.hadoop.conf.config-map.name is added\\n@Override\\npublic Optional<String> get\\n\\nExistingHadoopConfigurationConfigMap() {\\n    final String existingHadoopConfigMap =\\n            flinkConfig.getString(KubernetesConfigOptions.HADOOP_CONF_CONFIG_MAP);\\n    if (StringUtils.isBlank(existingHadoopConfigMap)) {\\n        return Optional.empty();\\n    } else {\\n        return Optional.of(existingHadoopConfigMap.trim());\\n    }\\n}\\n\\n@Override\\npublic Optional<String> getLocalHadoopConfigurationDirectory() {\\n    // 2. If parameter 1 is not specified, check for the HADOOP_CONF_DIR environment variable in the local environment where the native command is submitted\\n    final String hadoopConfDirEnv = System.getenv(Constants.ENV_HADOOP_CONF_DIR);\\n    if (StringUtils.isNotBlank(hadoopConfDirEnv)) {\\n        return Optional.of(hadoopConfDirEnv);\\n    }\\n    // 3. If environment variable 2 is not present, continue to check for the HADOOP_HOME environment variable\\n    final String hadoopHomeEnv = System.getenv(Constants.ENV_HADOOP_HOME);\\n    if (StringUtils.isNotBlank(hadoopHomeEnv)) {\\n        // Hadoop 2.2+\\n        final File hadoop2ConfDir = new File(hadoopHomeEnv, \\\"/etc/hadoop\\\");\\n        if (hadoop2ConfDir.exists()) {\\n            return Optional.of(hadoop2ConfDir.getAbsolutePath());\\n        }\\n\\n        // Hadoop 1.x\\n        final File hadoop1ConfDir = a new File(hadoopHomeEnv, \\\"/conf\\\");\\n        if (hadoop1ConfDir.exists()) {\\n            return Optional.of(hadoop1ConfDir.getAbsolutePath());\\n        }\\n    }\\n\\n    return Optional.empty();\\n}\\n\\nfinal List<File> hadoopConfigurationFileItems = getHadoopConfigurationFileItems(localHadoopConfigurationDirectory.get());\\n// If 1, 2, 3 are not found, it means there is no Hadoop environment\\nif (hadoopConfigurationFileItems.isEmpty()) {\\n    LOG.warn(\\n            \\\"Found 0 files in directory {}, skip to mount the Hadoop Configuration ConfigMap.\\\",\\n            localHadoopConfigurationDirectory.get());\\n    return flinkPod;\\n}\\n// If 2 or 3 exists, it will look for core-site.xml and hdfs-site.xml files in the path\\nprivate List<File> getHadoopConfigurationFileItems(String localHadoopConfigurationDirectory) {\\n    final List<String> expectedFileNames = new ArrayList<>();\\n    expectedFileNames.add(\\\"core-site.xml\\\");\\n    expectedFileNames.add(\\\"hdfs-site.xml\\\");\\n\\n    final File directory = new File(localHadoopConfigurationDirectory);\\n    if (directory.exists() and directory.isDirectory()) {\\n        return Arrays.stream(directory.listFiles())\\n                .filter(\\n                        file ->\\n                                file.isFile()\\n                                        and expectedFileNames.stream()\\n                                                .anyMatch(name -> file.getName().equals(name)))\\n                .collect(Collectors.toList());\\n    } else {\\n        return Collections.emptyList();\\n    }\\n}\\n\\n// If a Hadoop environment is present, the above two files will be parsed as key-value pairs, and then constructed into a ConfigMap, with the name following this naming rule\\npublic static String getHadoopConfConfigMapName(String clusterId) {\\n    return Constants.HADOOP_CONF_CONFIG_MAP_PREFIX + clusterId;\\n}\\n```\\n\\nThen conduct process port occupancy queries:\\n\\n```shell\\nnetstat -tlnp | grep 10000\\nnetstat -tlnp | grep 10002\\n```\\n\\n## **Benefits Brought**\\n\\nOur team has been using StreamX (the predecessor of StreamPark) and, after more than a year of practice and refinement, StreamPark has significantly improved our challenges in developing, managing, and operating Apache Flink\xae jobs. As a one-stop service platform, StreamPark greatly simplifies the entire development process. Now, we can complete job development, compilation, and release directly on the StreamPark platform, not only lowering the management and deployment threshold of Flink but also significantly improving development efficiency.\\n\\nSince deploying StreamPark, we have been using the platform on a large scale in a production environment. From initially managing over 50 FlinkSQL jobs to nearly 500 jobs now, as shown in the diagram, StreamPark is divided into 7 teams, each with dozens of jobs. This transformation not only demonstrates StreamPark's scalability and efficiency but also fully proves its strong practical value in actual business.\\n\\n![](/blog/ziru/production_environment.png)\\n\\n## **Future Expectations**\\n\\nAs one of the early users of StreamPark, we have maintained close communication with the community, participating in the stability improvement of StreamPark. We have submitted bugs encountered in production operation and new features to the community. In the future, we hope to manage the metadata information of Apache Paimon lake tables and the capability of auxiliary jobs for\\n\\nPaimon's Actions on StreamPark. Based on the Flink engine, by interfacing with the Catalog of lake tables and Action jobs, we aim to realize the management and optimization of lake table jobs in one integrated capability. Currently, StreamPark is working on integrating the capabilities with Paimon data, which will greatly assist in real-time data lake ingestion in the future.\\n\\nWe are very grateful for the technical support that the StreamPark team has provided us all along. We wish Apache StreamPark continued success, more users, and its early graduation to become a top-level Apache project.\"},{\"id\":\"streampark-usercase-changan\",\"metadata\":{\"permalink\":\"/blog/streampark-usercase-changan\",\"editUrl\":\"https://github.com/apache/incubator-streampark-website/edit/dev/blog/9-streampark-usercase-changan.md\",\"source\":\"@site/blog/9-streampark-usercase-changan.md\",\"title\":\"Changan Automobile\u2019s upgrade practice from self-developed platform to Apache StreamPark\u2122\",\"description\":\"Introduction\uff1aChangan Automobile is one of the four major automobile cluster companies in China. With the advancement of business development and digital intelligence, the requirements for the effectiveness of data are getting higher and higher. There are more and more Flink operations. Changan Automobile originally developed a set of flow processes. The application platform is used to meet the basic needs of developers for Flink job management and operation and maintenance. However, it faces many challenges and dilemmas in actual use. In the end, Apache StreamPark was used as a one-stop real-time computing platform, which effectively solved the previous problems. Faced with many difficulties, the solution provided by StreamPark simplifies the entire development process, saves a lot of time in Flink job development and deployment, gets out of the quagmire of job operation and maintenance management, and significantly improves efficiency.\",\"date\":\"2024-10-18T07:44:07.000Z\",\"tags\":[{\"label\":\"StreamPark\",\"permalink\":\"/blog/tags/stream-park\"},{\"label\":\"Production Practice\",\"permalink\":\"/blog/tags/production-practice\"}],\"readingTime\":9.375,\"hasTruncateMarker\":true,\"authors\":[],\"frontMatter\":{\"slug\":\"streampark-usercase-changan\",\"title\":\"Changan Automobile\u2019s upgrade practice from self-developed platform to Apache StreamPark\u2122\",\"tags\":[\"StreamPark\",\"Production Practice\"]},\"unlisted\":false,\"prevItem\":{\"title\":\"Ziroom's Real-Time Computing Platform Practice Based on Apache StreamPark\u2122\",\"permalink\":\"/blog/streampark-usercase-ziru\"}},\"content\":\"![](/blog/changan/cover.png)\\n\\n**Introduction**\uff1aChangan Automobile is one of the four major automobile cluster companies in China. With the advancement of business development and digital intelligence, the requirements for the effectiveness of data are getting higher and higher. There are more and more Flink operations. Changan Automobile originally developed a set of flow processes. The application platform is used to meet the basic needs of developers for Flink job management and operation and maintenance. However, it faces many challenges and dilemmas in actual use. In the end, Apache StreamPark was used as a one-stop real-time computing platform, which effectively solved the previous problems. Faced with many difficulties, the solution provided by StreamPark simplifies the entire development process, saves a lot of time in Flink job development and deployment, gets out of the quagmire of job operation and maintenance management, and significantly improves efficiency.\\n\\nContributor | Changan Intelligent Research Institute Cloud Platform Development Institute\\n\\nEditing and proofreading\uff5cPan Yuepeng\\n\\n\x3c!-- truncate --\x3e\\n\\nChongqing Changan Automobile Co., Ltd. (referred to as \\\"Changan Automobile\\\") was established on October 31, 1996. It is an enterprise among the four major automobile groups in China and has a history of 161 years. It currently includes Changan Qiyuan, Deep Blue Automobile, Avita , Changan Gravity, Changan Kaicheng and other independent brands as well as Changan Ford, Changan Mazda and other joint venture brands.\\n\\nIn recent years, with the continuous increase in car sales and the continuous improvement of intelligence, vehicles will generate hundreds of billions of CAN data every day, and the cleaned and processed data is also at the level of 5 billion. Faced with such a huge and continuously expanding data scale, how to quickly extract and mine valuable information from massive data and provide data support for R&D, production, sales and other departments has become an urgent problem that needs to be solved.\\n\\n## **1.Challenges of real-time computing**\\n\\nThe storage and analysis processing of Changan Automobile data mainly consists of two parts: the upper cloud and the lower cloud. The upper cloud refers to the big data cluster we deployed on Tencent Cloud Server, which is built by CDH and is used to store hot data; the lower cloud is deployed on the Tencent Cloud Server. The CDP cluster in the local computer room pulls synchronized data from the cloud for storage and analysis.**Flink has always played an important role in Changan Automobile's real-time computing, and Changan Automobile has developed a streaming application platform to meet the basic needs of developers for Flink job management and O&M**\u3002However, in actual use, the streaming application platform has the following four problems:\\n\\n\u2460**Single function**: It only provides deployment functions, without user rights management, team management, role management, and project management, which cannot meet the needs of project and team management.\\n\\n\u2461**Lack of ease of use**: SQL writing is silent, log browsing is inconvenient, debugging is not supported, no error detection, no version management, no configuration center, etc.\\n\\n\u2462**Poor scalability**: Online expansion of Flink clusters, new or replacement of Flink versions, and addition of Flink Connectors and CDC are not supported.\\n\\n\u2463**Poor maintainability**: No log viewing, no version management, no configuration center, etc.\\n\\n## **2.Why use StreamPark**\\n\\nIn view of the difficulties faced by Flink operations, we decided to seek solutions from the open source field first. Therefore, we comprehensively investigated open source projects such as Apache StreamPark, Dxxky, and strxxing-xx-web,**from the core capabilities, stability, ease of use, scalability, maintainability, operation experience and other dimensions of a comprehensive evaluation and comparison, combined with our current streaming job development needs and our company's big data platform follow-up planning, we found that Apache StreamPark is the most in line with our current needs. That's why we adopted StreamPark as our streaming computing platform**\uff0cStreamPark has the following advantages\uff1a\\n\\n### **Perfect basic management capabilities**\\n\\nStreamPark solves the problems of our existing streaming application platform such as no user rights management, team management, role management, project management, team management, etc.\\n\\n\u2460 The StreamPark platform provides user rights management capabilities. These features ensure that users only have access to the data and resources they need, and limit their modification rights to the system or job. This management capability is necessary for enterprise-level users because it helps protect the organization's data, maintain system stability, and ensure the smooth running of jobs. By properly setting and managing user permissions, organizations have more control over data access and operations, reducing risk and increasing productivity.\\n\\n![](/blog/changan/capability.png)\\n\\n### **All-in-one streaming platform**\\n\\nStreamPark solves the problems of insufficient ease of use of the streaming application platform, such as no prompts for SQL writing, inconvenient log browsing, no error detection, no version management, and no configuration center.\\n\\n\u2460 StreamPark has a complete SQL validation function, implements automatic build/push images, and uses custom classloaders\uff0c**the Child-first loading mode solves the two running modes of YARN and K8s, and supports the free switching of Flink versions**.\\n\\n\u2461 StreamPark provides job monitoring and log management.**It can help users monitor the running status of jobs in real time, view job log information, and perform troubleshooting.\u3002This enables users to quickly find and resolve problems in job running**.\\n\\n### **Strong scalability**\\n\\nStreamPark solves problems such as poor scalability of the streaming application platform, inability to scale out Flink clusters online, add or replace Flink versions, and insufficient Flink connectors.\\n\\n\u2460 StreamPark is designed to be scalable and flexible\uff0c**can support large-scale Flink on Yarn job running, and can be seamlessly integrated with existing Yarn clusters**\uff0cAnd can be customized and expanded as needed. In addition, StreamPark also provides a variety of configuration options and function extension points to meet the diverse needs of users.\\n\\n\u2461 **StreamPark has a complete set of tools for Flink job development**\u3002In StreamPark, it provides a better solution for writing Flink jobs, standardizing program configurations, simplifying the programming model, and providing a series of connectors that significantly reduce the difficulty of DataStream development.\\n\\n![](/blog/changan/connectors.png)\\n\\nIn addition to the above-mentioned advantages\uff0c**StreamPark also manages the entire lifecycle of a job, including job development, deployment, operation, and problem diagnosis, so that developers can focus on the business itself without paying too much attention to the management and operation and maintenance of Flink jobs**\u3002The job development management module of StreamPark can be roughly divided into three parts: basic job management functions, JAR job management, and FlinkSQL job management.\\n\\n![](/blog/changan/job_management.png)\\n\\n## **3.StreamPark implementation practice**\\n\\nIn order to make StreamPark more consistent with our needs in daily production practice, we have made certain adaptations to it:\\n\\n### **Adaptable retrofits**\\n\\n#### **Alarm information modification**\\n\\nFor example, the alarms for StreamPark not only support email, Feishu, DingTalk, and WeCom push, but also support the classification of alarm information according to its importance, and push it to different alarm groups and relevant individuals, so that relevant O&M personnel can capture important alarm information in a timely manner and respond quickly.\\n\\n![](/blog/changan/alarm_information.png)\\n\\n#### **Nocos configuration management support**\\n\\nIn order to solve the inconvenience caused by the configuration information of Flink applications being written in the JAR package, which brings the following advantages:\\n\\n\u2460 Centralized configuration management: Through StreamPark, developers can centrally manage the configuration information of all Flink applications, realize centralized management and update of configurations, and avoid switching and duplicating operations between multiple platforms. At the same time, the security of configuration management is strengthened, and the permission control of access and modification of configuration is carried out to prevent unauthorized access and modification.\\n\\n\u2461 Dynamic configuration update: The integration of StreamPark and Nacos enables Flink applications to obtain the latest configuration information in a timely manner, realize dynamic configuration updates, avoid cumbersome processes such as recompiling, publishing, and restarting after modifying Flink configuration information, and improve the flexibility and response speed of the system. At the same time, StreamPark can use the configuration push function of Nacos to realize the automatic update of configuration information.\\n\\n![](/blog/changan/nocos.png)\\n\\n### **Problems encountered and solutions**\\n\\nOur current private cloud analysis cluster uses Cloudera CDP, and the Flink version is Cloudera Version 1.14. The overall Flink installation directory and configuration file structure are quite different from the community version. Deploying directly according to the official StreamPark documentation will make it impossible to configure Flink Home and submit subsequent overall Flink tasks to the cluster. Therefore, targeted adaptation and integration are required to provide a complete StreamPark usage experience to meet usage requirements.\\n\\n#### **1. Flink cluster cannot be accessed**\\n\\n   \u2460 Problem description: The installation path of Cloudera Flink is inconsistent with the actual submission path, and the conf directory is missing in the actual submission path.\\n\\n\u2461 Solution: The actual flink submission path is in\\n\\n```shell\\n/opt/cloudera/parcels/Flink-$ {version}/lib/flink/bin/flink\\n```\\n\\nTherefore the path\\n\\n```shell\\n/opt/cloudera/parcels/Flink-$ {version}/1ib/flink\\n```\\n\\nIt can be understood as the real Flink Home, check the content under this directory for details.\\n\\n![](/blog/changan/flink_home.png)\\n\\nIt was found that the conf directory is missing. If the directory is configured as Flink Home in StreamPark, the cluster will not be accessible. Therefore, you can soft-link the Flink configuration or edit the Flink configuration file in the cluster under this path.\\n\\n![](/blog/changan/conf.png)\\n\\nIn summary, after the pre-configuration and packaging of the code (the code may involve optimization and modification for your own use), it can be deployed.\\n\\nNote that if the version 2.0 is packaged, you can directly execute the build.sh in the source code, select hybrid deployment, and the generated package is in the dist directory.\\n\\n#### **2. Hadoop environment check failed**\\n\\n![](/blog/changan/hadoop.png)\\n\\n```shell\\nvi /etc/profile\\n```\\n\\nSolution: Add the hadoop environment to the node where StreamPark is deployed.\\n\\n```shell\\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\\n```\\n\\nsource to make it work, just restart StreamPark.\\n\\n#### **3. The lib directory is missing from the Flink path of HDFS**\\n\\n   Solution: The lib directory in the working directory of StreamPark on HDFS after deployment is not uploaded normally. Find the initialized StremPark work path on HDFS and observe whether the lib directory under hdfs:///streampark/flink/.../ Complete, if not complete, manually put the lib in the local Flink Home directory.\\n\\n![](/blog/changan/lib.png)\\n\\n#### **4. The job is automatically pulled up after it fails, causing the job to be repeated.**\\n\\n   StreamPark itself has the ability to restart the pull after the job is used. When a job failure is detected, it will be restarted. However, in practice, it is found that the same failed job will be started multiple times on YARN. After communicating with the community, it has been It was determined to be a bug, and the bug was finally resolved in 2.1.3.\\n\\n## **4.Benefits**\\n\\nCurrently, Changan Automobile has deployed StreamPark in cloud production/pre-production environment and local production/pre-production environment. As of press time, StreamPark manages a total of 150+ Flink jobs, including JAR and SQL jobs. It involves using technologies such as Flink and FlinkCDC to synchronize data from MySQL or Kafka to databases such as MySQL, Doris, HBase, and Hive. In the future, all Flink jobs in a total of 3,000+ environments will also be migrated to StreamPark in batches for centralized management.\\n\\n**The most obvious benefit of Apache StreamPark is that it provides a one-stop service, where business development students can complete job development, compilation and release on StreamPark, without using multiple tools to complete a FlinkSQL job development, simplifying the entire development process**\uff0cStreamPark saves us a lot of time in Flink job development and deployment, and significantly improves the efficiency of Flink application development.\\n\\n![](/blog/changan/job.png)\"}]}}")}}]);