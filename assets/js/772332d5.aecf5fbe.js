"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[4426],{79794:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>l});var s=r(86070),o=r(76113);const t={id:"docker-deployment",title:"Platform Deployment using Docker",sidebar_position:3},a=void 0,i={id:"get-started/docker-deployment",title:"Platform Deployment using Docker",description:"This tutorial for docker deploy StreamPark",source:"@site/docs/get-started/3.dockerDeployment.md",sourceDirName:"get-started",slug:"/get-started/docker-deployment",permalink:"/docs/get-started/docker-deployment",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/docs/get-started/3.dockerDeployment.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{id:"docker-deployment",title:"Platform Deployment using Docker",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Platform Deployment",permalink:"/docs/get-started/deployment"},next:{title:"How to Use Platform",permalink:"/docs/get-started/how-to-use"}},c={},l=[{value:"Prepare",id:"prepare",level:2},{value:"1. Install docker",id:"1-install-docker",level:3},{value:"2. Install docker-compose",id:"2-install-docker-compose",level:3},{value:"Apache StreamPark\u2122 Deployment",id:"apache-streampark-deployment",level:2},{value:"1. Apache StreamPark\u2122 deployment based on h2 and docker-compose",id:"1-apache-streampark-deployment-based-on-h2-and-docker-compose",level:3},{value:"2. Deployment",id:"2-deployment",level:3},{value:"3. Configure flink home",id:"3-configure-flink-home",level:3},{value:"4. Configure flink-session cluster",id:"4-configure-flink-session-cluster",level:3},{value:"5. Submit flink job",id:"5-submit-flink-job",level:3},{value:"Use existing Mysql services",id:"use-existing-mysql-services",level:4},{value:"Use existing Pgsql services",id:"use-existing-pgsql-services",level:4},{value:"Build images based on source code for Apache StreamPark\u2122 deployment",id:"build-images-based-on-source-code-for-apache-streampark-deployment",level:2},{value:"Docker-Compse Configuration",id:"docker-compse-configuration",level:2},{value:"Uploading Configuration to the Container",id:"uploading-configuration-to-the-container",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"This tutorial for docker deploy StreamPark"}),"\n",(0,s.jsx)(n.h2,{id:"prepare",children:"Prepare"}),"\n",(0,s.jsx)(n.p,{children:"Docker 1.13.1+\nDocker Compose 1.28.0+"}),"\n",(0,s.jsx)(n.h3,{id:"1-install-docker",children:"1. Install docker"}),"\n",(0,s.jsxs)(n.p,{children:["To start the service with docker, you need to install ",(0,s.jsx)(n.a,{href:"https://www.docker.com/",children:"docker"})," first"]}),"\n",(0,s.jsx)(n.h3,{id:"2-install-docker-compose",children:"2. Install docker-compose"}),"\n",(0,s.jsxs)(n.p,{children:["To start the service with docker-compose, you need to install ",(0,s.jsx)(n.a,{href:"https://docs.docker.com/compose/install/",children:"docker-compose"})," first"]}),"\n",(0,s.jsx)(n.h2,{id:"apache-streampark-deployment",children:"Apache StreamPark\u2122 Deployment"}),"\n",(0,s.jsx)(n.h3,{id:"1-apache-streampark-deployment-based-on-h2-and-docker-compose",children:"1. Apache StreamPark\u2122 deployment based on h2 and docker-compose"}),"\n",(0,s.jsx)(n.p,{children:"This method is suitable for beginners to learn and become familiar with the features. The configuration will reset after the container is restarted. Below, you can configure Mysql or Pgsql for persistence."}),"\n",(0,s.jsx)(n.h3,{id:"2-deployment",children:"2. Deployment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"wget https://raw.githubusercontent.com/apache/incubator-streampark/dev/docker/docker-compose.yaml\nwget https://raw.githubusercontent.com/apache/incubator-streampark/dev/docker/.env\ndocker-compose up -d\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Once the service is started, StreamPark can be accessed through ",(0,s.jsx)(n.a,{href:"http://localhost:10000",children:"http://localhost:10000"})," and also through ",(0,s.jsx)(n.a,{href:"http://localhost:8081",children:"http://localhost:8081"})," to access Flink. Accessing the StreamPark link will redirect you to the login page, where the default user and password for StreamPark are admin and streampark respectively. To learn more about the operation, please refer to the user manual for a quick start."]}),"\n",(0,s.jsx)(n.h3,{id:"3-configure-flink-home",children:"3. Configure flink home"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:r(21554).A+"",width:"1310",height:"498"})}),"\n",(0,s.jsx)(n.h3,{id:"4-configure-flink-session-cluster",children:"4. Configure flink-session cluster"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:r(34562).A+"",width:"1302",height:"550"})}),"\n",(0,s.jsxs)(n.p,{children:["Note",":When"," configuring the flink-sessin cluster address, the ip address is not localhost, but the host network ip, which can be obtained through ifconfig"]}),"\n",(0,s.jsx)(n.h3,{id:"5-submit-flink-job",children:"5. Submit flink job"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:r(55670).A+"",width:"1207",height:"627"})}),"\n",(0,s.jsx)(n.h4,{id:"use-existing-mysql-services",children:"Use existing Mysql services"}),"\n",(0,s.jsx)(n.p,{children:"This approach is suitable for enterprise production, where you can quickly deploy StreamPark based on docker and associate it with an online database\nNote: The diversity of deployment support is maintained through the .env configuration file, make sure there is one and only one .env file in the directory"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"wget https://raw.githubusercontent.com/apache/incubator-streampark/dev/deploy/docker/docker-compose.yaml\nwget https://raw.githubusercontent.com/apache/incubator-streampark/dev/deploy/docker/mysql/.env\nvim .env\n"})}),"\n",(0,s.jsx)(n.p,{children:'First, you need to create the "streampark" database in MySQL, and then manually execute the corresponding SQL found in the schema and data for the relevant data source.'}),"\n",(0,s.jsx)(n.p,{children:"After that, modify the corresponding connection information."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"SPRING_PROFILES_ACTIVE=mysql\nSPRING_DATASOURCE_URL=jdbc:mysql://localhost:3306/streampark?useSSL=false&useUnicode=true&characterEncoding=UTF-8&allowPublicKeyRetrieval=false&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=GMT%2B8\nSPRING_DATASOURCE_USERNAME=root\nSPRING_DATASOURCE_PASSWORD=streampark\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"docker-compose up -d\n"})}),"\n",(0,s.jsx)(n.h4,{id:"use-existing-pgsql-services",children:"Use existing Pgsql services"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"wget https://raw.githubusercontent.com/apache/incubator-streampark/dev/deploy/docker/docker-compose.yaml\nwget https://raw.githubusercontent.com/apache/incubator-streampark/dev/deploy/docker/pgsql/.env\nvim .env\n"})}),"\n",(0,s.jsx)(n.p,{children:"Modify the corresponding connection information"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"SPRING_PROFILES_ACTIVE=pgsql\nSPRING_DATASOURCE_URL=jdbc:postgresql://localhost:5432/streampark?stringtype=unspecified\nSPRING_DATASOURCE_USERNAME=postgres\nSPRING_DATASOURCE_PASSWORD=streampark\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"docker-compose up -d\n"})}),"\n",(0,s.jsx)(n.h2,{id:"build-images-based-on-source-code-for-apache-streampark-deployment",children:"Build images based on source code for Apache StreamPark\u2122 deployment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"git clone https://github.com/apache/incubator-streampark.git\ncd incubator-streampark/deploy/docker\nvim docker-compose\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"    build:\n      context: ../..\n      dockerfile: deploy/docker/console/Dockerfile\n#    image: ${HUB}:${TAG}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:r(97611).A+"",width:"731",height:"380"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"docker-compose up -d\n"})}),"\n",(0,s.jsx)(n.h2,{id:"docker-compse-configuration",children:"Docker-Compse Configuration"}),"\n",(0,s.jsx)(n.p,{children:"The docker-compose.yaml file will reference the configuration from the env file, and the modified configuration is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"version: '3.8'\nservices:\n  ## streampark-console container\n  streampark-console:\n    ## streampark image\n    image: apache/streampark:latest\n    ## streampark image startup command\n    command: ${\n   RUN_COMMAND}\n    ports:\n      - 10000:10000\n    ## Environment configuration file\n    env_file: .env\n    environment:\n      ## Declare environment variable\n      HADOOP_HOME: ${\n   HADOOP_HOME}\n    volumes:\n      - flink:/streampark/flink/${\n   FLINK}\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /etc/hosts:/etc/hosts:ro\n      - ~/.kube:/root/.kube:ro\n    privileged: true\n    restart: unless-stopped\n    networks:\n      - streampark\n\n  ## flink-jobmanager container\n  flink-jobmanager:\n    image: ${\n   FLINK_IMAGE}\n    ports:\n      - \"8081:8081\"\n    command: jobmanager\n    volumes:\n      - flink:/opt/flink\n    env_file: .env\n    restart: unless-stopped\n    privileged: true\n    networks:\n      - streampark\n\n  ## streampark-taskmanager container\n  flink-taskmanager:\n    image: ${\n   FLINK_IMAGE}\n    depends_on:\n      - flink-jobmanager\n    command: taskmanager\n    deploy:\n      replicas: 1\n    env_file: .env\n    restart: unless-stopped\n    privileged: true\n    networks:\n      - streampark\n\nnetworks:\n  streampark:\n    driver: bridge\n\nvolumes:\n  flink:\n"})}),"\n",(0,s.jsx)(n.p,{children:"Finally, execute the start command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"cd deploy/docker\ndocker-compose up -d\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You can use ",(0,s.jsx)(n.code,{children:"docker ps"})," to check if the installation was successful. If the following information is displayed, it indicates a successful installation:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:r(8401).A+"",width:"1080",height:"134"})}),"\n",(0,s.jsx)(n.h2,{id:"uploading-configuration-to-the-container",children:"Uploading Configuration to the Container"}),"\n",(0,s.jsxs)(n.p,{children:["In the previous ",(0,s.jsx)(n.code,{children:"env"})," file, ",(0,s.jsx)(n.code,{children:"HADOOP_HOME"}),' was declared, with the corresponding directory being "/streampark/hadoop". Therefore, you need to upload the ',(0,s.jsx)(n.code,{children:"/etc/hadoop"})," from the Hadoop installation package to the ",(0,s.jsx)(n.code,{children:"/streampark/hadoop"})," directory. The commands are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"## Upload Hadoop resources\ndocker cp entire etc directory streampark-docker_streampark-console_1:/streampark/hadoop\n## Enter the container\ndocker exec -it streampark-docker_streampark-console_1 bash\n## Check\nls\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:r(98253).A+"",width:"1080",height:"87"})}),"\n",(0,s.jsxs)(n.p,{children:["In addition, other configuration files, such as Maven's ",(0,s.jsx)(n.code,{children:"settings.xml"})," file, are uploaded in the same manner."]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},34562:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/remote-c75d520f5a61d9f100c0e1c58d304eb3.png"},55670:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/remoteSubmission-390200dc9d1b5a55add19d303add2698.png"},98253:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/streampark_docker_ls_hadoop-8e7848bd06b4c193ef02d0f722ad006a.png"},8401:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/streampark_docker_ps-c715c198d48a9cffef4731beedef4f94.png"},21554:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/streampark_flinkhome-d6e90dd6fa04a003feef2eb6086ca3c3.png"},97611:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/streampark_source_generation_image-6022aab5159e058f0cb295ef4f13f508.png"},76113:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>i});var s=r(30758);const o={},t=s.createContext(o);function a(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);