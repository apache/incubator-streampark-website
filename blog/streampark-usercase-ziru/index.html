<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.3.2">
<title data-rh="true">Ziroom&#x27;s Real-Time Computing Platform Practice Based on Apache StreamPark™ | Apache StreamPark (incubating)</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://streampark.apache.org/blog/streampark-usercase-ziru"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_CN"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Ziroom&#x27;s Real-Time Computing Platform Practice Based on Apache StreamPark™ | Apache StreamPark (incubating)"><meta data-rh="true" name="description" content="Introduction: Ziroom, an O2O internet company focusing on providing rental housing products and services, has built an online, data-driven, and intelligent platform that covers the entire chain of urban living. Real-time computing has always played an important role in Ziroom. To date, Ziroom processes TB-level data daily. This article, brought by the real-time computing team from Ziroom, introduces the in-depth practice of Ziroom&#x27;s real-time computing platform based on StreamPark."><meta data-rh="true" property="og:description" content="Introduction: Ziroom, an O2O internet company focusing on providing rental housing products and services, has built an online, data-driven, and intelligent platform that covers the entire chain of urban living. Real-time computing has always played an important role in Ziroom. To date, Ziroom processes TB-level data daily. This article, brought by the real-time computing team from Ziroom, introduces the in-depth practice of Ziroom&#x27;s real-time computing platform based on StreamPark."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-10-18T07:44:07.000Z"><meta data-rh="true" property="article:tag" content="StreamPark,Production Practice"><link data-rh="true" rel="icon" href="/image/favicon.ico"><link data-rh="true" rel="canonical" href="https://streampark.apache.org/blog/streampark-usercase-ziru"><link data-rh="true" rel="alternate" href="https://streampark.apache.org/blog/streampark-usercase-ziru" hreflang="en"><link data-rh="true" rel="alternate" href="https://streampark.apache.org/zh-CN/blog/streampark-usercase-ziru" hreflang="zh-CN"><link data-rh="true" rel="alternate" href="https://streampark.apache.org/blog/streampark-usercase-ziru" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache StreamPark (incubating) RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache StreamPark (incubating) Atom Feed"><link rel="stylesheet" href="/assets/css/styles.2815dd1e.css">
<script src="/assets/js/runtime~main.788200c2.js" defer="defer"></script>
<script src="/assets/js/main.237a4499.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):window.matchMedia("(prefers-color-scheme: light)").matches?t("light"):t("dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_BCtO" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/image/logo.png" alt="StreamPark Logo" class="themedComponent_Cvhi themedComponent--light_uT2j"><img src="/image/logo.png" alt="StreamPark Logo" class="themedComponent_Cvhi themedComponent--dark_ENih"></div><b class="navbar__title text--truncate">StreamPark</b></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/get-started/intro">Docs</a><a class="navbar__item navbar__link" href="/download">Download</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Community</a><ul class="dropdown__menu"><li><a href="https://www.apache.org/foundation/policies/conduct" target="_blank" rel="noopener noreferrer" class="dropdown__link">Code of conduct</a></li><li><a class="dropdown__link" href="/community/contribution_guide/mailing_lists">Join the mailing lists</a></li><li><a class="dropdown__link" href="/community/contribution_guide/become_committer">Become A Committer</a></li><li><a class="dropdown__link" href="/community/contribution_guide/become_pmc_member">Become A PMC member</a></li><li><a class="dropdown__link" href="/community/contribution_guide/new_committer_process">New Committer Process</a></li><li><a class="dropdown__link" href="/community/contribution_guide/new_pmc_ember_process">New PMC Member Process</a></li><li><a class="dropdown__link" href="/community/submit_guide/document">Documentation Notice</a></li><li><a class="dropdown__link" href="/community/submit_guide/submit_code">Submit Code</a></li><li><a class="dropdown__link" href="/community/submit_guide/code_style_and_quality_guide">Code style and quality guide</a></li><li><a class="dropdown__link" href="/community/submit_guide/documentation_style_guide">Documentation style guide</a></li><li><a class="dropdown__link" href="/community/release/how_to_release_version_2.1.x">How to release version 2.1.x</a></li><li><a class="dropdown__link" href="/community/release/how_to_verify_release">How to Verify Release</a></li></ul></div><a class="navbar__item navbar__link" href="/team">Team</a><a class="navbar__item navbar__link" href="/user">Users</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">ASF</a><ul class="dropdown__menu"><li><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Foundation</a></li><li><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="dropdown__link">License</a></li><li><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="dropdown__link">Events</a></li><li><a href="https://www.apache.org/security/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Security</a></li><li><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Sponsorship</a></li><li><a href="https://www.apache.org/foundation/policies/privacy.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Privacy</a></li><li><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Thanks</a></li></ul></div><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a href="https://github.com/apache/incubator-streampark/issues/507" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">FAQ</a><a href="https://github.com/apache/incubator-streampark" target="_blank" class="githubStars_qnS9"><div class="githubStarsContainer_O7dk"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 1.75C4.27062 1.75 1.25 4.77062 1.25 8.5C1.25 11.4869 3.18219 14.0097 5.86531 14.9041C6.20281 14.9631 6.32937 14.7606 6.32937 14.5834C6.32937 14.4231 6.32094 13.8916 6.32094 13.3263C4.625 13.6384 4.18625 12.9128 4.05125 12.5331C3.97531 12.3391 3.64625 11.74 3.35938 11.5797C3.12312 11.4531 2.78562 11.1409 3.35094 11.1325C3.8825 11.1241 4.26219 11.6219 4.38875 11.8244C4.99625 12.8453 5.96656 12.5584 6.35469 12.3813C6.41375 11.9425 6.59094 11.6472 6.785 11.4784C5.28312 11.3097 3.71375 10.7275 3.71375 8.14563C3.71375 7.41156 3.97531 6.80406 4.40563 6.33156C4.33812 6.16281 4.10187 5.47094 4.47312 4.54281C4.47312 4.54281 5.03844 4.36563 6.32937 5.23469C6.86937 5.08281 7.44313 5.00687 8.01688 5.00687C8.59063 5.00687 9.16438 5.08281 9.70438 5.23469C10.9953 4.35719 11.5606 4.54281 11.5606 4.54281C11.9319 5.47094 11.6956 6.16281 11.6281 6.33156C12.0584 6.80406 12.32 7.40312 12.32 8.14563C12.32 10.7359 10.7422 11.3097 9.24031 11.4784C9.485 11.6894 9.69594 12.0944 9.69594 12.7272C9.69594 13.63 9.6875 14.3556 9.6875 14.5834C9.6875 14.7606 9.81406 14.9716 10.1516 14.9041C12.8178 14.0097 14.75 11.4784 14.75 8.5C14.75 4.77062 11.7294 1.75 8 1.75Z" fill="currentColor"></path></svg><span class="githubText_YlX6">3.9k</span></div></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_DSK9"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg></a><ul class="dropdown__menu"><li><a href="/blog/streampark-usercase-ziru" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-CN/blog/streampark-usercase-ziru" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-CN">简体中文</a></li></ul></div><a class="colorModeButton_yITp undefined"><svg xmlns="http://www.w3.org/2000/svg" fill="none" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6"><path stroke-linecap="round" stroke-linejoin="round" d="M21.752 15.002A9.718 9.718 0 0118 15.75c-5.385 0-9.75-4.365-9.75-9.75 0-1.33.266-2.597.748-3.752A9.753 9.753 0 003 11.25C3 16.635 7.365 21 12.75 21a9.753 9.753 0 009.002-5.998z"></path></svg></a><div class="navbarSearchContainer_dCNk"><div class="navbar__search searchBarContainer_SuYZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_HgBb searchBarLoadingRing_om0w"><div></div><div></div><div></div><div></div></div></div></div><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_pmYA"><div class="container-wrapper blog-container"><div class="container margin-vert--lg"><div class="row"><aside class="col col--2 overflow-hidden" style="opacity:0;transform:translateX(-100px) translateZ(0)"><nav class="sidebar_brwN thin-scrollbar" aria-label="Blog recent posts navigation"><div class="backButton_MCHS"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M8 7v4L2 6l6-5v4h5a8 8 0 1 1 0 16H4v-2h9a6 6 0 0 0 0-12H8Z"></path></svg></div><a class="sidebarItemTitle_r4Q1 margin-bottom--sm" href="/blog">近期文章</a><ul class="sidebarItemList_QwSx clean-list"><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-flink-on-k8s">Apache StreamPark™ Flink on Kubernetes practice</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/flink-development-framework-streampark">Apache StreamPark™ - Powerful Flink Development Framework</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-flink-with-paimon-in-ziru">Ziroom implements the best practice of one-key data input into the lake based on Apache StreamPark™ + Paimon</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-tianyancha">Apache StreamPark™ helps Tianyancha real-time platform construction｜Multiple-fold increase in efficiency</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-joymaker">Apache StreamPark™ Cloud Native Platform Practice at Joymaker</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-chinaunion">China Union&#x27;s Flink Real-Time Computing Platform Ops Practice</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-bondex-with-paimon">Based on Apache Paimon + Apache StreamPark™&#x27;s Streaming Data Warehouse Practice by Bondex</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-shunwang">Apache StreamPark™ in the Large-Scale Production Practice at Shunwang Technology</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-dustess">Apache StreamPark™&#x27;s Best Practices at Dustess, Simplifying Complexity for the Ultimate Experience</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-joyme">Apache StreamPark™&#x27;s Production Practice in Joyme</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-haibo">An All-in-One Computation Tool in Haibo Tech&#x27;s Production Practice and facilitation in Smart City Construction</a></li><li class="sidebarItem_lnhn"><a aria-current="page" class="sidebarItemLink_yNGZ sidebarItemLinkActive_oSRm" href="/blog/streampark-usercase-ziru">Ziroom&#x27;s Real-Time Computing Platform Practice Based on Apache StreamPark™</a></li><li class="sidebarItem_lnhn"><a class="sidebarItemLink_yNGZ" href="/blog/streampark-usercase-changan">Changan Automobile’s upgrade practice from self-developed platform to Apache StreamPark™</a></li></ul></nav></aside><main class="col col--8 overflow-hidden" itemscope="" itemtype="http://schema.org/Blog"><div><div class="row" style="margin:0"><div class="col col--12 article__details article-bg"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Introduction: Ziroom, an O2O internet company focusing on providing rental housing products and services, has built an online, data-driven, and intelligent platform that covers the entire chain of urban living. Real-time computing has always played an important role in Ziroom. To date, Ziroom processes TB-level data daily. This article, brought by the real-time computing team from Ziroom, introduces the in-depth practice of Ziroom&#x27;s real-time computing platform based on StreamPark."><header><h1 class="margin-bottom--md blogPostTitle_thoQ text--center post--titleLink" itemprop="headline">Ziroom&#x27;s Real-Time Computing Platform Practice Based on Apache StreamPark™</h1><div class="margin-vert--md"><time datetime="2024-10-18T07:44:07.000Z" itemprop="datePublished"></time> · <!-- -->23 min read</div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p><img decoding="async" loading="lazy" src="/assets/images/cover-87ff8832da8f5d9398ed5a1e2f283d0d.png" width="1080" height="460" class="img__KcZ"></p>
<p><strong>Introduction:</strong> Ziroom, an O2O internet company focusing on providing rental housing products and services, has built an online, data-driven, and intelligent platform that covers the entire chain of urban living. Real-time computing has always played an important role in Ziroom. To date, Ziroom processes TB-level data daily. This article, brought by the real-time computing team from Ziroom, introduces the in-depth practice of Ziroom&#x27;s real-time computing platform based on StreamPark.</p>
<ul>
<li>Challenges in real-time computing</li>
<li>The journey to the solution</li>
<li>In-depth practice based on StreamPark</li>
<li>Summary of practical experience and examples</li>
<li>Benefits brought by the implementation</li>
<li>Future plans</li>
</ul>
<p>As an O2O internet brand offering rental housing products and services, Ziroom was established in October 2011. To date, Ziroom has served nearly 500,000 landlords and 5 million customers, managing over 1 million housing units. As of March 2021, Ziroom has expanded to 10 major cities including Beijing, Shanghai, Shenzhen, Hangzhou, Nanjing, Guangzhou, Chengdu, Tianjin, Wuhan, and Suzhou. Ziroom has created an online, data-driven, and intelligent platform for quality residential products for both To C and To B markets, covering the entire chain of urban living. The Ziroom app has accumulated 140 million downloads, with an average of 400 million online service calls per day, and owns tens of thousands of smart housing units. Ziroom has now established an O2O closed loop for renting, services, and community on PC, app, and WeChat platforms, eliminating all redundant steps in the traditional renting model, restructuring the residential market through the O2O model, and building China&#x27;s largest O2O youth living community.</p>
<p>With a vast user base, Ziroom has been committed to providing superior product experiences and achieving digital transformation of the enterprise. Since 2021, real-time computing, particularly Flink, has played an important role in Ziroom. To date, Ziroom processes TB-level data daily, with over 500 real-time jobs supporting more than 10 million data calls per day.</p>
<h2 class="anchor anchorWithStickyNavbar_myWT" id="challenges-in-real-time-computing"><strong>Challenges in Real-Time Computing</strong><a href="#challenges-in-real-time-computing" class="hash-link" aria-label="Direct link to challenges-in-real-time-computing" title="Direct link to challenges-in-real-time-computing">​</a></h2>
<p>At Ziroom, real-time computing is mainly divided into two application scenarios:</p>
<ul>
<li>
<p>Data synchronization: Includes Kafka, MySQL, and MongoDB data synchronization to Hive / Paimon / ClickHouse, etc.</p>
</li>
<li>
<p>Real-time data warehouse: Includes real-time indicators for businesses like rentals, acquisitions, and home services.</p>
</li>
</ul>
<p>In the process of implementing real-time computing, we faced several challenges, roughly as follows:</p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="01-low-efficiency-in-job-deployment"><strong>01 Low Efficiency in Job Deployment</strong><a href="#01-low-efficiency-in-job-deployment" class="hash-link" aria-label="Direct link to 01-low-efficiency-in-job-deployment" title="Direct link to 01-low-efficiency-in-job-deployment">​</a></h3>
<p>The process of developing and deploying real-time jobs at Ziroom is as follows: data warehouse developers embed Flink SQL code in the program, debug locally, compile into FatJar, and then submit the job as a work order and JAR package to the operation team. The operation team member responsible for job deployment then deploys the job to the online Kubernetes session environment through the command line. This process involves many steps, each requiring manual intervention, resulting in extremely low efficiency and being prone to errors, affecting work efficiency and stability. Therefore, there is an urgent need to build an efficient and automated real-time computing platform to meet the growing demands of real-time computing.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/job_goes_online-ae38a4f939687b0e80e8e5677885fe0d.png" width="1080" height="356" class="img__KcZ"></p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="02-unclear-job-ownership-information"><strong>02 Unclear Job Ownership Information</strong><a href="#02-unclear-job-ownership-information" class="hash-link" aria-label="Direct link to 02-unclear-job-ownership-information" title="Direct link to 02-unclear-job-ownership-information">​</a></h3>
<p>Due to the lack of unified management of the real-time computing platform, business code is managed by GitLab. Although this solved some problems, we found deficiencies between the repository code and the management of online Flink jobs: lack of clear ownership, lack of grouping and effective permission control, leading to chaotic job management and difficult responsibility tracing. To ensure the consistency and controllability of code and online jobs, there is an urgent need to establish a strict and clear job management system, including strict code version control, clear job ownership and responsible persons, and effective permission control.</p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="03-difficulty-in-job-maintenance"><strong>03 Difficulty in Job Maintenance</strong><a href="#03-difficulty-in-job-maintenance" class="hash-link" aria-label="Direct link to 03-difficulty-in-job-maintenance" title="Direct link to 03-difficulty-in-job-maintenance">​</a></h3>
<p>At Ziroom, multiple versions of Flink jobs are running. Due to frequent API changes and lack of backward compatibility in major version upgrades of Apache Flink, the cost of upgrading project code becomes very high. Therefore, managing these different versions of jobs has become a headache.</p>
<p>Without a</p>
<p>unified job platform, these jobs are submitted using scripts. Jobs vary in importance and data volume, requiring different resources and runtime parameters, necessitating corresponding modifications. Modifications can be made by editing the submission script or directly setting parameters in the code, but this makes configuration information difficult to access, especially when jobs restart or fail and FlinkUI is unavailable, turning configuration information into a black box. Therefore, there is an urgent need for a more efficient platform that supports configuration of real-time computing jobs.</p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="04-difficulty-in-job-development-and-debugging"><strong>04 Difficulty in Job Development and Debugging</strong><a href="#04-difficulty-in-job-development-and-debugging" class="hash-link" aria-label="Direct link to 04-difficulty-in-job-development-and-debugging" title="Direct link to 04-difficulty-in-job-development-and-debugging">​</a></h3>
<p>In our previous development process, we typically embedded SQL code within program code in the local IDEA environment for job development and debugging, verifying the correctness of the program. However, this approach has several disadvantages:</p>
<ol>
<li>
<p>Difficulty in multi-data source debugging. Often, a requirement involves multiple different data sources. For local environment debugging, developers need to apply for white-list access to data, which is both time-consuming and cumbersome.</p>
</li>
<li>
<p>SQL code is hard to read and modify. As SQL code is embedded in program code, it&#x27;s difficult to read and inconvenient to modify. More challenging is debugging through SQL segments, as the lack of SQL version control and syntax verification makes it hard for developers to locate specific SQL lines in client logs to identify the cause of execution failure.</p>
</li>
</ol>
<p>Therefore, there is a need to improve the efficiency of development and debugging.</p>
<h2 class="anchor anchorWithStickyNavbar_myWT" id="the-journey-to-the-solution"><strong>The Journey to the Solution</strong><a href="#the-journey-to-the-solution" class="hash-link" aria-label="Direct link to the-journey-to-the-solution" title="Direct link to the-journey-to-the-solution">​</a></h2>
<p>In the early stages of platform construction, we comprehensively surveyed almost all relevant projects in the industry, covering both commercial paid versions and open-source versions, starting from early 2022. After investigation and comparison, we found that these projects have their limitations to varying extents, and their usability and stability could not be effectively guaranteed.</p>
<p>Overall, StreamPark performed best in our evaluation. It was the only project without major flaws and with strong extensibility: supporting both SQL and JAR jobs, with the most complete and stable deployment mode for Flink jobs. Its unique architectural design not only avoids locking in specific Flink versions but also supports convenient version switching and parallel processing, effectively solving job dependency isolation and conflict issues. The job management &amp; operations capabilities we focused on were also very complete, including monitoring, alerts, SQL validation, SQL version comparison, CI, etc. StreamPark&#x27;s support for Flink on K8s was the most comprehensive among all the open-source projects we surveyed. However, StreamPark&#x27;s K8s mode submission required local image building, leading to storage resource consumption.</p>
<p>In the latest 2.2 version, the community has already restructured this part.</p>
<p>After analyzing the pros and cons of many open-source projects, we decided to participate in projects with excellent architecture, development potential, and an actively dedicated core team. Based on this understanding, we made the following decisions:</p>
<ol>
<li>
<p>In terms of job deployment mode, we decided to adopt the On Kubernetes mode. Real-time jobs have dynamic resource consumption, creating a strong need for Kubernetes&#x27; elastic scaling, which helps us better cope with data output fluctuations and ensure job stability.</p>
</li>
<li>
<p>In the selection of open-source components, after comprehensive comparison and evaluation of various indicators, we finally chose what was then StreamX. Subsequent close communication with the community allowed us to deeply appreciate the serious and responsible attitude of the founders and the united and friendly atmosphere of the community. We also witnessed the project&#x27;s inclusion in the Apache Incubator in September 2022, making us hopeful for its future.</p>
</li>
<li>
<p>On the basis of StreamPark, we aim to promote integration with the existing ecosystem of the company to better meet our business needs.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_myWT" id="in-depth-practice-based-on-apache-streampark"><strong>In-depth Practice Based on Apache StreamPark™</strong><a href="#in-depth-practice-based-on-apache-streampark" class="hash-link" aria-label="Direct link to in-depth-practice-based-on-apache-streampark" title="Direct link to in-depth-practice-based-on-apache-streampark">​</a></h2>
<p>Based on the above decisions, we initiated the evolution of the real-time computing platform, oriented by &quot;pain point needs,&quot; and built a stable, efficient, and easy-to-maintain real-time computing platform based on StreamPark. Since the beginning of 2022, we have participated in the construction of the community while officially scheduling our internal platform construction.</p>
<p>First, we further improved related functionalities on the basis of StreamPark:</p>
<p><img decoding="async" loading="lazy" src="/assets/images/platform_construction-b73e12b57a4a9f558ada77038496bf50.png" width="1080" height="522" class="img__KcZ"></p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="01-ldap-login-support"><strong>01 LDAP Login Support</strong><a href="#01-ldap-login-support" class="hash-link" aria-label="Direct link to 01-ldap-login-support" title="Direct link to 01-ldap-login-support">​</a></h3>
<p>On the basis of StreamPark, we further improved related functionalities, including support for LDAP, so that in the future we can fully open up real-time capabilities, allowing analysts from the company&#x27;s four business lines to use the platform, expected to reach about 170 people. With the increase in numbers, account management becomes increasingly important, especially in the case of personnel changes, account cancellation, and application become frequent and time-consuming operations. Therefore, integrating LDAP becomes particularly important. We communicated with the community in a timely manner and initiated a discussion, eventually contributing this Feature. Now, starting LDAP in StreamPark has become very simple, requiring just two steps:</p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step1-fill-in-the-corresponding-ldap">step1: Fill in the corresponding LDAP<a href="#step1-fill-in-the-corresponding-ldap" class="hash-link" aria-label="Direct link to step1: Fill in the corresponding LDAP" title="Direct link to step1: Fill in the corresponding LDAP">​</a></h4>
<p>configuration:</p>
<p>Edit the application.yml file, setting the LDAP basic information as follows:</p>
<div class="language-yaml codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-yaml codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token key atrule">ldap</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token comment" style="color:rgb(106, 153, 85)"># Is ldap enabled?</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">enable</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token boolean important">false</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token comment" style="color:rgb(106, 153, 85)">## AD server IP, default port 389</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">urls</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> ldap</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain">//99.99.99.99</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token number" style="color:rgb(181, 206, 168)">389</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token comment" style="color:rgb(106, 153, 85)">## Login Account</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">base-dn</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> dc=streampark</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain">dc=com</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">username</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> cn=Manager</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain">dc=streampark</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain">dc=com</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">password</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> streampark</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">user</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token key atrule">identity-attribute</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> uid</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token key atrule">email-attribute</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> mail</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step2-ldap-login">step2: LDAP Login<a href="#step2-ldap-login" class="hash-link" aria-label="Direct link to step2: LDAP Login" title="Direct link to step2: LDAP Login">​</a></h4>
<p>On the login interface, click LDAP login method, then enter the corresponding account and password, and click to log in.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/ldap-39a8f66b7bba105117ecbfc54551dc95.png" width="1080" height="581" class="img__KcZ"></p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="02-automatic-ingress-generation-for-job-submission"><strong>02 Automatic Ingress Generation for Job Submission</strong><a href="#02-automatic-ingress-generation-for-job-submission" class="hash-link" aria-label="Direct link to 02-automatic-ingress-generation-for-job-submission" title="Direct link to 02-automatic-ingress-generation-for-job-submission">​</a></h3>
<p>Due to the company&#x27;s network security policy, only port 80 is opened on the Kubernetes host machines by the operation team, making it impossible to directly access the job WebUI on Kubernetes via &quot;domain + random port.&quot; To solve this problem, we needed to use Ingress to add a proxy layer to the access path, achieving the effect of access routing. In StreamPark version 2.0, we contributed the functionality related to Ingress [3]. We adopted a strategy pattern implementation, initially obtaining Kubernetes metadata information to identify its version and accordingly constructing respective objects, ensuring smooth use of the Ingress function across various Kubernetes environments.</p>
<p>The specific configuration steps are as follows:</p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step1-click-to-enter-setting--choose-ingress-setting-fill-in-the-domain-name">step1: Click to enter Setting-&gt; Choose Ingress Setting, fill in the domain name<a href="#step1-click-to-enter-setting--choose-ingress-setting-fill-in-the-domain-name" class="hash-link" aria-label="Direct link to step1: Click to enter Setting-&gt; Choose Ingress Setting, fill in the domain name" title="Direct link to step1: Click to enter Setting-&gt; Choose Ingress Setting, fill in the domain name">​</a></h4>
<p><img decoding="async" loading="lazy" src="/assets/images/ingress_setting-2b8e1403aae4ff708a05f61f87d34733.png" width="1080" height="384" class="img__KcZ"></p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step2-submit-a-job">step2: Submit a job<a href="#step2-submit-a-job" class="hash-link" aria-label="Direct link to step2: Submit a job" title="Direct link to step2: Submit a job">​</a></h4>
<p><img decoding="async" loading="lazy" src="/assets/images/k8s_job-1759da059df126162f2b7e0c7b9b121a.png" width="1080" height="367" class="img__KcZ"></p>
<p>Upon entering the K8s management platform, you can observe that submitting a Flink job also corresponds to submitting an Ingress with the same name.</p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step3-click-to-enter-flinks-webui">step3: Click to enter Flink&#x27;s WebUI<a href="#step3-click-to-enter-flinks-webui" class="hash-link" aria-label="Direct link to step3: Click to enter Flink&#x27;s WebUI" title="Direct link to step3: Click to enter Flink&#x27;s WebUI">​</a></h4>
<p>You will notice that the generated address consists of three parts: domain + job submission namespace + job name.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/flink_webui-ca719f066a60a19004af7d6761eae27d.png" width="1080" height="451" class="img__KcZ"></p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="03-support-for-viewing-job-deployment-logs"><strong>03 Support for Viewing Job Deployment Logs</strong><a href="#03-support-for-viewing-job-deployment-logs" class="hash-link" aria-label="Direct link to 03-support-for-viewing-job-deployment-logs" title="Direct link to 03-support-for-viewing-job-deployment-logs">​</a></h3>
<p>In the process of continuous job deployment, we gradually realized that without logs, we cannot perform effective operations. Log retention, archiving, and viewing became an important part in our later problem-solving process. Therefore, in StreamPark version 2.0, we contributed the ability to archive startup logs in On Kubernetes mode and view them on the page [4]. Now, by clicking the log viewing button in the job list, it is very convenient to view the real-time logs of the job.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/k8s_log-aafb7f2dff97d584b187ab4a22de9eaf.png" width="1080" height="473" class="img__KcZ"></p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="04-integration-of-grafana-monitoring-chart-links"><strong>04 Integration of Grafana Monitoring Chart Links</strong><a href="#04-integration-of-grafana-monitoring-chart-links" class="hash-link" aria-label="Direct link to 04-integration-of-grafana-monitoring-chart-links" title="Direct link to 04-integration-of-grafana-monitoring-chart-links">​</a></h3>
<p>In actual use, as the number of jobs increased, the number of users rose, and more departments were involved, we faced the problem of difficulty in self-troubleshooting. Our team&#x27;s operational capabilities are actually very limited. Due to the difference in professional fields, when we tell users to view charts and logs on Grafana and ELK, users often feel at a loss and do not know how to find information related to their jobs.</p>
<p>To solve this problem, we proposed a demand in the community: we hoped that each job could directly jump to the corresponding monitoring chart and log archive page through a hyperlink, so that users could directly view the monitoring information and logs related to their jobs. This avoids tedious searches in complex system interfaces, thus improving the efficiency of troubleshooting.</p>
<p>We had a discussion in the community, and it was quickly responded to, as everyone thought it was a common need. Soon, a developer contributed a design and related PR, and the issue was quickly resolved. Now, to enable this feature in StreamPark has become very simple:</p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step1-create-a-badge-label">step1: Create a badge label<a href="#step1-create-a-badge-label" class="hash-link" aria-label="Direct link to step1: Create a badge label" title="Direct link to step1: Create a badge label">​</a></h4>
<p><img decoding="async" loading="lazy" src="/assets/images/create_badge_label-a6542822bd5a1e9274ca0017d25a9ccd.png" width="1080" height="649" class="img__KcZ"></p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step2-associate-the-badge-label-with-a-redirect-link">step2: Associate the badge label with a redirect link<a href="#step2-associate-the-badge-label-with-a-redirect-link" class="hash-link" aria-label="Direct link to step2: Associate the badge label with a redirect link" title="Direct link to step2: Associate the badge label with a redirect link">​</a></h4>
<p><img decoding="async" loading="lazy" src="/assets/images/relevancy-a1914717d6479d16e07f7f1e6193f409.png" width="1062" height="400" class="img__KcZ"></p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step3-click-the-badge-label-for-link-redirection">step3: Click the badge label for link redirection<a href="#step3-click-the-badge-label-for-link-redirection" class="hash-link" aria-label="Direct link to step3: Click the badge label for link redirection" title="Direct link to step3: Click the badge label for link redirection">​</a></h4>
<p><img decoding="async" loading="lazy" src="/assets/images/link_jump-f353a479fac3e5576056edcaab705dda.png" width="1080" height="381" class="img__KcZ"></p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="05-integration-of-flink-sql-security-for-permission-control"><strong>05 Integration of Flink SQL Security for Permission Control</strong><a href="#05-integration-of-flink-sql-security-for-permission-control" class="hash-link" aria-label="Direct link to 05-integration-of-flink-sql-security-for-permission-control" title="Direct link to 05-integration-of-flink-sql-security-for-permission-control">​</a></h3>
<p>In our system, lineage management is based on Apache Atlas, and permission management is based on the open-source project Flink-sql-security, which supports user-level data desensitization and row-level data access control, allowing specific users to only access desensitized data or authorized rows.</p>
<p>This design is to handle some complex inheritance logic. For example, when joining encrypted field age in Table A with Table B to obtain Table C, the age field in Table C should inherit the encryption logic of Table A to ensure the encryption status of data is not lost during processing. This way, we can better protect data security and ensure that data complies with security standards throughout the processing process.</p>
<p>For permission control, we developed a Flink-sql-security-streampark plugin based on Flink-s</p>
<p>ql-security. The basic implementation is as follows:</p>
<ol>
<li>
<p>During submission check, the system parses the submitted SQL, obtaining InputTable and OutputTable datasets.</p>
</li>
<li>
<p>The system queries the remote permission service to obtain the user&#x27;s bound RBAC (Role-Based Access Control) permissions.</p>
</li>
<li>
<p>Based on the RBAC permissions, the system gets the encryption rules for the corresponding tables.</p>
</li>
<li>
<p>The system rewrites the SQL, wrapping the original SQL query fields with a preset encryption algorithm, thereby reorganizing the logic.</p>
</li>
<li>
<p>Finally, the system submits according to the reorganized logic.</p>
</li>
</ol>
<p>Through this integration and plugin development, we implemented permission control for user query requests, thereby ensuring data security.</p>
<p><strong>01 Row-level Permission Conditions</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/row_level_permissions-cd7665f926dd3c92e8479be34f9084d5.png" width="1080" height="348" class="img__KcZ"></p>
<p><img decoding="async" loading="lazy" src="/assets/images/row_level_permissions_table-780239498d6b6d09c8a5c1ffe4ad44ef.png" width="1080" height="151" class="img__KcZ"></p>
<p>Input SQL</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">SELECT * FROM orders;</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>User A&#x27;s actual execution SQL:</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">SELECT * FROM orders WHERE region = &#x27;beijing&#x27;;</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>User B&#x27;s actual execution SQL:</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">SELECT * FROM orders WHERE region = &#x27;hangzhou&#x27;;</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><strong>02 Field Desensitization Conditions</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/field_desensitization-3005c6f1d9c2f7ad0530d09a5a9aecea.png" width="1080" height="269" class="img__KcZ"></p>
<p><img decoding="async" loading="lazy" src="/assets/images/field_desensitization_table-e200cbbf0081e2d416411659a188bb90.png" width="1080" height="160" class="img__KcZ"></p>
<p>Input SQL</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">SELECT name, age, price, phone FROM user;</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Execution SQL:</p>
<p>User A&#x27;s actual execution SQL:</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">SELECT Encryption_function(name), age, price, Sensitive_field_functions(phone) FROM user;</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>User B&#x27;s actual execution SQL:</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">SELECT name, Encryption_function(age), price, Sensitive_field_functions(phone) FROM user;</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="06-data-synchronization-platform-based-on-apache-streampark"><strong>06 Data Synchronization Platform Based on Apache StreamPark™</strong><a href="#06-data-synchronization-platform-based-on-apache-streampark" class="hash-link" aria-label="Direct link to 06-data-synchronization-platform-based-on-apache-streampark" title="Direct link to 06-data-synchronization-platform-based-on-apache-streampark">​</a></h3>
<p>With the successful implementation of StreamPark&#x27;s technical solutions in the company, we achieved deep support for Flink jobs, bringing a qualitative leap in data processing. This prompted us to completely revamp our past data synchronization logic, aiming to reduce operational costs through technical optimization and integration. Therefore, we gradually replaced historical Sqoop jobs, Canal jobs, and Hive JDBC Handler jobs with Flink CDC jobs, Flink stream, and batch jobs. In this process, we continued to optimize and strengthen StreamPark&#x27;s interface capabilities, adding a status callback mechanism and achieving perfect integration with the DolphinScheduler [7] scheduling system, further enhancing our data processing capabilities.</p>
<p>External system integration with StreamPark is simple, requiring only a few steps:</p>
<ol>
<li>First, create a token for API access:</li>
</ol>
<p><img decoding="async" loading="lazy" src="/assets/images/token-94cb364497fe129fc62be3bbb7193390.png" width="1080" height="288" class="img__KcZ"></p>
<ol start="2">
<li>View the external call link of the Application:</li>
</ol>
<p><img decoding="async" loading="lazy" src="/assets/images/call_link-da9d24e3a1993590cf25da174d5fb676.png" width="1080" height="390" class="img__KcZ"></p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">curl -X POST &#x27;/flink/app/start&#x27; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">-H &#x27;Authorization: $token&#x27; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">-H &#x27;Content-Type: application/x-www-form-urlencoded; charset=UTF-8&#x27; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">--data-urlencode &#x27;savePoint=&#x27; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">--data-urlencode &#x27;allowNonRestored=false&#x27; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">--data-urlencode &#x27;savePointed=false&#x27; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">--data-urlencode &#x27;id=100501&#x27;</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="3">
<li>Configure Http scheduling in DolphinScheduler</li>
</ol>
<p><img decoding="async" loading="lazy" src="/assets/images/http_scheduling-267e8ae0aa04d5901cd98bea50370d69.png" width="1080" height="576" class="img__KcZ"></p>
<h2 class="anchor anchorWithStickyNavbar_myWT" id="summary-of-practical-experience"><strong>Summary of Practical Experience</strong><a href="#summary-of-practical-experience" class="hash-link" aria-label="Direct link to summary-of-practical-experience" title="Direct link to summary-of-practical-experience">​</a></h2>
<p>During our in-depth use of StreamPark, we summarized some common issues and explored solutions in the practice process, which we have compiled into examples for reference.</p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="01-building-base-images"><strong>01 Building Base Images</strong><a href="#01-building-base-images" class="hash-link" aria-label="Direct link to 01-building-base-images" title="Direct link to 01-building-base-images">​</a></h3>
<p>To deploy a Flink job on Kubernetes using StreamPark, you first need to prepare a Base image built on Flink. Then, on the Kubernetes platform, the user-provided image is used to start the Flink job. If we continue to use the official &quot;bare image,&quot; it is far from sufficient for actual development. Business logic developed by users often involves multiple upstream and downstream data sources, requiring related data source Connectors and dependencies like Hadoop. Therefore, these dependencies need to be included in the image. Below, I will introduce the specific operation steps.</p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step1-first-create-a-folder-containing-two-folders-and-a-dockerfile-file">step1: First, create a folder containing two folders and a Dockerfile file<a href="#step1-first-create-a-folder-containing-two-folders-and-a-dockerfile-file" class="hash-link" aria-label="Direct link to step1: First, create a folder containing two folders and a Dockerfile file" title="Direct link to step1: First, create a folder containing two folders and a Dockerfile file">​</a></h4>
<p><img decoding="async" loading="lazy" src="data:image/png;base64,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" width="796" height="166" class="img__KcZ"></p>
<p>conf folder: Contains HDFS configuration files, mainly for configuring Flink&#x27;s Checkpoint writing and using Hive metadata in FlinkSQL</p>
<p><img decoding="async" loading="lazy" src="/assets/images/hdfs_conf-8af6d1660d09b58fc28a6fec1bff604b.png" width="824" height="284" class="img__KcZ"></p>
<p>lib folder: Contains the related Jar dependency packages, as follows:</p>
<p><img decoding="async" loading="lazy" src="/assets/images/lib-faa76eddded5bdcb4228bf65c2d3122d.png" width="842" height="722" class="img__KcZ"></p>
<p>Dockerfile file for defining image construction</p>
<div class="language-dockerfile codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-dockerfile codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">FROM apache/flink:1.14.5-scala_2.11-java8</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">ENV TIME_ZONE=Asia/Shanghai</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">COPY ./conf /opt/hadoop/conf</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">COPY lib $FLINK_HOME/lib/</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step2-image-build-command-using-multi-architecture-build-mode-as-follows">step2: Image build command using multi-architecture build mode, as follows:<a href="#step2-image-build-command-using-multi-architecture-build-mode-as-follows" class="hash-link" aria-label="Direct link to step2: Image build command using multi-architecture build mode, as follows:" title="Direct link to step2: Image build command using multi-architecture build mode, as follows:">​</a></h4>
<div class="language-dockerfile codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-dockerfile codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">docker buildx build --push --platform linux/amd64 -t ${private image repository address}</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="02-base-image-integration-with-arthas-example"><strong>02 Base Image Integration with Arthas Example</strong><a href="#02-base-image-integration-with-arthas-example" class="hash-link" aria-label="Direct link to 02-base-image-integration-with-arthas-example" title="Direct link to 02-base-image-integration-with-arthas-example">​</a></h3>
<p>As more jobs are released and go live within our company, we often encounter performance degradation in long-running jobs, such as reduced Kafka consumption capacity, increased memory usage, and extended GC time. We recommend using Arthas, an open-source Java diagnostic tool by Alibaba. It allows real-time global viewing of Java application load, memory, GC, thread status, and without modifying application code, it enables viewing method call parameters, exceptions, monitoring method execution time, class loading information, etc., greatly enhancing our efficiency in troubleshooting online issues.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/arthas-31f8df2a167d53420d2b4a828003a449.png" width="1080" height="526" class="img__KcZ"></p>
<p><img decoding="async" loading="lazy" src="/assets/images/advanced-e5c462ae9a888deddb5939a7c8065732.png" width="1080" height="416" class="img__KcZ"></p>
<p><img decoding="async" loading="lazy" src="/assets/images/arthas_log-fbc7dc0891690e569c6ad63359fc52fd.png" width="1080" height="390" class="img__KcZ"></p>
<p>Therefore, we integrated Arthas into the base image to facilitate runtime problem troubleshooting.</p>
<div class="language-dockerfile codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-dockerfile codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">FROM apache/flink:1.14.5-scala_2.11-java8</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">ENV TIME_ZONE=Asia/Shanghai</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">COPY ./conf /opt/hadoop/conf</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">COPY lib $FLINK_HOME/lib/</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">RUN apt-get update --fix-missing &amp;&amp; apt-get install -y fontconfig --fix-missing &amp;&amp; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    apt-get install -y openjdk-8-jdk &amp;&amp; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    apt-get install -y ant &amp;&amp; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    apt-get clean;</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">RUN apt-get install sudo -y</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"># Fix certificate issues</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">RUN apt-get update &amp;&amp; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    apt-get install ca-certificates-java &amp;&amp; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    apt-get clean &amp;&amp; \</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    update-ca-certificates -f;</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"># Setup JAVA_HOME -- useful for docker commandline</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">RUN export JAVA_HOME</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">RUN apt-get install -y unzip</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">RUN curl -Lo arthas-packaging-latest-bin.zip  &#x27;https://arthas.aliyun.com/download/latest_version?mirror=aliyun&#x27;</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">RUN unzip -d arthas-latest-bin arthas-packaging-latest-bin.zip</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="03-resolution-of-dependency-conflicts-in-images"><strong>03 Resolution of Dependency Conflicts in Images</strong><a href="#03-resolution-of-dependency-conflicts-in-images" class="hash-link" aria-label="Direct link to 03-resolution-of-dependency-conflicts-in-images" title="Direct link to 03-resolution-of-dependency-conflicts-in-images">​</a></h3>
<p>In the process of using StreamPark, we often encounter dependency conflict exceptions like NoClassDefFoundError, ClassNotFoundException, and NoSuchMethodError in Flink jobs running on base images. The troubleshooting approach is to find the package path of the conflicting class indicated in the error. For example, if the error class is in org.apache.orc<!-- -->:orc-core<!-- -->, go to the corresponding module directory, run <code>mvn dependency::tree</code>, search for orc-core, see who brought in the dependency, and remove it using exclusion. Below, I will introduce in detail a method of custom packaging to resolve dependency conflicts, illustrated by a dependency conflict caused by the flink-shaded-hadoop-3-uber JAR package in a base image.</p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step1-clone-the-flink-shaded-project-locally">step1: Clone the flink-shaded project locally👇<a href="#step1-clone-the-flink-shaded-project-locally" class="hash-link" aria-label="Direct link to step1: Clone the flink-shaded project locally👇" title="Direct link to step1: Clone the flink-shaded project locally👇">​</a></h4>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">git clone https://github.com/apache/flink-shaded.git</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="/assets/images/flink_shaded-f21909c81059b2ea4ebd8307998cbc26.png" width="1080" height="529" class="img__KcZ"></p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step2-load-the-project-into-idea">step2: Load the project into IDEA<a href="#step2-load-the-project-into-idea" class="hash-link" aria-label="Direct link to step2: Load the project into IDEA" title="Direct link to step2: Load the project into IDEA">​</a></h4>
<p><img decoding="async" loading="lazy" src="/assets/images/idea-d654040d5fad6fefd756ede9a9ac8a4b.png" width="1080" height="586" class="img__KcZ"></p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step3-exclude-the-conflicting-parts-and-then-package-them">step3: Exclude the conflicting parts and then package them.<a href="#step3-exclude-the-conflicting-parts-and-then-package-them" class="hash-link" aria-label="Direct link to step3: Exclude the conflicting parts and then package them." title="Direct link to step3: Exclude the conflicting parts and then package them.">​</a></h4>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="04-centralized-job-configuration-example"><strong>04 Centralized Job Configuration Example</strong><a href="#04-centralized-job-configuration-example" class="hash-link" aria-label="Direct link to 04-centralized-job-configuration-example" title="Direct link to 04-centralized-job-configuration-example">​</a></h3>
<p>One of the great conveniences of using StreamPark is centralized configuration management. You can configure all settings in the conf file in the Flink directory bound to the platform.</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">cd /flink-1.14.5/conf</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">vim flink-conf.yaml</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" src="/assets/images/conf-ff77bc47a1f6af137a3326cc05cfdfee.png" width="1080" height="562" class="img__KcZ"></p>
<p>After completing the configuration, save it. Then go to the platform&#x27;s Setting, and click on the Flink Conf icon.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/flink_conf-db4fdbcbac08f9e19eaf550c1332de25.png" width="1080" height="351" class="img__KcZ"></p>
<p>Clicking Sync Conf will synchronize the global configuration file, and new jobs will be submitted with the new configuration.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/sync_conf-9d2b45b6f42376859fda062127e33d82.png" width="1080" height="485" class="img__KcZ"></p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="05-apache-streampark-dns-resolution-configuration"><strong>05 Apache StreamPark™ DNS Resolution Configuration</strong><a href="#05-apache-streampark-dns-resolution-configuration" class="hash-link" aria-label="Direct link to 05-apache-streampark-dns-resolution-configuration" title="Direct link to 05-apache-streampark-dns-resolution-configuration">​</a></h3>
<p>A correct and reasonable DNS resolution configuration is very important when submitting FlinkSQL on the StreamPark platform. It mainly involves the following points:</p>
<ol>
<li>
<p>Flink jobs&#x27; Checkpoint writing to HDFS requires a snapshot write to an HDFS node obtained through ResourceManager. If there are expansions in the Hadoop cluster in the enterprise, and these new nodes are not covered by the DNS resolution service, this will directly lead to Checkpoint failure, affecting online stability.</p>
</li>
<li>
<p>Flink jobs typically need to configure connection strings for different internal data sources. Configuring the database&#x27;s real IP address often leads to job exits due to IP changes during database migration. Therefore, in production, connection strings are often composed of domain names and attribute parameters, with DNS services resolving them to real IP addresses for access.</p>
</li>
</ol>
<p>Initially, we maintained DNS configuration through Pod Template.</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">apiVersion: v1</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">kind: Pod</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">metadata:</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  name: pod-template</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">spec:</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  hostAliases:</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    - ip: 10.216.xxx.79</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      hostnames:</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        - handoop1</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    - host</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">names:</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        - handoop2</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      ip: 10.16.xx.48</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    - hostnames:</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        - handoop3</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      ip: 10.16.xx.49</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    - hostnames:</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        - handoop4</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">      ip: 10.16.xx.50</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">   .......</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Although theoretically feasible, we encountered a series of problems in practice. When expanding HDFS, we found failures in Flink&#x27;s Checkpoint function, and database migration also faced connection failures, causing sudden online service outages. After in-depth investigation, we found that the root cause was in DNS resolution.</p>
<p>Previously, we used hostAliases to maintain the mapping between domain names and IP addresses. However, this method was costly in practice, as every update of hostAliases required stopping all Flink jobs, undoubtedly increasing our operational costs. To seek a more flexible and reliable method to manage DNS resolution configuration and ensure the normal operation of Flink jobs, we decided to build dnsmasq for bidirectional DNS resolution.</p>
<p>After configuring and installing dnsmasq, we first needed to override the resolv.conf configuration file in the /etc directory of the Flink image. However, since resolv.conf is a read-only file, if we want to override it, we need to use mounting. Therefore, we first configured resolv.conf as a ConfigMap for use during the override. This way, we can more flexibly and reliably manage DNS resolution configuration, ensuring stable operation of Flink jobs.</p>
<div class="language-yaml codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-yaml codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token key atrule">apiVersion</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> v1</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">data</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">resolv.conf</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> &quot;nameserver  10.216.138.226&quot; //DNS service</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">kind</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> ConfigMap</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">creationTimestamp</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(206, 145, 120)">&quot;2022-07-13T10:16:18Z&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">managedFields</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> dns</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain">configmap</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">  </span><span class="token key atrule">namespace</span><span class="token punctuation" style="color:rgb(212, 212, 212)">:</span><span class="token plain"> native</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain">flink</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Mounting it through Pod Template.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/pod_template-d8a9864bc8486d493025cc5e8b0e6dd1.png" width="1080" height="476" class="img__KcZ"></p>
<p>This way, DNS related to the big data platform can be maintained on dnsmasq, while the host machine running Flink jobs can follow the DNS resolution process.</p>
<ol>
<li>
<p>First, check the local hosts file to see if there is a corresponding relationship, read the record for resolution, and proceed to the next step if not.</p>
</li>
<li>
<p>The operating system checks the local DNS cache, and if not found, moves to the next step.</p>
</li>
<li>
<p>The operating system searches the DNS server address defined in our network configuration.</p>
</li>
</ol>
<p>This achieves dynamic recognition of DNS changes.</p>
<h3 class="anchor anchorWithStickyNavbar_myWT" id="06-multi-instance-deployment-practice"><strong>06 Multi-Instance Deployment Practice</strong><a href="#06-multi-instance-deployment-practice" class="hash-link" aria-label="Direct link to 06-multi-instance-deployment-practice" title="Direct link to 06-multi-instance-deployment-practice">​</a></h3>
<p>In actual production environments, we often need to operate multiple clusters, including a set for testing and a set for official online use. Tasks are first verified and performance tested in the test cluster, then released to the official online cluster after ensuring accuracy.</p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step1-modify-the-port-number-to-avoid-conflicts-between-multiple-service-ports">step1: Modify the port number to avoid conflicts between multiple service ports<a href="#step1-modify-the-port-number-to-avoid-conflicts-between-multiple-service-ports" class="hash-link" aria-label="Direct link to step1: Modify the port number to avoid conflicts between multiple service ports" title="Direct link to step1: Modify the port number to avoid conflicts between multiple service ports">​</a></h4>
<p><img decoding="async" loading="lazy" src="/assets/images/port_number-e421801c946def0b0e2df3747239fec6.png" width="1080" height="622" class="img__KcZ"></p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step2-modify-workspace">step2: Modify workspace<a href="#step2-modify-workspace" class="hash-link" aria-label="Direct link to step2: Modify workspace" title="Direct link to step2: Modify workspace">​</a></h4>
<p>Different instance services need to configure different workspaces to avoid resource interference leading to strange bugs.</p>
<h4 class="anchor anchorWithStickyNavbar_myWT" id="step3-launch-multi-instance-services">step3: Launch multi-instance services<a href="#step3-launch-multi-instance-services" class="hash-link" aria-label="Direct link to step3: Launch multi-instance services" title="Direct link to step3: Launch multi-instance services">​</a></h4>
<p>To achieve isolation between production and testing environments, we introduced a key step at the beginning of the startup process. We input the command (for the Hadoop B cluster):</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">export HADOOP_CONF_DIR=/home/streamx/conf</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This effectively cut off the default logic of Flink on K8s loading HDFS configuration. This operation ensures that A StreamPark only connects to A Hadoop environment, while B StreamPark connects to B Hadoop environment, thus achieving complete isolation between testing and production environments.</p>
<p>Specifically, after this command takes effect, we can ensure that Flink jobs submitted on port 10002 connect to the B Hadoop environment. Thus, the B Hadoop environment is isolated from the Hadoop environment used by Flink jobs submitted on port 10000 in the past, effectively preventing interference between different environments and ensuring system stability and reliability.</p>
<p>The following content is an analysis of Flink&#x27;s logic for loading the Hadoop environment:</p>
<div class="language-yaml codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-yaml codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">// Process of finding Hadoop configuration files</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">//1. First</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> check if the parameter kubernetes.hadoop.conf.config</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain">map.name is added</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">@Override</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">public Optional&lt;String</span><span class="token punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"> get</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">ExistingHadoopConfigurationConfigMap() </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    final String existingHadoopConfigMap =</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            flinkConfig.getString(KubernetesConfigOptions.HADOOP_CONF_CONFIG_MAP);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    if (StringUtils.isBlank(existingHadoopConfigMap)) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        return Optional.empty();</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"> else </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        return Optional.of(existingHadoopConfigMap.trim());</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">@Override</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">public Optional&lt;String</span><span class="token punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"> getLocalHadoopConfigurationDirectory() </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    // 2. If parameter 1 is not specified</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> check for the HADOOP_CONF_DIR environment variable in the local environment where the native command is submitted</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    final String hadoopConfDirEnv = System.getenv(Constants.ENV_HADOOP_CONF_DIR);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    if (StringUtils.isNotBlank(hadoopConfDirEnv)) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        return Optional.of(hadoopConfDirEnv);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    // 3. If environment variable 2 is not present</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> continue to check for the HADOOP_HOME environment variable</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    final String hadoopHomeEnv = System.getenv(Constants.ENV_HADOOP_HOME);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    if (StringUtils.isNotBlank(hadoopHomeEnv)) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        // Hadoop 2.2+</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        final File hadoop2ConfDir = new File(hadoopHomeEnv</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> &quot;/etc/hadoop&quot;);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        if (hadoop2ConfDir.exists()) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            return Optional.of(hadoop2ConfDir.getAbsolutePath());</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        // Hadoop 1.x</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        final File hadoop1ConfDir = a new File(hadoopHomeEnv</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> &quot;/conf&quot;);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        if (hadoop1ConfDir.exists()) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            return Optional.of(hadoop1ConfDir.getAbsolutePath());</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    return Optional.empty();</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">final List&lt;File</span><span class="token punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"> hadoopConfigurationFileItems = getHadoopConfigurationFileItems(localHadoopConfigurationDirectory.get());</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">// If 1</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> </span><span class="token number" style="color:rgb(181, 206, 168)">2</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> 3 are not found</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> it means there is no Hadoop environment</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">if (hadoopConfigurationFileItems.isEmpty()) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    LOG.warn(</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            &quot;Found 0 files in directory </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> skip to mount the Hadoop Configuration ConfigMap.&quot;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">            localHadoopConfigurationDirectory.get());</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    return flinkPod;</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">// If 2 or 3 exists</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> it will look for core</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain">site.xml and hdfs</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain">site.xml files in the path</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">private List&lt;File</span><span class="token punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"> getHadoopConfigurationFileItems(String localHadoopConfigurationDirectory) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    final List&lt;String</span><span class="token punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain"> expectedFileNames = new ArrayList&lt;</span><span class="token punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token plain">();</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    expectedFileNames.add(&quot;core</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain">site.xml&quot;);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    expectedFileNames.add(&quot;hdfs</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain">site.xml&quot;);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    final File directory = new File(localHadoopConfigurationDirectory);</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    if (directory.exists() and directory.isDirectory()) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        return Arrays.stream(directory.listFiles())</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">                .filter(</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">                        file </span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token punctuation" style="color:rgb(212, 212, 212)">&gt;</span><span class="token scalar string" style="color:rgb(206, 145, 120)"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token scalar string" style="color:rgb(206, 145, 120)">                                file.isFile()</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token scalar string" style="color:rgb(206, 145, 120)">                                        and expectedFileNames.stream()</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token scalar string" style="color:rgb(206, 145, 120)">                                                .anyMatch(name -&gt; file.getName().equals(name)))</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">                .collect(Collectors.toList());</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"> else </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">        return Collections.emptyList();</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">// If a Hadoop environment is present</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> the above two files will be parsed as key</span><span class="token punctuation" style="color:rgb(212, 212, 212)">-</span><span class="token plain">value pairs</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> and then constructed into a ConfigMap</span><span class="token punctuation" style="color:rgb(212, 212, 212)">,</span><span class="token plain"> with the name following this naming rule</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">public static String getHadoopConfConfigMapName(String clusterId) </span><span class="token punctuation" style="color:rgb(212, 212, 212)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">    return Constants.HADOOP_CONF_CONFIG_MAP_PREFIX + clusterId;</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain"></span><span class="token punctuation" style="color:rgb(212, 212, 212)">}</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Then conduct process port occupancy queries:</p>
<div class="language-shell codeBlockContainer_APcc theme-code-block"><div class="codeBlockContent_IhPH"><pre tabindex="0" class="prism-code language-shell codeBlock_tTXe thin-scrollbar" style="color:#9CDCFE;background-color:#1E1E1E"><code class="codeBlockLines_qImF"><span class="token-line" style="color:#9CDCFE"><span class="token plain">netstat -tlnp | grep 10000</span><br></span><span class="token-line" style="color:#9CDCFE"><span class="token plain">netstat -tlnp | grep 10002</span><br></span></code></pre><div class="buttonGroup_inAn"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_N8Av" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_EGLt"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_gJas"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_myWT" id="benefits-brought"><strong>Benefits Brought</strong><a href="#benefits-brought" class="hash-link" aria-label="Direct link to benefits-brought" title="Direct link to benefits-brought">​</a></h2>
<p>Our team has been using StreamX (the predecessor of StreamPark) and, after more than a year of practice and refinement, StreamPark has significantly improved our challenges in developing, managing, and operating Apache Flink® jobs. As a one-stop service platform, StreamPark greatly simplifies the entire development process. Now, we can complete job development, compilation, and release directly on the StreamPark platform, not only lowering the management and deployment threshold of Flink but also significantly improving development efficiency.</p>
<p>Since deploying StreamPark, we have been using the platform on a large scale in a production environment. From initially managing over 50 FlinkSQL jobs to nearly 500 jobs now, as shown in the diagram, StreamPark is divided into 7 teams, each with dozens of jobs. This transformation not only demonstrates StreamPark&#x27;s scalability and efficiency but also fully proves its strong practical value in actual business.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/production_environment-4f155f3b14a4e95c5367ba99a6c5a6f8.png" width="1080" height="522" class="img__KcZ"></p>
<h2 class="anchor anchorWithStickyNavbar_myWT" id="future-expectations"><strong>Future Expectations</strong><a href="#future-expectations" class="hash-link" aria-label="Direct link to future-expectations" title="Direct link to future-expectations">​</a></h2>
<p>As one of the early users of StreamPark, we have maintained close communication with the community, participating in the stability improvement of StreamPark. We have submitted bugs encountered in production operation and new features to the community. In the future, we hope to manage the metadata information of Apache Paimon lake tables and the capability of auxiliary jobs for</p>
<p>Paimon&#x27;s Actions on StreamPark. Based on the Flink engine, by interfacing with the Catalog of lake tables and Action jobs, we aim to realize the management and optimization of lake table jobs in one integrated capability. Currently, StreamPark is working on integrating the capabilities with Paimon data, which will greatly assist in real-time data lake ingestion in the future.</p>
<p>We are very grateful for the technical support that the StreamPark team has provided us all along. We wish Apache StreamPark continued success, more users, and its early graduation to become a top-level Apache project.</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_Wr5y"><div class="post-footer"><div class="col"><b>Tags:</b><ul class="tags_bUDc padding--none margin-left--sm"><li class="tag_edSN"><a class="tag_mtSO tagRegular_u5wg" href="/blog/tags/stream-park">StreamPark</a></li><li class="tag_edSN"><a class="tag_mtSO tagRegular_u5wg" href="/blog/tags/production-practice">Production Practice</a></li></ul></div><div class="col col-3 text--right"><a href="https://github.com/apache/incubator-streampark-website/edit/dev/blog/8-streampark-usercase-ziru.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_WHnd" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></footer></article></div></div></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev paginationNavLink_UdUv" href="/blog/streampark-usercase-haibo"><svg width="24" height="25" viewBox="0 0 24 25" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.2751 19.175L10.3251 18.125L5.4501 13.25H21.6001V11.75H5.4501L10.3251 6.87501L9.2751 5.82501L2.5751 12.5L9.2751 19.175Z" fill="currentColor"></path></svg><div class="paginationNavContent__3xr"><div class="pagination-nav__sublabel">Newer Post</div><div class="paginationNavLabel_YPzM pagination-nav__label">An All-in-One Computation Tool in Haibo Tech&#x27;s Production Practice and facilitation in Smart City Construction</div></div></a><a class="pagination-nav__link pagination-nav__link--next paginationNavLink_UdUv" href="/blog/streampark-usercase-changan"><div class="paginationNavContent__3xr"><div class="pagination-nav__sublabel">Older Post</div><div class="paginationNavLabel_YPzM pagination-nav__label">Changan Automobile’s upgrade practice from self-developed platform to Apache StreamPark™</div></div><svg width="24" height="25" viewBox="0 0 24 25" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.7249 19.175L13.6749 18.125L18.5499 13.25H2.3999V11.75H18.5499L13.6749 6.87501L14.7249 5.82501L21.4249 12.5L14.7249 19.175Z" fill="currentColor"></path></svg></a></nav></main><div class="col col--2"><div class="tableOfContents_jeP5 thin-scrollbar" style="opacity:0;transform:translateX(100px) translateZ(0)"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#challenges-in-real-time-computing" class="table-of-contents__link toc-highlight"><strong>Challenges in Real-Time Computing</strong></a><ul><li><a href="#01-low-efficiency-in-job-deployment" class="table-of-contents__link toc-highlight"><strong>01 Low Efficiency in Job Deployment</strong></a></li><li><a href="#02-unclear-job-ownership-information" class="table-of-contents__link toc-highlight"><strong>02 Unclear Job Ownership Information</strong></a></li><li><a href="#03-difficulty-in-job-maintenance" class="table-of-contents__link toc-highlight"><strong>03 Difficulty in Job Maintenance</strong></a></li><li><a href="#04-difficulty-in-job-development-and-debugging" class="table-of-contents__link toc-highlight"><strong>04 Difficulty in Job Development and Debugging</strong></a></li></ul></li><li><a href="#the-journey-to-the-solution" class="table-of-contents__link toc-highlight"><strong>The Journey to the Solution</strong></a></li><li><a href="#in-depth-practice-based-on-apache-streampark" class="table-of-contents__link toc-highlight"><strong>In-depth Practice Based on Apache StreamPark™</strong></a><ul><li><a href="#01-ldap-login-support" class="table-of-contents__link toc-highlight"><strong>01 LDAP Login Support</strong></a></li><li><a href="#02-automatic-ingress-generation-for-job-submission" class="table-of-contents__link toc-highlight"><strong>02 Automatic Ingress Generation for Job Submission</strong></a></li><li><a href="#03-support-for-viewing-job-deployment-logs" class="table-of-contents__link toc-highlight"><strong>03 Support for Viewing Job Deployment Logs</strong></a></li><li><a href="#04-integration-of-grafana-monitoring-chart-links" class="table-of-contents__link toc-highlight"><strong>04 Integration of Grafana Monitoring Chart Links</strong></a></li><li><a href="#05-integration-of-flink-sql-security-for-permission-control" class="table-of-contents__link toc-highlight"><strong>05 Integration of Flink SQL Security for Permission Control</strong></a></li><li><a href="#06-data-synchronization-platform-based-on-apache-streampark" class="table-of-contents__link toc-highlight"><strong>06 Data Synchronization Platform Based on Apache StreamPark™</strong></a></li></ul></li><li><a href="#summary-of-practical-experience" class="table-of-contents__link toc-highlight"><strong>Summary of Practical Experience</strong></a><ul><li><a href="#01-building-base-images" class="table-of-contents__link toc-highlight"><strong>01 Building Base Images</strong></a></li><li><a href="#02-base-image-integration-with-arthas-example" class="table-of-contents__link toc-highlight"><strong>02 Base Image Integration with Arthas Example</strong></a></li><li><a href="#03-resolution-of-dependency-conflicts-in-images" class="table-of-contents__link toc-highlight"><strong>03 Resolution of Dependency Conflicts in Images</strong></a></li><li><a href="#04-centralized-job-configuration-example" class="table-of-contents__link toc-highlight"><strong>04 Centralized Job Configuration Example</strong></a></li><li><a href="#05-apache-streampark-dns-resolution-configuration" class="table-of-contents__link toc-highlight"><strong>05 Apache StreamPark™ DNS Resolution Configuration</strong></a></li><li><a href="#06-multi-instance-deployment-practice" class="table-of-contents__link toc-highlight"><strong>06 Multi-Instance Deployment Practice</strong></a></li></ul></li><li><a href="#benefits-brought" class="table-of-contents__link toc-highlight"><strong>Benefits Brought</strong></a></li><li><a href="#future-expectations" class="table-of-contents__link toc-highlight"><strong>Future Expectations</strong></a></li></ul></div></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Join Community</div><ul class="footer__items clean-list"><li class="footer__item">
                <div class="subscribe-box btns">
                  <a class="btn btn-primary" href="https://github.com/apache/incubator-streampark"><i class="fa fa-github"></i><span>Github</span></a>
                  <a class="btn btn-primary" href="https://github.com/apache/incubator-streampark/issues"><i class="fa fa-slack"></i><span>Issue Tracking</span></a>
                  <a class="btn btn-primary" href="javascript:void(0)">
                    <i class="fa fa-wechat"></i>
                    <span>Wechat</span>
                    <div class="wechat-dropdown"><img src="/image/join_wechat.png" alt="weChat"></div>
                  </a>
                </div>
              </li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright"><div>
        <div>
          <div style="margin-bottom: 30px;">
            <a href="https://incubator.apache.org/" class="footerLogoLink" one-link-mark="yes">
              <img alt="Apache Incubator logo" class="footer__logo" width="200">
            </a>
          </div>
          <div>
            <p>
            Apache StreamPark is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
            </p>
          </div>
        </div>

        <div>
          <span>
            Copyright © 2022-2024 The Apache Software Foundation. Apache StreamPark, StreamPark, and its feather logo are trademarks of The Apache Software Foundation.
          </span>
        </div>
      </div></div></div></div></footer></div>
</body>
</html>